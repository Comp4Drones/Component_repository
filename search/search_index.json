{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"COMP4DRONES components repository Welcome to the COMP4DRONES component repository. To know more about COMP4DRONES please visit comp4drones.eu . This repository aims at providing common components usable in different application domains, in particular those covered by project use-cases. The requirements for using a components will be listed, as well as a documentation on how to use it. The component itself will be hosted by the partner who provides it.","title":"Home"},{"location":"#comp4drones-components-repository","text":"Welcome to the COMP4DRONES component repository. To know more about COMP4DRONES please visit comp4drones.eu . This repository aims at providing common components usable in different application domains, in particular those covered by project use-cases. The requirements for using a components will be listed, as well as a documentation on how to use it. The component itself will be hosted by the partner who provides it.","title":"COMP4DRONES components repository"},{"location":"about/","text":"COMP4DRONES is an ECSEL JU project coordinated by Indra that brings together a consortium of 49 partners with the aim of providing a framework of key enabling technologies for safe and autonomous drones. It brings to bear a holistically designed ecosystem from application to electronic components, realized as a tightly integrated multi-vendor and compositional UAV embedded architecture solution and a tool chain complementing the compositional architecture principles. The project will mainly focus on the following objectives: Ease the integration and customization of embedded drone systems. Enable drones to take safe autonomous decisions . Ensure the deployment of trusted communications . Minimize the design and verification effort for complex drone applications. Ensuring sustainable impact and creation of an industry-driven community. Demonstration and validation activities are essential to ensure the quality and relevance of innovations. COMP4DRONES will ease the development of new application and functionalities on the fields of transport, construction, surveillance and inspection, logistics, and agriculture","title":"About"},{"location":"wp3_components_list/","text":"Components list ID Contributor Title WP3-01 IKERLAN Safety function - Pre-Certified SOM WP3-02 EDI Modular SoC-based embedded reference architecture WP3-03 BUT Sensor information algorithms WP3-04 HIB Computer Vision Components for drones WP3-10 IFAT Component for trusted communication WP3-13 ENAC Paparazzi UAV WP3-14_1 ENSMA Control components that implement potential barriers WP3-14_2 ENSMA Multi-agent swarm control WP3-15_1 ACORDE UWB based indoor positioning WP3-15_2 ACORDE Multi-antenna GNSS/INS based navigation WP3-16 SCALIAN EZ_Chains Fleet Architecture WP3-19_1 IMEC Hyperspectral payload WP3-19_2 IMEC Hyperspectral image processing WP3-20 MODIS Multi-sensor positioning WP3-22 UNIMORE Onboard Compute Platform Desing Methodology WP3-24 UNIVAQ Efficient digital implementation of controllers WP3-26 UWB Droneport: an autonomous drone battery management system WP3-28 UNISS Onboard Compute Platform Design Paradigm WP3-36_1 UDANET Smart and predictive energy management system WP3-36_2 UDANET AI drone system modules WP3-37 Aitek Video and data analytics WP4-2 SCALIAN EZ_Land Precision landing WP4-5 SCALIAN AI detection for clearance WP4-18_A TEKNE Drone-Rover Transponder WP4-42 SCALIAN AI Stabilization WP5-03 SCALIAN EZ_Com Safe fleet communication WP5-05_A TEKNE LP-WAN for UAV identification and monitoring WP4-33 UNIVAQ Autonomy, cooperation, and awareness","title":"Components list"},{"location":"wp3_components_list/#components-list","text":"ID Contributor Title WP3-01 IKERLAN Safety function - Pre-Certified SOM WP3-02 EDI Modular SoC-based embedded reference architecture WP3-03 BUT Sensor information algorithms WP3-04 HIB Computer Vision Components for drones WP3-10 IFAT Component for trusted communication WP3-13 ENAC Paparazzi UAV WP3-14_1 ENSMA Control components that implement potential barriers WP3-14_2 ENSMA Multi-agent swarm control WP3-15_1 ACORDE UWB based indoor positioning WP3-15_2 ACORDE Multi-antenna GNSS/INS based navigation WP3-16 SCALIAN EZ_Chains Fleet Architecture WP3-19_1 IMEC Hyperspectral payload WP3-19_2 IMEC Hyperspectral image processing WP3-20 MODIS Multi-sensor positioning WP3-22 UNIMORE Onboard Compute Platform Desing Methodology WP3-24 UNIVAQ Efficient digital implementation of controllers WP3-26 UWB Droneport: an autonomous drone battery management system WP3-28 UNISS Onboard Compute Platform Design Paradigm WP3-36_1 UDANET Smart and predictive energy management system WP3-36_2 UDANET AI drone system modules WP3-37 Aitek Video and data analytics WP4-2 SCALIAN EZ_Land Precision landing WP4-5 SCALIAN AI detection for clearance WP4-18_A TEKNE Drone-Rover Transponder WP4-42 SCALIAN AI Stabilization WP5-03 SCALIAN EZ_Com Safe fleet communication WP5-05_A TEKNE LP-WAN for UAV identification and monitoring WP4-33 UNIVAQ Autonomy, cooperation, and awareness","title":"Components list"},{"location":"enabling_technologies/WP3/ACORDE_1/","text":"WP3-15_1 - UWB based indoor positioning ID WP3-15_1 Contributor ACORDE Levels Platform, Function Require Energy, Raw sensed data from UWB transceiver and IMU Provide Navigation Sensor Input Raw sensed data from UWB transceiver and low-cost INS Output Position (main focus), attitude C4D building block UWB-based indoor positioning system TRL 4 Figure 61: Overall UWB-based indoor positioning system block (as reported in D2.3) Detailed Description In COMP4DRONES, ACORDE is developing an Indoor Positioning System (IPS), a solution for the challenges posed by the construction use case, demo tunnel (UC2-demo 2). ACORDE IPS corresponds to the building block in section 6.3.1 of D3.2 [24]. ACORDE IPS is specifically oriented to serve reliable and precise geo-referenced position to a drone flying on a tunnel under construction, in charge of the capture of raw data for tunnel digitization, to be employed for Building Information Modelling (BIM) [25]. The objective of the ACORDE IPS is two-fold, i.e., to ensure a safe navigation along a planned path, and to optimize the accuracy of the navigation data (and of raw UWB ranges) synchronized with the thermography or LIDAR capture so that it can be exploited in the offline digitalization process associated to the BIM. The specific environment and the usage scenario posed by the construction stakeholder (ACCIONA) and the drone integrator&operator (FADA-CATEC), configures a set of constraints in terms of cost of the solution, power consumption, size, weight, and precision (reported in [26]and [27]) which, added to the specific geometry of the indoor infrastructure, has leaded ACORDE to a novel and improved IPS solution. ACORDE IPS solution was first introduced in [28]. Here it is briefly overviewed and represented in Figure 62 (update of Fig.42 in [28]) for reader convenience. It is based on mounting a \u201ctag node\u201d on the indoor drone, in order to measure distances to a set of strategically deployed \u201canchor nodes\u201d, called ranges. The tag will rely on an INS sensor for providing a processed tag position. In addition, the tag can also provide range data (if required). A standard Mavlink interface supports the provision of these data to the drone navigation system. Figure 62: Indoor Positioning System developed by ACORDE The posed solution contributes improvements and innovations accounting the scenario and the current state of the art (as explained in section 1.2). The solution is also \u201ccomplete\u201d in the sense that ACORDE covers the design&implementation of the solution (anchor&tags platforms and their firmware). Moreover, ACORDE has also improved its design flow by developing the IPS-MAF tool. All this activity, comprised by technical packages WP3-WP6, can be summarized as follows: Design and implementation of anchor and tag platforms (including the required board support package (BSP) (WP3-15_2). Design and implementation of anchor and tag firmware, including the configuration, positioning algorithms and interfacing (WP4-17). Ensuring a robust and enriched communication among anchors, and among the tag (within the drone) and the anchors, for a more robust and improved positioning (WP5-19-ACO). Developing an Indoor Positioning System Modelling and Analysis Framework (IPS-MAF) for indoor structures, specifically tunnels (WP6). Providing at the tag a Mavlink [29] interface to support providing both processed positioning information, and \u201cpre-processed\u201d information (ranges) (WP3, WP4, and WP5). Current Status Up to the report date, ACORDE has advanced in parallel in all the afore mentioned tasks. The current advance can be summarized as follows: For anchor and tag platform design, ACORDE has already decided the ranging technology, i.e., Ultrawideband (UWB), and the module sustaining the implementation (DW1000 from Qorvo [DW1K]). ACORDE has also evaluated the basic performance of this technology, after acquiring an evaluation kit. Moreover, ACORDE has already decided on main components of the anchor and tag architectures (e.g., microcontrolers), and provisioning them (which has turned out to be critical during these COVID\u201919 times). Related to application design, ACORDE checked code available with acquired evaluation kit and its unsuitability for the posed use case. ACORDE has been already able to start to develop and test modified trilateration algorithms for anchors autopositioning and tag positioning on top of IPS-MAF simulations. The assessment, implementation and basic test of a novel and improved Medium Access Control (MAC) and ranging protocol for single-tag scenario has been also performed. A former version of IPS-MAF supporting static and simulation-based analysis is ready. A Mavlink library ready for supporting the tag to provide position is ready. A Mavlink extension is also proposed for enabling the tag providing the ranges. This is allowing ACORDE tackling the final design and implementation of the anchor&tag platforms, firmware implementation and validation (according to the plan in [D1.2]). Contribution and Improvements The stakeholder (ACCIONA) described the digitization scenario at a specific time slot where the construction 7days/24hours activity stopped for 1-2hours. At that time, it is possible to lie some \u201cmobile\u201d anchors, together with the \u201cfixed\u201d anchors, and later let the drone perform a pre-programmed digitization flight, where the tunnel under-construction can present some eventual obstacles (e.g., machines, signals). The posed scenario presents several challenges, many of them specific in relation to other existing real-time location systems: The specific long geometry of tunnel (which challenges UWB ranges, cost of the solution and the validity of conventional trilateration algorithms). Providing features for a flexible and agile anchor deployment, which does not oblige surveyors to geo-positioning all anchors, especially mobile ones. Need to provide real-time, 3D geo-referenced positioning on the tag (while most solutions focus on 2D relative positioning, commonly computed on a ground platform which centralizes data from anchors). Cost and energy/power optimization (the latter important for fixed anchors) Need for system-level design tooling. The IPS is a complex system with many parameters and aspects (specific deployment, transmission powers, sensitivities, latencies of the ranging phases, algorithmic alternatives, etc) with potential significant impact on the overall performance. Means for facilitating a holistic design, accounting for those many different aspects at early design stage are required even for experienced designers in the field. The same scenario also presented the opportunity to exploit the fact that during digitization flight only one drone, i.e., one tag needs to be serviced. The question is how to exploit this single-tag assumption for a safer flight and better digitalization. Figure 63: Some results at the current status of ACORDE IPS system Development. The ACORDE solution tackles these challenges and opportunities. So far, a number of contributions in comparison to other existing solutions are highlighted: Customized, cost-effective anchor and tag platforms, specifically designed to cope with the computational needs (specially for tag) and energy/power efficiency needs (specially anchors). Novel MAC protocol called Asynchronous Tag trigger, Slotted Anchor response with Deterministic and Random Allocation (ATSA-DRA), specifically adapted to the single-tag assumption, that ensures deterministic latency, while optimizing the number of anchors in view. This protocol has been already implemented and formerly tested on the evaluation kit as reported in [30]. At application level, the solution enables geo-referenced, real-time 3D positioning. Moreover, the application overcomes the challenges of the tunnel geometry. A modified trilateration algorithm has been already developed which enables positioning in regions of limited anchors visibility (coverage) and poor dilution of precision (DOP), where a conventional least-squares based approach is not working. This is specifically required for enabling anchors auto-positioning at the initial phase. The development of IPS-MAF, is a qualitative step on ACORDE capabilities for tackling custom design of IPS systems for long indoor infrastructures (tunnels, mines, large pipes, \u2026). It enables a newer system-level design flow (as explained later). It is remarkable that IPS-MAF already served to detect the afore mentioned issues on the usage of conventional least-squares algorithms, and to design and test the modified trilateration algorithm, even before the anchor or tag platforms are ready. IPS-MAF reveals not straightforward outcomes, e.g., notice the \u201ccoverage\u201d pattern and the \u201cshadow\u201d provoked by obstacles in Figure 63b. The support of a Mavlink interface at the tag side, to provide both completely processed (e.g., position data) and partially processed (e.g., ranges). The latter enable the offering ACORDE IPS as a complementary positioning sub-system, which integrators can fuse with other alternative technologies. Moreover, ranges transfer is a \u201csmooth\u201d (in the form of dialect) Mavlink extension contributed by ACORDE in COMP4DRONES [30]. Design and Implementation ACORDE IPS development was started in COMP4DRONES, motivated by the needs of the stakeholder (ACCIONA) and drone integrator (FADA-CATEC), by relying on the expertise of ACORDE on outdoor positioning, and on embedded HW and SW development. The statement of the solution requirements and the wish list from the stakeholder and the drone integrator, made early evident that a more holistic, model-based design was required. It motivated the development of the IPS-MAF, which, as shown in Figure 64, can be now used at early design stage to build up a holistic model of the IPS system. IPS-MAF can be used to analyse and decide key aspect at different levels of system (deployment of the anchors, sensitivities and transmission powers, transmission frequencies, etc) while keeping a holistic view of the system. Figure 64: IPS-MAF provides a qualitative step towards a holistic, model-based design of Indoor Positioning Solutions for long infrastructures That is, IPS-MAF enables a holistic model of the IPS, while integrating some actual pieces of the application, so in that sense it also enables to advance some part of the application development. Therefore IPS-MAF feeds the conventional platform development and embedded application development phases, where ACORDE has already long expertise. At the same time, the measurement and characterizations done for platform and application development serve to feedback and polish the holistic model. Summing up, an extended system-level design flow has been enabled, after coupling IPS-MAF to conventional ACORDE development processes for platform development (which includes PCB design, mechanical design, drivers\u2019 development, embedded application development) and application development (where ACORDE typically develops in C/C++, relying on a GUI based (typically Eclipse), microcontroller specific cross-development environment.","title":"ACORDE - UWB based indoor positioning"},{"location":"enabling_technologies/WP3/ACORDE_1/#wp3-15_1-uwb-based-indoor-positioning","text":"ID WP3-15_1 Contributor ACORDE Levels Platform, Function Require Energy, Raw sensed data from UWB transceiver and IMU Provide Navigation Sensor Input Raw sensed data from UWB transceiver and low-cost INS Output Position (main focus), attitude C4D building block UWB-based indoor positioning system TRL 4 Figure 61: Overall UWB-based indoor positioning system block (as reported in D2.3)","title":"WP3-15_1 - UWB based indoor positioning"},{"location":"enabling_technologies/WP3/ACORDE_1/#detailed-description","text":"In COMP4DRONES, ACORDE is developing an Indoor Positioning System (IPS), a solution for the challenges posed by the construction use case, demo tunnel (UC2-demo 2). ACORDE IPS corresponds to the building block in section 6.3.1 of D3.2 [24]. ACORDE IPS is specifically oriented to serve reliable and precise geo-referenced position to a drone flying on a tunnel under construction, in charge of the capture of raw data for tunnel digitization, to be employed for Building Information Modelling (BIM) [25]. The objective of the ACORDE IPS is two-fold, i.e., to ensure a safe navigation along a planned path, and to optimize the accuracy of the navigation data (and of raw UWB ranges) synchronized with the thermography or LIDAR capture so that it can be exploited in the offline digitalization process associated to the BIM. The specific environment and the usage scenario posed by the construction stakeholder (ACCIONA) and the drone integrator&operator (FADA-CATEC), configures a set of constraints in terms of cost of the solution, power consumption, size, weight, and precision (reported in [26]and [27]) which, added to the specific geometry of the indoor infrastructure, has leaded ACORDE to a novel and improved IPS solution. ACORDE IPS solution was first introduced in [28]. Here it is briefly overviewed and represented in Figure 62 (update of Fig.42 in [28]) for reader convenience. It is based on mounting a \u201ctag node\u201d on the indoor drone, in order to measure distances to a set of strategically deployed \u201canchor nodes\u201d, called ranges. The tag will rely on an INS sensor for providing a processed tag position. In addition, the tag can also provide range data (if required). A standard Mavlink interface supports the provision of these data to the drone navigation system. Figure 62: Indoor Positioning System developed by ACORDE The posed solution contributes improvements and innovations accounting the scenario and the current state of the art (as explained in section 1.2). The solution is also \u201ccomplete\u201d in the sense that ACORDE covers the design&implementation of the solution (anchor&tags platforms and their firmware). Moreover, ACORDE has also improved its design flow by developing the IPS-MAF tool. All this activity, comprised by technical packages WP3-WP6, can be summarized as follows: Design and implementation of anchor and tag platforms (including the required board support package (BSP) (WP3-15_2). Design and implementation of anchor and tag firmware, including the configuration, positioning algorithms and interfacing (WP4-17). Ensuring a robust and enriched communication among anchors, and among the tag (within the drone) and the anchors, for a more robust and improved positioning (WP5-19-ACO). Developing an Indoor Positioning System Modelling and Analysis Framework (IPS-MAF) for indoor structures, specifically tunnels (WP6). Providing at the tag a Mavlink [29] interface to support providing both processed positioning information, and \u201cpre-processed\u201d information (ranges) (WP3, WP4, and WP5).","title":"Detailed Description"},{"location":"enabling_technologies/WP3/ACORDE_1/#current-status","text":"Up to the report date, ACORDE has advanced in parallel in all the afore mentioned tasks. The current advance can be summarized as follows: For anchor and tag platform design, ACORDE has already decided the ranging technology, i.e., Ultrawideband (UWB), and the module sustaining the implementation (DW1000 from Qorvo [DW1K]). ACORDE has also evaluated the basic performance of this technology, after acquiring an evaluation kit. Moreover, ACORDE has already decided on main components of the anchor and tag architectures (e.g., microcontrolers), and provisioning them (which has turned out to be critical during these COVID\u201919 times). Related to application design, ACORDE checked code available with acquired evaluation kit and its unsuitability for the posed use case. ACORDE has been already able to start to develop and test modified trilateration algorithms for anchors autopositioning and tag positioning on top of IPS-MAF simulations. The assessment, implementation and basic test of a novel and improved Medium Access Control (MAC) and ranging protocol for single-tag scenario has been also performed. A former version of IPS-MAF supporting static and simulation-based analysis is ready. A Mavlink library ready for supporting the tag to provide position is ready. A Mavlink extension is also proposed for enabling the tag providing the ranges. This is allowing ACORDE tackling the final design and implementation of the anchor&tag platforms, firmware implementation and validation (according to the plan in [D1.2]).","title":"Current Status"},{"location":"enabling_technologies/WP3/ACORDE_1/#contribution-and-improvements","text":"The stakeholder (ACCIONA) described the digitization scenario at a specific time slot where the construction 7days/24hours activity stopped for 1-2hours. At that time, it is possible to lie some \u201cmobile\u201d anchors, together with the \u201cfixed\u201d anchors, and later let the drone perform a pre-programmed digitization flight, where the tunnel under-construction can present some eventual obstacles (e.g., machines, signals). The posed scenario presents several challenges, many of them specific in relation to other existing real-time location systems: The specific long geometry of tunnel (which challenges UWB ranges, cost of the solution and the validity of conventional trilateration algorithms). Providing features for a flexible and agile anchor deployment, which does not oblige surveyors to geo-positioning all anchors, especially mobile ones. Need to provide real-time, 3D geo-referenced positioning on the tag (while most solutions focus on 2D relative positioning, commonly computed on a ground platform which centralizes data from anchors). Cost and energy/power optimization (the latter important for fixed anchors) Need for system-level design tooling. The IPS is a complex system with many parameters and aspects (specific deployment, transmission powers, sensitivities, latencies of the ranging phases, algorithmic alternatives, etc) with potential significant impact on the overall performance. Means for facilitating a holistic design, accounting for those many different aspects at early design stage are required even for experienced designers in the field. The same scenario also presented the opportunity to exploit the fact that during digitization flight only one drone, i.e., one tag needs to be serviced. The question is how to exploit this single-tag assumption for a safer flight and better digitalization. Figure 63: Some results at the current status of ACORDE IPS system Development. The ACORDE solution tackles these challenges and opportunities. So far, a number of contributions in comparison to other existing solutions are highlighted: Customized, cost-effective anchor and tag platforms, specifically designed to cope with the computational needs (specially for tag) and energy/power efficiency needs (specially anchors). Novel MAC protocol called Asynchronous Tag trigger, Slotted Anchor response with Deterministic and Random Allocation (ATSA-DRA), specifically adapted to the single-tag assumption, that ensures deterministic latency, while optimizing the number of anchors in view. This protocol has been already implemented and formerly tested on the evaluation kit as reported in [30]. At application level, the solution enables geo-referenced, real-time 3D positioning. Moreover, the application overcomes the challenges of the tunnel geometry. A modified trilateration algorithm has been already developed which enables positioning in regions of limited anchors visibility (coverage) and poor dilution of precision (DOP), where a conventional least-squares based approach is not working. This is specifically required for enabling anchors auto-positioning at the initial phase. The development of IPS-MAF, is a qualitative step on ACORDE capabilities for tackling custom design of IPS systems for long indoor infrastructures (tunnels, mines, large pipes, \u2026). It enables a newer system-level design flow (as explained later). It is remarkable that IPS-MAF already served to detect the afore mentioned issues on the usage of conventional least-squares algorithms, and to design and test the modified trilateration algorithm, even before the anchor or tag platforms are ready. IPS-MAF reveals not straightforward outcomes, e.g., notice the \u201ccoverage\u201d pattern and the \u201cshadow\u201d provoked by obstacles in Figure 63b. The support of a Mavlink interface at the tag side, to provide both completely processed (e.g., position data) and partially processed (e.g., ranges). The latter enable the offering ACORDE IPS as a complementary positioning sub-system, which integrators can fuse with other alternative technologies. Moreover, ranges transfer is a \u201csmooth\u201d (in the form of dialect) Mavlink extension contributed by ACORDE in COMP4DRONES [30].","title":"Contribution and Improvements"},{"location":"enabling_technologies/WP3/ACORDE_1/#design-and-implementation","text":"ACORDE IPS development was started in COMP4DRONES, motivated by the needs of the stakeholder (ACCIONA) and drone integrator (FADA-CATEC), by relying on the expertise of ACORDE on outdoor positioning, and on embedded HW and SW development. The statement of the solution requirements and the wish list from the stakeholder and the drone integrator, made early evident that a more holistic, model-based design was required. It motivated the development of the IPS-MAF, which, as shown in Figure 64, can be now used at early design stage to build up a holistic model of the IPS system. IPS-MAF can be used to analyse and decide key aspect at different levels of system (deployment of the anchors, sensitivities and transmission powers, transmission frequencies, etc) while keeping a holistic view of the system. Figure 64: IPS-MAF provides a qualitative step towards a holistic, model-based design of Indoor Positioning Solutions for long infrastructures That is, IPS-MAF enables a holistic model of the IPS, while integrating some actual pieces of the application, so in that sense it also enables to advance some part of the application development. Therefore IPS-MAF feeds the conventional platform development and embedded application development phases, where ACORDE has already long expertise. At the same time, the measurement and characterizations done for platform and application development serve to feedback and polish the holistic model. Summing up, an extended system-level design flow has been enabled, after coupling IPS-MAF to conventional ACORDE development processes for platform development (which includes PCB design, mechanical design, drivers\u2019 development, embedded application development) and application development (where ACORDE typically develops in C/C++, relying on a GUI based (typically Eclipse), microcontroller specific cross-development environment.","title":"Design and Implementation"},{"location":"enabling_technologies/WP3/ACORDE_2/","text":"WP3-15_2 Geo-referenced Position and Attitude Estimation ID WP3-15_2 Contributor ACORDE Levels Platform, Function Require Energy, Raw sensed data navigation messages of GNSS receivers, raw data from low-cost IMU and barometer. Provide Navigation Sensor Input Raw sensed data from UWB transceiver and low-cost INS Output Position (main focus), attitude C4D building block Geo-referenced Position and Attitude Estimation System TRL 5 Figure 65: Geo-referenced Position and Attitude Estimation system block (as reported in D2.3) Detailed Description In COMP4DRONES, ACORDE is developing GLAD+, and outdoor geo-referencing system which abides to the \u201cGeo-referenced Positioning and Attitude estimation system\u201d block introduced in the COMP4DRONES architecture introduced (section 8.11 of D3.2 [24]), and whose representation is reproduced in Figure 65 for convenience. GLAD+ is an improved version of its predecessor GLAD (GNSS-based Low-Cost position and Attitude Determination system). Section 1.2 sums up the contributions and improvements brought by COMP4DRONES with respect to GLAD. GLAD+ is specifically oriented to provide drones quality navigation information (position, velocity and attitude) at a reduced cost in challenged outdoor scenarios. Drone scenarios expose position/attitude estimation systems to challenging conditions (e.g., shadows, more challenging dynamics than land-vehicles, calibration constraints), and stringent cost, size and weight requirements, as the ones reported in [26] and [27]. As GLAD, GLAD+ is a \u201ccomplete\u201d solution from ACORDE, in the sense that ACORDE performs both application and platform design and development (including a COTS based HW design). In addition to a significant upgrade of its positioning products, in COMP4DRONES ACORDE is also aiming the improvement of its productivity on the modelling, design, implementation and validation procedures of these type of systems. ACORDE is tackling the design and development of GLAD+, and the improvement of the related design flow in different parallel activities: The design of and improved HW/SW platform for the navigation solutions (WP3-15_1). It includes a new HW platform with improved capabilities on GNSS receivers, and also the assessment of license-free real-time RTOS on top. Enhancement of the navigation software, for adapting it to the new platform and to apply algorithmic improvements, including the assessment of possible AI based improvements (WP4-16). Providing support of anti-jamming and anti-spoofing features (WP5-11-ACO). Providing an improved/extended interface for a smoother integration on drone systems (activity globally associated to WP3, with implications in WP4 and WP5). Current status So far, ACORDE has advanced in parallel in all the afore mentioned tasks. The current advance can be summarized as follows: The new HW platform for GLAD+ has been designed and implemented (shown in Figure 66a). The new HW platform includes new low-cost GNSS receivers, with multi-constellation and anti-jamming and anti-spoofing capabilities. A custom Linux Real-Time port for the GLAD+ HW platform has been developed. This port relies on an updated u-boot, on buildroot-2020.02-LTS (generating a filesystem to be loaded as ramdisk), and on kernel-rt-4.19.59 (includes RT-PREEMT patch). In past GLAD development, a platform abstraction layer was developed in order to separate platform dependent code (e.g., threads creation, interruption management, POSIX calls, call to drivers). This layer has been already updated to cover RT-Linux target. A preliminary version of the driver for making available jamming and spoofing events has been already developed and tested. A logger system has been updated and sent together the GLAD+ HW/SW platform ACORDE to the drone integrator in construction use case (UC2-demo1). ACORDE generated documentation for integration and supported the integrator in what resulted a smooth and quick integration. The integrator performed flight tests. ACORDE supported the integrator with additional documentation (i.e., for operation and tests of the integration, and for indicating specific maneuvers to let validation tests cover as many working conditions as possible). With that information, ACORDE has done some preliminary evaluation of the current algorithms, and tested the potential of some improvements, i.e.., multi-constellation support. Algorithmic improvement is in progress. Figure 66: New HW/SW platform for the ACORDE outdoor geo-referencing system and integration on a drone platform int he COMP4DRONES for the outdoor demo of the construction use case Contribution and Improvements GLAD+ is expected to bring important improvements or contributions that can be summarized as follows: More interoperability , given by the support of a Mavlink interface for providing the position estimates. Security features , enabled by the anti-jamming and anti-spoofing capabilities Lesser cost , a validated Linux-based port implementation means an important license cost saving with regard GLAD implementation. Better performance (higher resilience, precision, and integrity) of the positioning solution (including attitude). A proposal of smooth Mavlink extension to provide all attitude components, and to report jamming and spoofing events. Figure 67a) shows the boot of the custom RT Linux distribution developed by ACORDE for GLAD+ platform. As can be seen, the kernel boots in little bit more 3 seconds (time labels are relative to kernel start). Taking into account the bootloader (not shown in the capture) took around4-5s, it leads to ~8s for booting the position&attitude estimation application (actually, the boot process goes on to get Ethernet link ready and launching SSH services, however, they are used for configuration or debugging, and the position&attitude application is launched before triggering). Further tests were conducted. Different resolution timers (from 100 to 1000 Hz) were tested. Moreover, the latencies of the scheduler (context switches) were estimated under heavy workload conditions, relying on the \u201ccyclictest\u201d [31] and \u201chackbench\u201d [32] standard Linux utilities. An average latency 41 us, with 72 us maximum latency was obtained. This is a key figure for tick timer setting. For instance, 20 context-switches per second would take 1.4ms max. overhead, and less than 1ms on average, which seems affordable for GLAD+ application margins. Further study with final GLAD+ firmware is still required though. Figure 67: Some results of the activity on design and development of GLAD+ in COMP4DRONES Figure 67b) shows the results of some preliminary analysis enabled by the data log from the integration tests of Dec. 2020. That test was performed in static conditions in front of the facilities of the integrator (FADA-CATEC). Specifically, Figure 67b) shows three stacked graphics with the three components of a \u201ctail-head\u201d baseline estimation, fundamental pre-processing for attitude estimation (heading and pitch components can be directly derived via a simple formula). The top one is a non-causal estimate, and thus only possible after offline post-processing, which is taken as a ground truth reference. Green colors reflect convergence of the ambiguity resolution algorithms (and thus resembles a precise estimate), while orange reflect no-convergence, and thus no-valid, in general, inaccurate estimate. The bottom graphic shows a \u201cforward\u201d (i.e., that can be computed in real-time) estimate of the baseline using only GPS constellation. As can be seen, most of the capture (97.9%) lacks convergence. The problem is related to the buildings around the drone at the capture time, which shadowed several satellites of the GPS constellation. The graphic on the middle, shows a forward estimate of the baseline using both GPS and Galileo constellations, as enabled by the GLAD+ HW/SW platform. This estimate gets the same 97.7% figure, but of convergence time, indeed very close to the 98.3% time achieved with non-causal processing. Design and Implementation As well as the significant improvements regarding GLAD, which are leading to a new improved low-cost geo-referenced position&attitude estimation system, i.e., GLAD+, in COMP4DRONES, ACORDE is making progress towards a qualitative enhancement of its design procedures. This piece of work is in direct relation to WP6 activities. Figure 68: Design flow followed by ACORDE for its outdoor geo-referenced position&attitude estimation systems Figure 68 shows the development methodology that ACORDE is applying for its georeferenced outdoor positioning systems. The methodology is relatively conventional in its two bottom layers. ACORDE has long expertise in platform development, which includes PCB design, mechanical design, drivers\u2019 development. ACORDE also has experience on embedded application development. GLAD, and so GLAD+ were and are developed in C/C++, on a microcontroller targeted cross-development environment, and relying on a GUI based environment (Visual Studio or Eclipse). On top of these two layers, ACORDE develops and tries different versions of the positioning algorithms by using a Matlab model. This model is fed with raw data logged on captures taken on integration tests and real flights. Additional Matlab scripts serve to test and compare output results. This approach has proven to be useful and enables model building/refinement relying on raw sensor data logged at past captures. However, it has also some important disadvantages. One of them is associated to the conventional, sequential HW/SW development. HW platform design and implementation comes before driver development or RTOS port (if required). In turn, HW/SW platform availability is a pre-condition for application development. Once the application is ready, it is possible to evaluate if the performance (timing, memory, energy and power consumption, etc) and validate it. As well as the long latencies involved, the flow adds the risk of late finding of performance bottlenecks that involve a drastic re-design, of hardware in the worst case. To these drawbacks, we need to add the high cost of translation of Matlab models to C/C++ implementation, and the traceability problems generated at the validation time. In COMP4DRONES, ACORDE is developing and evaluating a newer approach to overcome those drawbacks. It is sketched in Figure 69. The ESDE framework developed in WP6 [33], is encrusted in at a top level layer, for a system-level approach to embedded software design and development, to encompass the conventional HW/SW platform development capabilities and processes of ACORDE. Figure 69: System-level methodology for the design and implementation of outdoor geo-referenced position&attitude estimation systems in COMP4DRONES Some key aspects of the ESDE flow for productivity improvement are: The fast functional models that can be built at the top lever, able to significantly speed-up functional validation vs Matlab model execution. The automated embedded software generation mechanisms that avoid a significant translation effort from the system model to the implementation C/C++ code. The possibility to parallelize of the development of firmware (binary or object file), or very close version of it with HW development, by relying on a high-fidelity virtual platform. The possibility to validate firmware (without availability of the physical platform), eventually using several virtual platforms for test parallelization More details on ESDE are reported on WP6 reports.","title":"ACORDE - Multi-antenna GNSS/INS based navigation"},{"location":"enabling_technologies/WP3/ACORDE_2/#wp3-15_2-geo-referenced-position-and-attitude-estimation","text":"ID WP3-15_2 Contributor ACORDE Levels Platform, Function Require Energy, Raw sensed data navigation messages of GNSS receivers, raw data from low-cost IMU and barometer. Provide Navigation Sensor Input Raw sensed data from UWB transceiver and low-cost INS Output Position (main focus), attitude C4D building block Geo-referenced Position and Attitude Estimation System TRL 5 Figure 65: Geo-referenced Position and Attitude Estimation system block (as reported in D2.3)","title":"WP3-15_2 Geo-referenced Position and Attitude Estimation"},{"location":"enabling_technologies/WP3/ACORDE_2/#detailed-description","text":"In COMP4DRONES, ACORDE is developing GLAD+, and outdoor geo-referencing system which abides to the \u201cGeo-referenced Positioning and Attitude estimation system\u201d block introduced in the COMP4DRONES architecture introduced (section 8.11 of D3.2 [24]), and whose representation is reproduced in Figure 65 for convenience. GLAD+ is an improved version of its predecessor GLAD (GNSS-based Low-Cost position and Attitude Determination system). Section 1.2 sums up the contributions and improvements brought by COMP4DRONES with respect to GLAD. GLAD+ is specifically oriented to provide drones quality navigation information (position, velocity and attitude) at a reduced cost in challenged outdoor scenarios. Drone scenarios expose position/attitude estimation systems to challenging conditions (e.g., shadows, more challenging dynamics than land-vehicles, calibration constraints), and stringent cost, size and weight requirements, as the ones reported in [26] and [27]. As GLAD, GLAD+ is a \u201ccomplete\u201d solution from ACORDE, in the sense that ACORDE performs both application and platform design and development (including a COTS based HW design). In addition to a significant upgrade of its positioning products, in COMP4DRONES ACORDE is also aiming the improvement of its productivity on the modelling, design, implementation and validation procedures of these type of systems. ACORDE is tackling the design and development of GLAD+, and the improvement of the related design flow in different parallel activities: The design of and improved HW/SW platform for the navigation solutions (WP3-15_1). It includes a new HW platform with improved capabilities on GNSS receivers, and also the assessment of license-free real-time RTOS on top. Enhancement of the navigation software, for adapting it to the new platform and to apply algorithmic improvements, including the assessment of possible AI based improvements (WP4-16). Providing support of anti-jamming and anti-spoofing features (WP5-11-ACO). Providing an improved/extended interface for a smoother integration on drone systems (activity globally associated to WP3, with implications in WP4 and WP5).","title":"Detailed Description"},{"location":"enabling_technologies/WP3/ACORDE_2/#current-status","text":"So far, ACORDE has advanced in parallel in all the afore mentioned tasks. The current advance can be summarized as follows: The new HW platform for GLAD+ has been designed and implemented (shown in Figure 66a). The new HW platform includes new low-cost GNSS receivers, with multi-constellation and anti-jamming and anti-spoofing capabilities. A custom Linux Real-Time port for the GLAD+ HW platform has been developed. This port relies on an updated u-boot, on buildroot-2020.02-LTS (generating a filesystem to be loaded as ramdisk), and on kernel-rt-4.19.59 (includes RT-PREEMT patch). In past GLAD development, a platform abstraction layer was developed in order to separate platform dependent code (e.g., threads creation, interruption management, POSIX calls, call to drivers). This layer has been already updated to cover RT-Linux target. A preliminary version of the driver for making available jamming and spoofing events has been already developed and tested. A logger system has been updated and sent together the GLAD+ HW/SW platform ACORDE to the drone integrator in construction use case (UC2-demo1). ACORDE generated documentation for integration and supported the integrator in what resulted a smooth and quick integration. The integrator performed flight tests. ACORDE supported the integrator with additional documentation (i.e., for operation and tests of the integration, and for indicating specific maneuvers to let validation tests cover as many working conditions as possible). With that information, ACORDE has done some preliminary evaluation of the current algorithms, and tested the potential of some improvements, i.e.., multi-constellation support. Algorithmic improvement is in progress. Figure 66: New HW/SW platform for the ACORDE outdoor geo-referencing system and integration on a drone platform int he COMP4DRONES for the outdoor demo of the construction use case","title":"Current status"},{"location":"enabling_technologies/WP3/ACORDE_2/#contribution-and-improvements","text":"GLAD+ is expected to bring important improvements or contributions that can be summarized as follows: More interoperability , given by the support of a Mavlink interface for providing the position estimates. Security features , enabled by the anti-jamming and anti-spoofing capabilities Lesser cost , a validated Linux-based port implementation means an important license cost saving with regard GLAD implementation. Better performance (higher resilience, precision, and integrity) of the positioning solution (including attitude). A proposal of smooth Mavlink extension to provide all attitude components, and to report jamming and spoofing events. Figure 67a) shows the boot of the custom RT Linux distribution developed by ACORDE for GLAD+ platform. As can be seen, the kernel boots in little bit more 3 seconds (time labels are relative to kernel start). Taking into account the bootloader (not shown in the capture) took around4-5s, it leads to ~8s for booting the position&attitude estimation application (actually, the boot process goes on to get Ethernet link ready and launching SSH services, however, they are used for configuration or debugging, and the position&attitude application is launched before triggering). Further tests were conducted. Different resolution timers (from 100 to 1000 Hz) were tested. Moreover, the latencies of the scheduler (context switches) were estimated under heavy workload conditions, relying on the \u201ccyclictest\u201d [31] and \u201chackbench\u201d [32] standard Linux utilities. An average latency 41 us, with 72 us maximum latency was obtained. This is a key figure for tick timer setting. For instance, 20 context-switches per second would take 1.4ms max. overhead, and less than 1ms on average, which seems affordable for GLAD+ application margins. Further study with final GLAD+ firmware is still required though. Figure 67: Some results of the activity on design and development of GLAD+ in COMP4DRONES Figure 67b) shows the results of some preliminary analysis enabled by the data log from the integration tests of Dec. 2020. That test was performed in static conditions in front of the facilities of the integrator (FADA-CATEC). Specifically, Figure 67b) shows three stacked graphics with the three components of a \u201ctail-head\u201d baseline estimation, fundamental pre-processing for attitude estimation (heading and pitch components can be directly derived via a simple formula). The top one is a non-causal estimate, and thus only possible after offline post-processing, which is taken as a ground truth reference. Green colors reflect convergence of the ambiguity resolution algorithms (and thus resembles a precise estimate), while orange reflect no-convergence, and thus no-valid, in general, inaccurate estimate. The bottom graphic shows a \u201cforward\u201d (i.e., that can be computed in real-time) estimate of the baseline using only GPS constellation. As can be seen, most of the capture (97.9%) lacks convergence. The problem is related to the buildings around the drone at the capture time, which shadowed several satellites of the GPS constellation. The graphic on the middle, shows a forward estimate of the baseline using both GPS and Galileo constellations, as enabled by the GLAD+ HW/SW platform. This estimate gets the same 97.7% figure, but of convergence time, indeed very close to the 98.3% time achieved with non-causal processing.","title":"Contribution and Improvements"},{"location":"enabling_technologies/WP3/ACORDE_2/#design-and-implementation","text":"As well as the significant improvements regarding GLAD, which are leading to a new improved low-cost geo-referenced position&attitude estimation system, i.e., GLAD+, in COMP4DRONES, ACORDE is making progress towards a qualitative enhancement of its design procedures. This piece of work is in direct relation to WP6 activities. Figure 68: Design flow followed by ACORDE for its outdoor geo-referenced position&attitude estimation systems Figure 68 shows the development methodology that ACORDE is applying for its georeferenced outdoor positioning systems. The methodology is relatively conventional in its two bottom layers. ACORDE has long expertise in platform development, which includes PCB design, mechanical design, drivers\u2019 development. ACORDE also has experience on embedded application development. GLAD, and so GLAD+ were and are developed in C/C++, on a microcontroller targeted cross-development environment, and relying on a GUI based environment (Visual Studio or Eclipse). On top of these two layers, ACORDE develops and tries different versions of the positioning algorithms by using a Matlab model. This model is fed with raw data logged on captures taken on integration tests and real flights. Additional Matlab scripts serve to test and compare output results. This approach has proven to be useful and enables model building/refinement relying on raw sensor data logged at past captures. However, it has also some important disadvantages. One of them is associated to the conventional, sequential HW/SW development. HW platform design and implementation comes before driver development or RTOS port (if required). In turn, HW/SW platform availability is a pre-condition for application development. Once the application is ready, it is possible to evaluate if the performance (timing, memory, energy and power consumption, etc) and validate it. As well as the long latencies involved, the flow adds the risk of late finding of performance bottlenecks that involve a drastic re-design, of hardware in the worst case. To these drawbacks, we need to add the high cost of translation of Matlab models to C/C++ implementation, and the traceability problems generated at the validation time. In COMP4DRONES, ACORDE is developing and evaluating a newer approach to overcome those drawbacks. It is sketched in Figure 69. The ESDE framework developed in WP6 [33], is encrusted in at a top level layer, for a system-level approach to embedded software design and development, to encompass the conventional HW/SW platform development capabilities and processes of ACORDE. Figure 69: System-level methodology for the design and implementation of outdoor geo-referenced position&attitude estimation systems in COMP4DRONES Some key aspects of the ESDE flow for productivity improvement are: The fast functional models that can be built at the top lever, able to significantly speed-up functional validation vs Matlab model execution. The automated embedded software generation mechanisms that avoid a significant translation effort from the system model to the implementation C/C++ code. The possibility to parallelize of the development of firmware (binary or object file), or very close version of it with HW development, by relying on a high-fidelity virtual platform. The possibility to validate firmware (without availability of the physical platform), eventually using several virtual platforms for test parallelization More details on ESDE are reported on WP6 reports.","title":"Design and Implementation"},{"location":"enabling_technologies/WP3/Aitek/","text":"WP3-37 - Video and data analytics ID WP3-37 Contributor Aitek Levels System Require On board camera processing units with sufficient resources Provide Object detection Input Video flows Output Images and video flows (both with bounding box to highlight target detected), metadata (es. JSON files which describe the detected information), alarms, other information C4D building block Perception TRL 5 Detailed Description Video analytics component (WP3-37) is a software module that implements video analysis algorithms based on Deep Learning approaches. It will be used to process RGB (mainly) and infrared (eventually) images. This block is deputy to analyse image collected by onboard camera in order to extract relevant information, description/understanding of what has happened or what is happening. One of the most common tasks is object detection, that consists in recognizing the presence of a person or a specific object in an image, also detecting its position. Video analytics for object detection consists of the following four steps, as reported in Figure 88. While training phase is done offline, obviously not on the drone, the final steps, the real detection, can be done in real-time or in post processing and can be done on board or on the ground segment. Figure 88: Steps in detecting objects from video Contribution and Improvements Neural network optimization needed to simplify its complexity. In this way computational requirements can be relaxed and therefore such networks can be deployed also on low-power edge devices. A proper training phase is needed according to the application requirements. In particular, this component will be part of Use Case 5, and therefore will be adapted to be used in the context of the smart agriculture for plants detection. Design and Implementation Further details about this component are reported in this section. Targets are detected analysing each video frame as an individual image. The adopted technique is known as SSD (Single Shot Detector) in which the detection is done in a single stage (a single shot). Such approach is faster and simpler compared with other ones like RPNs (Regional Proposal Networks) that require at least two shots to perform the same task. In Figure 89 is reported the structure of an SSD. Figure 89: Structure of a Single Shot Detector The main characteristics of an SSD are reported here below: Single Shot: this means that the tasks of object localization and classification are done in a single forward pass of the network Detector: The network is an object detector that also classifies those detected objects Grid: an SSD divides the image using a grid and have each grid cell be responsible for detecting objects in that region of the image. Priors: each grid cell in SSD can be assigned with multiple anchor/prior boxes. These anchor boxes are pre-defined and each one is responsible for a size and shape within a grid cell","title":"Aitek - Video and data analytics"},{"location":"enabling_technologies/WP3/Aitek/#wp3-37-video-and-data-analytics","text":"ID WP3-37 Contributor Aitek Levels System Require On board camera processing units with sufficient resources Provide Object detection Input Video flows Output Images and video flows (both with bounding box to highlight target detected), metadata (es. JSON files which describe the detected information), alarms, other information C4D building block Perception TRL 5","title":"WP3-37 - Video and data analytics"},{"location":"enabling_technologies/WP3/Aitek/#detailed-description","text":"Video analytics component (WP3-37) is a software module that implements video analysis algorithms based on Deep Learning approaches. It will be used to process RGB (mainly) and infrared (eventually) images. This block is deputy to analyse image collected by onboard camera in order to extract relevant information, description/understanding of what has happened or what is happening. One of the most common tasks is object detection, that consists in recognizing the presence of a person or a specific object in an image, also detecting its position. Video analytics for object detection consists of the following four steps, as reported in Figure 88. While training phase is done offline, obviously not on the drone, the final steps, the real detection, can be done in real-time or in post processing and can be done on board or on the ground segment. Figure 88: Steps in detecting objects from video","title":"Detailed Description"},{"location":"enabling_technologies/WP3/Aitek/#contribution-and-improvements","text":"Neural network optimization needed to simplify its complexity. In this way computational requirements can be relaxed and therefore such networks can be deployed also on low-power edge devices. A proper training phase is needed according to the application requirements. In particular, this component will be part of Use Case 5, and therefore will be adapted to be used in the context of the smart agriculture for plants detection.","title":"Contribution and Improvements"},{"location":"enabling_technologies/WP3/Aitek/#design-and-implementation","text":"Further details about this component are reported in this section. Targets are detected analysing each video frame as an individual image. The adopted technique is known as SSD (Single Shot Detector) in which the detection is done in a single stage (a single shot). Such approach is faster and simpler compared with other ones like RPNs (Regional Proposal Networks) that require at least two shots to perform the same task. In Figure 89 is reported the structure of an SSD. Figure 89: Structure of a Single Shot Detector The main characteristics of an SSD are reported here below: Single Shot: this means that the tasks of object localization and classification are done in a single forward pass of the network Detector: The network is an object detector that also classifies those detected objects Grid: an SSD divides the image using a grid and have each grid cell be responsible for detecting objects in that region of the image. Priors: each grid cell in SSD can be assigned with multiple anchor/prior boxes. These anchor boxes are pre-defined and each one is responsible for a size and shape within a grid cell","title":"Design and Implementation"},{"location":"enabling_technologies/WP3/BUT/","text":"WP3-03 - Sensor information algorithms ID WP3-03 Contributor BUT Levels Functional Require Payload data (drone images) Provide Tone mapped HDR video as Axi4 Video Stream Input RGB images captured by drone (Payload data) Output RGB images processed by component (HDR tone mapped images) C4D building block (Video) Data Analytics TRL 4 Detailed Description HDR is part of image processing that focuses on capturing, processing, and displaying images with a High Dynamic Range. Its primary goal is to achieve reproduction of the captured scene on digital devices. HDR (HDR) for single images is deeply studied topic (although it is still quite opened). HDR can be obtained using specialized HDR sensors, or using standard sensors by acquisition of image sequence with different exposure times. These images can be captured simultaneously e.g. through beam splitter on several CCD/CMOS sensors or more often sequentially. HDR can be obtained using specialized HDR sensors, or using standard sensors by acquisition of image sequence with different exposure times. These images can be captured simultaneously e.g. through beam splitter on several CCD/CMOS or more often sequentially (merging/fusion of exposures). With the advent of HDR video capturing new problems arose. Application of TMO without careful consideration of temporal coherence between consecutive frames may lead to adverse effects. Video tone mapping methods needs to carefully consider temporal coherence to preserve this temporal character, for example during fast luminance changes. In the C4D project BUT is improving HDR video tone mapping FPGA IP core. The design is based on programmable hardware tightly connected to an \"embedded\" processor (FPGA SoC Xilinx Zynq, but may also be implemented on other platforms with FPGA). It covers all functionality: reading data from a camera sensor, merging multiple images with alternating exposures into HDR images/HDR video and applying HDR tone mapping. The system can be extended with other functions (software, hardware or FPGA IP core) such as HDR video compression, image pre-processing, exposure control, and the \u201cghost-free\u201d function removes possible artifacts caused by the movement of objects in the programmable hardware. This block provides acquired data in HDR or tone mapped format and can be extended with other data analytics tools/algorithms (e.g., detectors). In the C4D reference architecture context, this block supports data acquisition for further processing either in the FPGA or in the following systems. Sensor information algorithms is part of payload management \u2013 data acquisition block, and component provides inputs to the data management block (i.e., payload data analytics). Contribution and Improvements BUT is implementing and improving sensor data processing algorithms which include software and firmware for FPGA. This involves video processing algorithms (for example HDR algorithms). HDR multi-exposure fusion algorithm to be implemented in the drone, possibly also implementing tone mapping and/or ghost removal in order to \"feed\" further image and video processing subsystems in the drone by image information with high dynamic range. BUT increased performance of the algorithms, which reduce latency and increase throughput (currently IP core can process up to 200 mega pixels per second). Robustness of the controller with respect to environmental disturbances and increased resiliency. This improvement will be based on increased robustness of the video processing with respect to HDR while keeping the processing means and extent of video processing \"unchanged\" thanks to the tone mapping that virtually brings the \"same image format\" as in usual processing. Design and Implementation Component is divided into four main blocks (each block can be use independently): Sensor data acquisition Buffering HDR Merging and deghosting HDR Tone Mapping Sensor data acquisition Architecture is based on Xilinx Zynq platform which is connected to Python 2000 CMOS sensor using LVDS (Low-voltage differential signaling) interface. The CMOS output consists of a raw CFA (Color Filter Array) image data with a Bayer filter mosaic. Buffering The raw image is stored to DDR memory using DMA and double buffering to avoid overwriting of the data. For DDR write one DMA is used. For reading image data 3 DMAs are used. HDR Merging and deghosting The HDR merge block reads three image streams simultaneously through the DMAs. First, it applies inverse camera response function to obtain image with linear response and merge HDR image. The merging algorithm performs per-pixel processing and requires a relatively small number of per-pixel operations. Some of its functionality is computationally demanding (e.g. division and Gauss function calculation), however, it can be optimised and/or tabulated. The Gaussian function used for ghosting suppression can be convenient because the pixel values are discrete and only a finite combination of pixel values is possible. HDR Tone Mapping HDR pipeline is implemented in FPGA and pipelined at 200MHz while processing one pixel per clock. Input of Tone mapping block is 18bit CFA pixel in 10.8 fixed-point (FP) representation (10 integer and 4 fractional bits) and output is RGB pixel in <0,1> interval. Algorithm is based on Durand and Dorsey tone mapping operator. Durand operator is originally two pass algorithms, because it requires extreme values of the base layer. Implementation of multi-pass image processing algorithm in FPGA is problematic because of limited memory size. Typically, there is not enough space to store whole image directly in FPGA. In our case we need to compute only minimum and maximum value (or percentiles) of the base layer and we selected approach where minimum and maximum value is used from previous frame. Figure 35: Example of HDR processing during test flight. Top images contain tone mapped images, bottom images show LDR images used for merging and tone mapping","title":"BUT algorithms"},{"location":"enabling_technologies/WP3/BUT/#wp3-03-sensor-information-algorithms","text":"ID WP3-03 Contributor BUT Levels Functional Require Payload data (drone images) Provide Tone mapped HDR video as Axi4 Video Stream Input RGB images captured by drone (Payload data) Output RGB images processed by component (HDR tone mapped images) C4D building block (Video) Data Analytics TRL 4","title":"WP3-03 - Sensor information algorithms"},{"location":"enabling_technologies/WP3/BUT/#detailed-description","text":"HDR is part of image processing that focuses on capturing, processing, and displaying images with a High Dynamic Range. Its primary goal is to achieve reproduction of the captured scene on digital devices. HDR (HDR) for single images is deeply studied topic (although it is still quite opened). HDR can be obtained using specialized HDR sensors, or using standard sensors by acquisition of image sequence with different exposure times. These images can be captured simultaneously e.g. through beam splitter on several CCD/CMOS sensors or more often sequentially. HDR can be obtained using specialized HDR sensors, or using standard sensors by acquisition of image sequence with different exposure times. These images can be captured simultaneously e.g. through beam splitter on several CCD/CMOS or more often sequentially (merging/fusion of exposures). With the advent of HDR video capturing new problems arose. Application of TMO without careful consideration of temporal coherence between consecutive frames may lead to adverse effects. Video tone mapping methods needs to carefully consider temporal coherence to preserve this temporal character, for example during fast luminance changes. In the C4D project BUT is improving HDR video tone mapping FPGA IP core. The design is based on programmable hardware tightly connected to an \"embedded\" processor (FPGA SoC Xilinx Zynq, but may also be implemented on other platforms with FPGA). It covers all functionality: reading data from a camera sensor, merging multiple images with alternating exposures into HDR images/HDR video and applying HDR tone mapping. The system can be extended with other functions (software, hardware or FPGA IP core) such as HDR video compression, image pre-processing, exposure control, and the \u201cghost-free\u201d function removes possible artifacts caused by the movement of objects in the programmable hardware. This block provides acquired data in HDR or tone mapped format and can be extended with other data analytics tools/algorithms (e.g., detectors). In the C4D reference architecture context, this block supports data acquisition for further processing either in the FPGA or in the following systems. Sensor information algorithms is part of payload management \u2013 data acquisition block, and component provides inputs to the data management block (i.e., payload data analytics).","title":"Detailed Description"},{"location":"enabling_technologies/WP3/BUT/#contribution-and-improvements","text":"BUT is implementing and improving sensor data processing algorithms which include software and firmware for FPGA. This involves video processing algorithms (for example HDR algorithms). HDR multi-exposure fusion algorithm to be implemented in the drone, possibly also implementing tone mapping and/or ghost removal in order to \"feed\" further image and video processing subsystems in the drone by image information with high dynamic range. BUT increased performance of the algorithms, which reduce latency and increase throughput (currently IP core can process up to 200 mega pixels per second). Robustness of the controller with respect to environmental disturbances and increased resiliency. This improvement will be based on increased robustness of the video processing with respect to HDR while keeping the processing means and extent of video processing \"unchanged\" thanks to the tone mapping that virtually brings the \"same image format\" as in usual processing.","title":"Contribution and Improvements"},{"location":"enabling_technologies/WP3/BUT/#design-and-implementation","text":"Component is divided into four main blocks (each block can be use independently): Sensor data acquisition Buffering HDR Merging and deghosting HDR Tone Mapping Sensor data acquisition Architecture is based on Xilinx Zynq platform which is connected to Python 2000 CMOS sensor using LVDS (Low-voltage differential signaling) interface. The CMOS output consists of a raw CFA (Color Filter Array) image data with a Bayer filter mosaic. Buffering The raw image is stored to DDR memory using DMA and double buffering to avoid overwriting of the data. For DDR write one DMA is used. For reading image data 3 DMAs are used. HDR Merging and deghosting The HDR merge block reads three image streams simultaneously through the DMAs. First, it applies inverse camera response function to obtain image with linear response and merge HDR image. The merging algorithm performs per-pixel processing and requires a relatively small number of per-pixel operations. Some of its functionality is computationally demanding (e.g. division and Gauss function calculation), however, it can be optimised and/or tabulated. The Gaussian function used for ghosting suppression can be convenient because the pixel values are discrete and only a finite combination of pixel values is possible. HDR Tone Mapping HDR pipeline is implemented in FPGA and pipelined at 200MHz while processing one pixel per clock. Input of Tone mapping block is 18bit CFA pixel in 10.8 fixed-point (FP) representation (10 integer and 4 fractional bits) and output is RGB pixel in <0,1> interval. Algorithm is based on Durand and Dorsey tone mapping operator. Durand operator is originally two pass algorithms, because it requires extreme values of the base layer. Implementation of multi-pass image processing algorithm in FPGA is problematic because of limited memory size. Typically, there is not enough space to store whole image directly in FPGA. In our case we need to compute only minimum and maximum value (or percentiles) of the base layer and we selected approach where minimum and maximum value is used from previous frame. Figure 35: Example of HDR processing during test flight. Top images contain tone mapped images, bottom images show LDR images used for merging and tone mapping","title":"Design and Implementation"},{"location":"enabling_technologies/WP3/EDI_SoC/","text":"WP3-02 - Modular SoC-based embedded reference architecture ID WP3-02 Contributor EDI Levels System Require Specification of the architecture, D3.2 Provide Heterogeneous (FPGA + MPU) platform for carrying out computationally demanding algorithms on a drone Input HW accelerators and SW implementation of the core drone building blocks, i.e. for perception, actuation, flight planning, guidance and control, health and payload management, communication, data management and mission management. Output Energy efficient medium (SoM with a carrier board) and supporting software infrastructure for deploying multitude of autonomous flight control architectures. C4D building block TRL 5 Figure 33: Internal system architecture of the Modular SoC-based embedded reference architecture Detailed Description Core enablers for reducing energy consumption and enabling mobile computing are System-on-Chip (SoC) and their extension Heterogeneous System-on-Chip (HSoC) technologies. The currently available embedded flight controller platforms mostly use the sequential processing paradigm, which is a limiting factor for the drone\u2019s onboard computational capacity. Our objective is to improve the SoA drone capabilities by using modern HSoCs, which reduce energy consumption and accelerate computationally intensive algorithms, e.g. localization based on visual SLAM. We focus on employing HSoCs that combine Field Programmable Gate Arrays (FPGA) and processor computing paradigms within a single chip. Hence MPU can be used for providing a high-level control and for processing non-localized (requiring intense scatter-gather data access) algorithms, while FPGA is dedicated to processing pipelines and tasks requiring parallel and/or determined execution. In the C4D project, EDI designs and contributes a Modular SoC-based embedded reference architecture that includes the following hardware and software components: MPSoC-based autonomous flight controller hardware , including a Zynq UltraScale+ ZU15EG System on Module (SoM) from Trenz Electronics with a custom carrier board tailored for use with midsize drones. FPGA vendor-agnostic IP cores for standardized communication with the accelerators and their run-time configuration. Linux modules for Direct Memory Access (DMA) engine control, virtual memory management, accessing accelerator configuration and providing user-space API. System architecture frameworks for implementing component-based software architectures and ensuring appropriate inter-communication mechanism, i.e. inside process, between systems, zero data copy. Contribution and Improvements EDI is designing a specialized carrier board PCB for the Ultrascale+ SoMs from Trenz, shown in Figure 34. The design involves exposing standard connectivity such as SPI, I2C, UART, QSPI for MMC as well as high-speed buses \u2013 USB2.0 and USB3.0. The reference platform is intended for but not limited to mid-sized drones. Further iterations should bring the size of the computing platform even drones of smaller indoor-type. Figure 34: Core component of the reference platform \u2013 Trenz Ultrascale+ SoM During the project, EDI plans to utilize the platform to deploy and validate the developed localization component - Hardware-accelerated Optical flow and SLAM (WP4-01) \u2013 and, hopefully, accelerators developed by other partners. The FPGA portion of the design utilizes standardized interfaces and vendor-agnostic DMA engines, which enable the unification of the driver modules (for high-performance HSoCs supplied by Xilinx or Intel). Another improvement is the system architecture implementation tools \u2013 compage1 (for component-based software management) and icom2 (for inter-component communication), both of which were originally developed under the PRYSTINE project (G.A. 783190) for high-performance controllers in autonomous cars, and during the C4D project, functionalities are extended for supporting drones. Design and Implementation As the PCB design has not yet been finalized (at the writing of this document, the components are selected, the circuit is done but the efforts are directed towards the actual placement & routing and spatial requirement considerations), the other software and hardware (FPGA) related work is being done using prototyping platforms, i.e. Xilinx ZCU102 Ultrascale+ MPSoC. The on-chip system architecture, shown in Figure 33, represents the hardware and software portions of the MPSoC package, i.e. FPGA, kernel-space and user-space. At the hardware level, the selected communication interface for data exchange is based on the Direct Memory Access (DMA) engines, as it offloads MPU and is best suited for high data throughput [15]. The DMA engines utilize the standard AXI4 interface, which ensures compatibility with a collection of high-bandwidth cores. Although Figure 33 follows a shared memory model [16], the FPGA may also utilize dedicated FPGA DDR memory, as long as quick access to the data is not required by the software. DMA engine serves as a translator from AXI4 memory mapped to AXIS streaming interface, which is suited for pipelined data-driven circuits, i.e. accelerators. The configuration of the accelerators is achieved by exposing I/Os of the internal registers. A block \u201cRegmap to AXI-Lite\u201d translates access into a memory-mapped protocol, which further is used to interface with the MPU portion of the system. The handoff information to the software is passed in a standardized way (especially for ARM-based SoCs) \u2013 by utilizing Device Tree Blob (DTB) format. The autonomous flight controller runs a Linux operating system which assumes the incorporation of hardware abstraction mechanisms, most notably memory virtualization. These complexities are handled by Linux kernel modules that provide a user-space interface for DMA control and accelerator configuration. Notably, the memory virtualization mechanism results in a scattered physical layout of the memory. One option is to use scatter-gather memory access capabilities of the DMA engine. Nevertheless, this involves the creation of transaction descriptor tables (often a linked list) and additional memory access requests from another bus master apart from the MPU. We utilize kernel\u2019s Contiguous Memory Allocator [17] (CMA) feature, which marks the physical memory region and manages its run-time swaps whenever a new allocation takes place. The user space is supplied with a malloc-like interface, which handles memory management and contiguous page mapping. The user-space application follows a blackboard programming pattern [18], which is well-suited for research work. We provide two low-footprint open-source frameworks for the implementation of system architectures, the aforementioned compage and icom. Compage stores component configuration information in a separate Executable Linked Format (ELF) file segment, which provides a modular addition of new components (threads or processes). It also provides an INI-based file for configuring and replicating the software components. Icom, on the other hand, provides inter-component communication. It supports multiple communication paradigms (PUB-SUB, REQ-REP, PUSH-PULL) and can use the most appropriate communication mechanism (sockets, FIFO, message queues, etc.) depending on the communicating component location (distinct machines, same machine or same memory space).","title":"EDI SoC"},{"location":"enabling_technologies/WP3/EDI_SoC/#wp3-02-modular-soc-based-embedded-reference-architecture","text":"ID WP3-02 Contributor EDI Levels System Require Specification of the architecture, D3.2 Provide Heterogeneous (FPGA + MPU) platform for carrying out computationally demanding algorithms on a drone Input HW accelerators and SW implementation of the core drone building blocks, i.e. for perception, actuation, flight planning, guidance and control, health and payload management, communication, data management and mission management. Output Energy efficient medium (SoM with a carrier board) and supporting software infrastructure for deploying multitude of autonomous flight control architectures. C4D building block TRL 5 Figure 33: Internal system architecture of the Modular SoC-based embedded reference architecture","title":"WP3-02 - Modular\u00a0SoC-based\u00a0embedded reference architecture"},{"location":"enabling_technologies/WP3/EDI_SoC/#detailed-description","text":"Core enablers for reducing energy consumption and enabling mobile computing are System-on-Chip (SoC) and their extension Heterogeneous System-on-Chip (HSoC) technologies. The currently available embedded flight controller platforms mostly use the sequential processing paradigm, which is a limiting factor for the drone\u2019s onboard computational capacity. Our objective is to improve the SoA drone capabilities by using modern HSoCs, which reduce energy consumption and accelerate computationally intensive algorithms, e.g. localization based on visual SLAM. We focus on employing HSoCs that combine Field Programmable Gate Arrays (FPGA) and processor computing paradigms within a single chip. Hence MPU can be used for providing a high-level control and for processing non-localized (requiring intense scatter-gather data access) algorithms, while FPGA is dedicated to processing pipelines and tasks requiring parallel and/or determined execution. In the C4D project, EDI designs and contributes a Modular SoC-based embedded reference architecture that includes the following hardware and software components: MPSoC-based autonomous flight controller hardware , including a Zynq UltraScale+ ZU15EG System on Module (SoM) from Trenz Electronics with a custom carrier board tailored for use with midsize drones. FPGA vendor-agnostic IP cores for standardized communication with the accelerators and their run-time configuration. Linux modules for Direct Memory Access (DMA) engine control, virtual memory management, accessing accelerator configuration and providing user-space API. System architecture frameworks for implementing component-based software architectures and ensuring appropriate inter-communication mechanism, i.e. inside process, between systems, zero data copy.","title":"Detailed Description"},{"location":"enabling_technologies/WP3/EDI_SoC/#contribution-and-improvements","text":"EDI is designing a specialized carrier board PCB for the Ultrascale+ SoMs from Trenz, shown in Figure 34. The design involves exposing standard connectivity such as SPI, I2C, UART, QSPI for MMC as well as high-speed buses \u2013 USB2.0 and USB3.0. The reference platform is intended for but not limited to mid-sized drones. Further iterations should bring the size of the computing platform even drones of smaller indoor-type. Figure 34: Core component of the reference platform \u2013 Trenz Ultrascale+ SoM During the project, EDI plans to utilize the platform to deploy and validate the developed localization component - Hardware-accelerated Optical flow and SLAM (WP4-01) \u2013 and, hopefully, accelerators developed by other partners. The FPGA portion of the design utilizes standardized interfaces and vendor-agnostic DMA engines, which enable the unification of the driver modules (for high-performance HSoCs supplied by Xilinx or Intel). Another improvement is the system architecture implementation tools \u2013 compage1 (for component-based software management) and icom2 (for inter-component communication), both of which were originally developed under the PRYSTINE project (G.A. 783190) for high-performance controllers in autonomous cars, and during the C4D project, functionalities are extended for supporting drones.","title":"Contribution and Improvements"},{"location":"enabling_technologies/WP3/EDI_SoC/#design-and-implementation","text":"As the PCB design has not yet been finalized (at the writing of this document, the components are selected, the circuit is done but the efforts are directed towards the actual placement & routing and spatial requirement considerations), the other software and hardware (FPGA) related work is being done using prototyping platforms, i.e. Xilinx ZCU102 Ultrascale+ MPSoC. The on-chip system architecture, shown in Figure 33, represents the hardware and software portions of the MPSoC package, i.e. FPGA, kernel-space and user-space. At the hardware level, the selected communication interface for data exchange is based on the Direct Memory Access (DMA) engines, as it offloads MPU and is best suited for high data throughput [15]. The DMA engines utilize the standard AXI4 interface, which ensures compatibility with a collection of high-bandwidth cores. Although Figure 33 follows a shared memory model [16], the FPGA may also utilize dedicated FPGA DDR memory, as long as quick access to the data is not required by the software. DMA engine serves as a translator from AXI4 memory mapped to AXIS streaming interface, which is suited for pipelined data-driven circuits, i.e. accelerators. The configuration of the accelerators is achieved by exposing I/Os of the internal registers. A block \u201cRegmap to AXI-Lite\u201d translates access into a memory-mapped protocol, which further is used to interface with the MPU portion of the system. The handoff information to the software is passed in a standardized way (especially for ARM-based SoCs) \u2013 by utilizing Device Tree Blob (DTB) format. The autonomous flight controller runs a Linux operating system which assumes the incorporation of hardware abstraction mechanisms, most notably memory virtualization. These complexities are handled by Linux kernel modules that provide a user-space interface for DMA control and accelerator configuration. Notably, the memory virtualization mechanism results in a scattered physical layout of the memory. One option is to use scatter-gather memory access capabilities of the DMA engine. Nevertheless, this involves the creation of transaction descriptor tables (often a linked list) and additional memory access requests from another bus master apart from the MPU. We utilize kernel\u2019s Contiguous Memory Allocator [17] (CMA) feature, which marks the physical memory region and manages its run-time swaps whenever a new allocation takes place. The user space is supplied with a malloc-like interface, which handles memory management and contiguous page mapping. The user-space application follows a blackboard programming pattern [18], which is well-suited for research work. We provide two low-footprint open-source frameworks for the implementation of system architectures, the aforementioned compage and icom. Compage stores component configuration information in a separate Executable Linked Format (ELF) file segment, which provides a modular addition of new components (threads or processes). It also provides an INI-based file for configuring and replicating the software components. Icom, on the other hand, provides inter-component communication. It supports multiple communication paradigms (PUB-SUB, REQ-REP, PUSH-PULL) and can use the most appropriate communication mechanism (sockets, FIFO, message queues, etc.) depending on the communicating component location (distinct machines, same machine or same memory space).","title":"Design and Implementation"},{"location":"enabling_technologies/WP3/ENAC_paparazzi/","text":"WP3-13 - Paparazzi UAV ID WP3-13 Contributor ENAC Levels Platform Require C2LINK or Flight Plan definition Provide Modular and flexible UAV autopilot system for testing and operation Input Depending on the flight mode: - flight plan definition, - mission sequence from datalink, - position/speed from datalink, - low-level remote control Output Stabilized aircraft (fixedwing, multicopter, hybrid) on a given trajectory or attitude C4D building block Flight control, Actuation, + parts of Perception and Communication TRL 4 to 6 Current status Paparazzi is a complete system of open source hardware and software for Unmanned Aircraft Systems (UAS), including both the airborne autopilot as well as complete ground station mission planning and monitoring software utilizing a bi-directional datalink for telemetry and control. Paparazzi has been created at ENAC in 2003, and is now supported by other institutes such as MAVLAB of the TU-Delft, individual developers, and some private UAV companies from several countries. The Paparazzi system was initially designed for robust small fixed-wing aircrafts in 2003, but it now supports several other configurations and concepts such as high-aspect ratio gliders, multi-rotors, transitioning vehicles, and rovers. The communication between the software blocks running on the ground and the airborne autopilot is based on the PPRZLINK library, which provides API in C/C++, Python and OCaml. Figure 46: Paparazzi communication architecture Figure 47: Control options for Paparazzi UAVs See the Wiki and the Github repository . Contribution and Improvements Several improvements are intended in the scope of C4D projects. Improvement of the internal airborne code organization: a detailed analysis of the timing between the elements of the embedded software have led to a new definition of the internal tasks\u2019 groups. A particular attention has been given to the timing between the tasks and functions to provide a stable and reliable end-to-end execution time. Explicit definition of the dependencies between modules, both at functional and logic level: it allows to simplify the configuration of an aircraft (only relevant high-level modules needs to be specified), the dependency solver will include all the required modules. The overall system is also more robust as it can detect circular dependencies, conflicts and missing functionalities. Improvements of the static scheduling for the telemetry messages to spread the link loading over time and avoid buffer saturation and delays. The same approach can be applied to the scheduling of some of the functions calls inside the different tasks of the autopilot, linked to the new code organization mentioned in point 1. Performance analysis have been carried out. Some results are presented in the following figures. The first two are the different tasks and timing for the legacy architecture for fixedwing and rotorcraft firmwares. It can be seen that the group of tasks are not harmonized and don\u2019t reflect the actual functional blocks of the system. If most of the timing are respected, the \u2018event\u2019 polling function that is expected to run at 10kHz is a bit slower than expected. Figure 48: Performance analysis of fixed-wing legacy firmware at 100Hz Figure 49: Performance analysis of rotorcraft legacy firmware at 1000Hz The same work with the new architecture shows that the tasks are (almost) harmonized and reflect the reference architecture presented in the next section. The timings are all aligned with the base frequency and the \u2018event\u2019 polling function is called at the expected rate. What is not visible but is a result of this work is that the time between the sensor task (start reading and getting data from digital sensors) and the rest of the guidance and control loop is fixed and provides a correct sequencing of the data flow regardless of the user configuration. Figure 50: Performance analysis of new fixed-wing legacy firmware at 100Hz Figure 51: Performance analysis of new rotorcraft legacy firmware at 1000Hz Design and Implementation The new architecture corresponds to the architecture shown below. The grey boxes are tasks grouping related functionalities: sensors, estimation, control, actuators, payload, communication, etc. The components of the systems, known as modules, are described and configured thanks to an XML file, providing the relevant information: module name, task group documentation dependencies (required modules and functionalities, provided functionalities, conflicts) initialization, periodic and event based functions to be called source files and compilation flags testing flags for unit tests The module generator is using a topological tree search algorithm to solve the module dependencies, and then is generating C code for calling the functions, including the static schedulers for the periodic functions according to the system base frequency. The final sequence is controlled by the static dispatcher to guarantee that each task is called within a predefined order. Figure 52: Paparazzi airborne architecture","title":"Paparazzi"},{"location":"enabling_technologies/WP3/ENAC_paparazzi/#wp3-13-paparazzi-uav","text":"ID WP3-13 Contributor ENAC Levels Platform Require C2LINK or Flight Plan definition Provide Modular and flexible UAV autopilot system for testing and operation Input Depending on the flight mode: - flight plan definition, - mission sequence from datalink, - position/speed from datalink, - low-level remote control Output Stabilized aircraft (fixedwing, multicopter, hybrid) on a given trajectory or attitude C4D building block Flight control, Actuation, + parts of Perception and Communication TRL 4 to 6","title":"WP3-13 - Paparazzi UAV"},{"location":"enabling_technologies/WP3/ENAC_paparazzi/#current-status","text":"Paparazzi is a complete system of open source hardware and software for Unmanned Aircraft Systems (UAS), including both the airborne autopilot as well as complete ground station mission planning and monitoring software utilizing a bi-directional datalink for telemetry and control. Paparazzi has been created at ENAC in 2003, and is now supported by other institutes such as MAVLAB of the TU-Delft, individual developers, and some private UAV companies from several countries. The Paparazzi system was initially designed for robust small fixed-wing aircrafts in 2003, but it now supports several other configurations and concepts such as high-aspect ratio gliders, multi-rotors, transitioning vehicles, and rovers. The communication between the software blocks running on the ground and the airborne autopilot is based on the PPRZLINK library, which provides API in C/C++, Python and OCaml. Figure 46: Paparazzi communication architecture Figure 47: Control options for Paparazzi UAVs See the Wiki and the Github repository .","title":"Current status"},{"location":"enabling_technologies/WP3/ENAC_paparazzi/#contribution-and-improvements","text":"Several improvements are intended in the scope of C4D projects. Improvement of the internal airborne code organization: a detailed analysis of the timing between the elements of the embedded software have led to a new definition of the internal tasks\u2019 groups. A particular attention has been given to the timing between the tasks and functions to provide a stable and reliable end-to-end execution time. Explicit definition of the dependencies between modules, both at functional and logic level: it allows to simplify the configuration of an aircraft (only relevant high-level modules needs to be specified), the dependency solver will include all the required modules. The overall system is also more robust as it can detect circular dependencies, conflicts and missing functionalities. Improvements of the static scheduling for the telemetry messages to spread the link loading over time and avoid buffer saturation and delays. The same approach can be applied to the scheduling of some of the functions calls inside the different tasks of the autopilot, linked to the new code organization mentioned in point 1. Performance analysis have been carried out. Some results are presented in the following figures. The first two are the different tasks and timing for the legacy architecture for fixedwing and rotorcraft firmwares. It can be seen that the group of tasks are not harmonized and don\u2019t reflect the actual functional blocks of the system. If most of the timing are respected, the \u2018event\u2019 polling function that is expected to run at 10kHz is a bit slower than expected. Figure 48: Performance analysis of fixed-wing legacy firmware at 100Hz Figure 49: Performance analysis of rotorcraft legacy firmware at 1000Hz The same work with the new architecture shows that the tasks are (almost) harmonized and reflect the reference architecture presented in the next section. The timings are all aligned with the base frequency and the \u2018event\u2019 polling function is called at the expected rate. What is not visible but is a result of this work is that the time between the sensor task (start reading and getting data from digital sensors) and the rest of the guidance and control loop is fixed and provides a correct sequencing of the data flow regardless of the user configuration. Figure 50: Performance analysis of new fixed-wing legacy firmware at 100Hz Figure 51: Performance analysis of new rotorcraft legacy firmware at 1000Hz","title":"Contribution and Improvements"},{"location":"enabling_technologies/WP3/ENAC_paparazzi/#design-and-implementation","text":"The new architecture corresponds to the architecture shown below. The grey boxes are tasks grouping related functionalities: sensors, estimation, control, actuators, payload, communication, etc. The components of the systems, known as modules, are described and configured thanks to an XML file, providing the relevant information: module name, task group documentation dependencies (required modules and functionalities, provided functionalities, conflicts) initialization, periodic and event based functions to be called source files and compilation flags testing flags for unit tests The module generator is using a topological tree search algorithm to solve the module dependencies, and then is generating C code for calling the functions, including the static schedulers for the periodic functions according to the system base frequency. The final sequence is controlled by the static dispatcher to guarantee that each task is called within a predefined order. Figure 52: Paparazzi airborne architecture","title":"Design and Implementation"},{"location":"enabling_technologies/WP3/ENSMA_1/","text":"WP3-14_1 \u2013 Control component for implementation of potential barriers ID WP3-14_1 Contributor ENSMA Levels Function Require Proximity sensor Provide Geofencing, obstacle detection and avoidance, geofence preservation, safe trajectory Input X, Y, Z coordinates of the drone Roll, pitch and yaw angles of the drone Distance to the obstacle from sensor proximity detection Geofence virtual boundary information Output Required X, Y, Z torques and thrust or rotors\u2019 speed C4D building block Geo-fencing and geo-awareness TRL 3 Figure 53: Building Block diagram for WP3-14_1 Detailed Description The growing popularity of small civilian drones has generated a wide array of complex and unprecedented challenges related to the risks posed to security, safety, potential to disrupt people\u2019s privacy and interfere with activities on the ground and with the manned aircrafts. This necessitates the development of technologies that allow UAVs to safely navigate according to the regulations. Geofencing technique can be used to specify no-fly zones for drones. It is a technique that defines virtual boundaries in a specific geographical area. Once these virtual boundaries are defined, the drone should never be able to penetrate through these boundaries. In other words there should be a potential barrier that prevents drones from crossing the geofence boundary and redirect their trajectory to avoid any conflict. The drones must not only respect the geofence but should also be capable of detecting and avoiding any obstacle in their path. The obstacles could be stationary (e.g., buildings) or moving objects (e.g. birds, other drones or manned aircrafts). A drone could either be autonomous or remotely controlled by an operator. In both cases, the potential barriers must repel the drone automatically and generate a warning of alert whenever it is near the restricted area or any obstacle in path that could cause a collision. Figure 54: Geofencing example Specifications and contribution The purpose of this component is to provide a mechanism which ensures that the collision between the drones and obstacles never happens and geofence is always preserved. From a technical viewpoint, preventing drones from violating the boundaries defined by the geofence system can be considered as a constrained control problem. Constrained control addresses the problem of enforcing constraints satisfaction at all times while ensuring that control objectives are achieved. It is required that the geofence information is provided to the drone and that information could be updated online while the drone is in operation. In order to avoid collision with an obstacle, it is assumed that drone is equipped with proximity sensors and can detect any relative position of both non-cooperative and cooperative entities within a sensing range. A cooperative entity is another drone which has the same capability. In contrary, a non-cooperative entity is an entity without collision avoidance system. Sensing capability is required to sense the presence of any other entities in its close vicinity which may lead to a collision. These sensors only give the relative position of any entity in its range in the local frame and do not provide position information in the global frame. It is to note that this assumption is used for the purpose of collision avoidance only. Design and Implementation A potential barrier for geofencing and collision avoidance can be achieved using Artificial Potential Function (APF). In APF, a drone is considered as a point in a potential field. This drone experiences a repulsion force from the obstacles or geofence and therefore, instead of colliding with them, it steers away from them. Typically, potential functions are based on the relative distance between drone and the obstacle or the geofence and do not require any global information. Based on the practical aspects, an ideal potential function must have the following properties: The range of the potential field must be bounded. Usually, it depends on the range of obstacle sensors mounted on the agent. The value of the potential field and the corresponding repulsion must be infinity at the boundary of the obstacle/geofence and must decrease with the increase in the distance First and second derivatives of the potential function must exist in order to have a smooth repulsion force APF based repulsion mechanism is combined with the control algorithm, for instance it could be combined with the position control of the drone. The repulsive force remains zero when the distance is greater than some predefined value and position controller works normally. However, when the distance becomes less than the threshold, the repulsive force comes into play and lesser the distance more will be the repulsive force. Figure 55 shows a graph of distance based potential function. Figure 55: Potential function for collision avoidance and geo fencing The detection range is considered as 2 meters and the protection radius around the drone is 0.5 meters. It can be observed that the value of the potential field increases as the distance between the fence/obstacle and the drone. This will act as the repulsive force and push the drone away from the fence/obstacle.","title":"ENSMA - Control components that implement potential barriers"},{"location":"enabling_technologies/WP3/ENSMA_1/#wp3-14_1-control-component-for-implementation-of-potential-barriers","text":"ID WP3-14_1 Contributor ENSMA Levels Function Require Proximity sensor Provide Geofencing, obstacle detection and avoidance, geofence preservation, safe trajectory Input X, Y, Z coordinates of the drone Roll, pitch and yaw angles of the drone Distance to the obstacle from sensor proximity detection Geofence virtual boundary information Output Required X, Y, Z torques and thrust or rotors\u2019 speed C4D building block Geo-fencing and geo-awareness TRL 3 Figure 53: Building Block diagram for WP3-14_1","title":"WP3-14_1 \u2013 Control component for implementation of potential barriers"},{"location":"enabling_technologies/WP3/ENSMA_1/#detailed-description","text":"The growing popularity of small civilian drones has generated a wide array of complex and unprecedented challenges related to the risks posed to security, safety, potential to disrupt people\u2019s privacy and interfere with activities on the ground and with the manned aircrafts. This necessitates the development of technologies that allow UAVs to safely navigate according to the regulations. Geofencing technique can be used to specify no-fly zones for drones. It is a technique that defines virtual boundaries in a specific geographical area. Once these virtual boundaries are defined, the drone should never be able to penetrate through these boundaries. In other words there should be a potential barrier that prevents drones from crossing the geofence boundary and redirect their trajectory to avoid any conflict. The drones must not only respect the geofence but should also be capable of detecting and avoiding any obstacle in their path. The obstacles could be stationary (e.g., buildings) or moving objects (e.g. birds, other drones or manned aircrafts). A drone could either be autonomous or remotely controlled by an operator. In both cases, the potential barriers must repel the drone automatically and generate a warning of alert whenever it is near the restricted area or any obstacle in path that could cause a collision. Figure 54: Geofencing example","title":"Detailed Description"},{"location":"enabling_technologies/WP3/ENSMA_1/#specifications-and-contribution","text":"The purpose of this component is to provide a mechanism which ensures that the collision between the drones and obstacles never happens and geofence is always preserved. From a technical viewpoint, preventing drones from violating the boundaries defined by the geofence system can be considered as a constrained control problem. Constrained control addresses the problem of enforcing constraints satisfaction at all times while ensuring that control objectives are achieved. It is required that the geofence information is provided to the drone and that information could be updated online while the drone is in operation. In order to avoid collision with an obstacle, it is assumed that drone is equipped with proximity sensors and can detect any relative position of both non-cooperative and cooperative entities within a sensing range. A cooperative entity is another drone which has the same capability. In contrary, a non-cooperative entity is an entity without collision avoidance system. Sensing capability is required to sense the presence of any other entities in its close vicinity which may lead to a collision. These sensors only give the relative position of any entity in its range in the local frame and do not provide position information in the global frame. It is to note that this assumption is used for the purpose of collision avoidance only.","title":"Specifications and contribution"},{"location":"enabling_technologies/WP3/ENSMA_1/#design-and-implementation","text":"A potential barrier for geofencing and collision avoidance can be achieved using Artificial Potential Function (APF). In APF, a drone is considered as a point in a potential field. This drone experiences a repulsion force from the obstacles or geofence and therefore, instead of colliding with them, it steers away from them. Typically, potential functions are based on the relative distance between drone and the obstacle or the geofence and do not require any global information. Based on the practical aspects, an ideal potential function must have the following properties: The range of the potential field must be bounded. Usually, it depends on the range of obstacle sensors mounted on the agent. The value of the potential field and the corresponding repulsion must be infinity at the boundary of the obstacle/geofence and must decrease with the increase in the distance First and second derivatives of the potential function must exist in order to have a smooth repulsion force APF based repulsion mechanism is combined with the control algorithm, for instance it could be combined with the position control of the drone. The repulsive force remains zero when the distance is greater than some predefined value and position controller works normally. However, when the distance becomes less than the threshold, the repulsive force comes into play and lesser the distance more will be the repulsive force. Figure 55 shows a graph of distance based potential function. Figure 55: Potential function for collision avoidance and geo fencing The detection range is considered as 2 meters and the protection radius around the drone is 0.5 meters. It can be observed that the value of the potential field increases as the distance between the fence/obstacle and the drone. This will act as the repulsive force and push the drone away from the fence/obstacle.","title":"Design and Implementation"},{"location":"enabling_technologies/WP3/ENSMA_2/","text":"WP3-14_2 - Multi-agent swarm control ID WP3-14_2 Contributor ENSMA Levels Functional Require Communication service to obtain data from the neighbours Provide Control algorithm for swarm formation control and cooperation, Inter-agent collision avoidance, Obstacle detection and avoidance, as well as mathematical stability proof of formation tracking Input X, Y, Z coordinates of the drone Roll, pitch and yaw angles of the drone X, Y, Z coordinates of neighbours Formation vector Possibly proximity sensors Output Required roll and pitch angles and thrust if used standalone, or rotors\u2019 speed if used jointly with the attitude controller C4D building block Flight guidance if standalone, both Flight guidance and Flight control if used jointly with the attitude controller TRL 4 Figure 56: Building Block diagram for WP3-14_2 Detailed Description The effectiveness of the drones can be further improved by utilizing them in a cooperative manner to accomplish much more complex tasks which cannot be realized by solo operation. Control of such cooperative teamwork of Multi-Agent Systems (MAS) heavily depends on communication and information exchange among the agents. This gives rise to two natural choices of communication network which are centralised and distributed control network. In centralized communication, as apparent from the name, there is a central unit to which all the agents are connected to and send their information. The central station handles all the information exchange and sends control commands to the agents. This kind of scheme has some major drawbacks. For example, if the central unit fails, the whole network will collapse. Moreover, a centralized control scheme cannot handle large number of agents due to network saturation and/or limited computing and processing capability etc. On the other hand, a distributed control scheme does not require a central control unit. Agents communicate directly with their neighbouring agents and exchange information. The information received from neighbours is also called local information. Each agent in the distributed network uses this local information to compute its own control input. Distributed control schemes offer many advantages over the centralized control scheme in terms of efficiency, adaptability, robustness, and scalability. In many practical scenarios, it is required that the agents of MAS create and maintain a desired geometric shape. The required shape could either be fixed or time varying. In some cases, it is further required that the agents follow a trajectory while maintaining the shape. The reference trajectory is produced by a virtual or real leader. This is known as formation tracking. Contribution and Improvements The main goal of this enabling technology is to offer a distributed formation tracking controller for a swarm of drones. From the state of art, it is clear that the available cooperative control schemes do not take various practical limitations in-to account. Motivated by this, we focused on the design and implementation of distributed cooperative control laws for a swarm of drones with communication and sensor constraints described below in details. Generally, formation controller requires both position and velocity of the neighbours for its computation. However, it is not always possible to measure all the states in real applications. Moreover, sending partial information consumes less bandwidth and thus is very cost effective. Therefore, it is considered that only position information is shared between the agents. We consider that velocity and control input of neighbours are not available. The data is transmitted between the agents at irregular and asynchronous time instants. Here, asynchronous means that the time of the transmission of the position data by one agent does not depend on any other agent in the network. In other words, an agent can receive information from its neighbours at different time instants. In fact, having irregular and asynchronous time periods in any practical case is totally inevitable. Another constraint that is taken into account is that the communication between the agents could either be directed or undirected. Directed communication means that if agent i can receive information from agent j then it does not imply that agent j can receive information from agent i. Only a few agents in the network have the access to the reference trajectory produced by the virtual or real leader Since inter-agent collision is another important issue in formation tracking control, a potential function based collision avoidance algorithm is incorporated with the proposed formation tracking controller. The collision avoidance algorithm ensures that the drones converge to produce the desired geometric shape without colliding with each other. The desired geometric shape can be defined by a formation vector. Design and Implementation Since the drone is an under-actuated system (e.g. quadrotor has 6 states and 4 controllable rotor speeds), the design of controller is not straightforward. To achieve the desired formation of a swarm of drones, the overall dynamics of the drone is divided into two blocks namely position dynamics and attitude dynamics with a strong nonlinear coupling among them. The idea is to introduce an auxiliary position controller and design it while keeping all the above mentioned constraints in mind. The required thrust and attitude then can be computed through this auxiliary position controller. An attitude controller then can be designed to achieve the required attitude by the drone and which makes the drone to reach to the required position to achieve the desired formation shape. Figure 57 shows the block diagram of the overall control architecture of one drone to achieve formation tracking of the swarm of drones. The position controller computes auxiliary position controller which is then used to compute desired roll and pitch through nonlinear decoupling equations. These desired roll and pitch angles act as a reference input to the attitude controller which computes the required torques to track these desired angles. Figure 57: Overall control scheme for formation tracking Formation controller: The basic idea is to use a classical continuous linear consensus controller for MAS of second order dynamics. By introducing the auxiliary controller, the position dynamics can be represented as double integrator. However, trivial formation controller requires both position and velocity in continuous time. A continuous-discrete-time high-gain observer is used to compute both position and velocity from available discrete position data. Each follower has a local observer which not only estimates the agent\u2019s own states but also the states of its neighbours. The structure of the proposed observer-based formation tracking algorithm also has some advantages when it comes to communication delays and data packet dropouts. As each controller is using the estimated states provided by the local observer, by time stamping the measured position data, the estimation can be provided as soon as the data is received, even with a delay, by compensating it through increasing the computation speed of the observer. Moreover, if the information packet is lost during communication, the observer could still provide an estimation. An Artificial Potential Function (APF) based repulsive controller is also incorporated with the formation controller to avoid any kind of collision among the agents and with obstacles. APF provides a repulsive force whenever the distance between two bodies becomes less than some threshold. It is considered that each drone is equipped with some kind of proximity sensor to detect anything in its path. Attitude controller: For the Matlab simulations, the desired attitude is computed through the auxiliary controller and then a finite-time attitude controller is used to compute the required torques to achieve this desired attitude. Simulations: The designed control scheme is simulated for a group of one leader and three follower quadrotors where followers are required to make a triangle around the leader in xy-plane and maintain the same altitude as of the leader. Figure 58 illustrates the formation with a stationary leader while Figure 59 shows formation tracking results with a moving leader. The threshold distance for the repulsive force is considered as 2 meters while the protection radius around each drone is 0.5 meter. Figure 60 shows that the inter-agent distance always remains greater than 1 meter which implies that the agents never collide during the whole process. Figure 58: Formation tracking with stationary leader, distances in meters Figure 59: Formation tracking with a moving leader, distances in meters Figure 60: Inter-drone distance, in meters The simulation results show the stability of the system with the designed algorithm for formation tracking. However, simulation results are not sufficient to prove the stability of the system. Therefore, a rigorous mathematical proof has been also carried out to ensure the formation tracking and stability of the closed loop system with the designed controller.","title":"ENSMA - Multi-agent swarm control"},{"location":"enabling_technologies/WP3/ENSMA_2/#wp3-14_2-multi-agent-swarm-control","text":"ID WP3-14_2 Contributor ENSMA Levels Functional Require Communication service to obtain data from the neighbours Provide Control algorithm for swarm formation control and cooperation, Inter-agent collision avoidance, Obstacle detection and avoidance, as well as mathematical stability proof of formation tracking Input X, Y, Z coordinates of the drone Roll, pitch and yaw angles of the drone X, Y, Z coordinates of neighbours Formation vector Possibly proximity sensors Output Required roll and pitch angles and thrust if used standalone, or rotors\u2019 speed if used jointly with the attitude controller C4D building block Flight guidance if standalone, both Flight guidance and Flight control if used jointly with the attitude controller TRL 4 Figure 56: Building Block diagram for WP3-14_2","title":"WP3-14_2 - Multi-agent swarm control"},{"location":"enabling_technologies/WP3/ENSMA_2/#detailed-description","text":"The effectiveness of the drones can be further improved by utilizing them in a cooperative manner to accomplish much more complex tasks which cannot be realized by solo operation. Control of such cooperative teamwork of Multi-Agent Systems (MAS) heavily depends on communication and information exchange among the agents. This gives rise to two natural choices of communication network which are centralised and distributed control network. In centralized communication, as apparent from the name, there is a central unit to which all the agents are connected to and send their information. The central station handles all the information exchange and sends control commands to the agents. This kind of scheme has some major drawbacks. For example, if the central unit fails, the whole network will collapse. Moreover, a centralized control scheme cannot handle large number of agents due to network saturation and/or limited computing and processing capability etc. On the other hand, a distributed control scheme does not require a central control unit. Agents communicate directly with their neighbouring agents and exchange information. The information received from neighbours is also called local information. Each agent in the distributed network uses this local information to compute its own control input. Distributed control schemes offer many advantages over the centralized control scheme in terms of efficiency, adaptability, robustness, and scalability. In many practical scenarios, it is required that the agents of MAS create and maintain a desired geometric shape. The required shape could either be fixed or time varying. In some cases, it is further required that the agents follow a trajectory while maintaining the shape. The reference trajectory is produced by a virtual or real leader. This is known as formation tracking.","title":"Detailed Description"},{"location":"enabling_technologies/WP3/ENSMA_2/#contribution-and-improvements","text":"The main goal of this enabling technology is to offer a distributed formation tracking controller for a swarm of drones. From the state of art, it is clear that the available cooperative control schemes do not take various practical limitations in-to account. Motivated by this, we focused on the design and implementation of distributed cooperative control laws for a swarm of drones with communication and sensor constraints described below in details. Generally, formation controller requires both position and velocity of the neighbours for its computation. However, it is not always possible to measure all the states in real applications. Moreover, sending partial information consumes less bandwidth and thus is very cost effective. Therefore, it is considered that only position information is shared between the agents. We consider that velocity and control input of neighbours are not available. The data is transmitted between the agents at irregular and asynchronous time instants. Here, asynchronous means that the time of the transmission of the position data by one agent does not depend on any other agent in the network. In other words, an agent can receive information from its neighbours at different time instants. In fact, having irregular and asynchronous time periods in any practical case is totally inevitable. Another constraint that is taken into account is that the communication between the agents could either be directed or undirected. Directed communication means that if agent i can receive information from agent j then it does not imply that agent j can receive information from agent i. Only a few agents in the network have the access to the reference trajectory produced by the virtual or real leader Since inter-agent collision is another important issue in formation tracking control, a potential function based collision avoidance algorithm is incorporated with the proposed formation tracking controller. The collision avoidance algorithm ensures that the drones converge to produce the desired geometric shape without colliding with each other. The desired geometric shape can be defined by a formation vector.","title":"Contribution and Improvements"},{"location":"enabling_technologies/WP3/ENSMA_2/#design-and-implementation","text":"Since the drone is an under-actuated system (e.g. quadrotor has 6 states and 4 controllable rotor speeds), the design of controller is not straightforward. To achieve the desired formation of a swarm of drones, the overall dynamics of the drone is divided into two blocks namely position dynamics and attitude dynamics with a strong nonlinear coupling among them. The idea is to introduce an auxiliary position controller and design it while keeping all the above mentioned constraints in mind. The required thrust and attitude then can be computed through this auxiliary position controller. An attitude controller then can be designed to achieve the required attitude by the drone and which makes the drone to reach to the required position to achieve the desired formation shape. Figure 57 shows the block diagram of the overall control architecture of one drone to achieve formation tracking of the swarm of drones. The position controller computes auxiliary position controller which is then used to compute desired roll and pitch through nonlinear decoupling equations. These desired roll and pitch angles act as a reference input to the attitude controller which computes the required torques to track these desired angles. Figure 57: Overall control scheme for formation tracking Formation controller: The basic idea is to use a classical continuous linear consensus controller for MAS of second order dynamics. By introducing the auxiliary controller, the position dynamics can be represented as double integrator. However, trivial formation controller requires both position and velocity in continuous time. A continuous-discrete-time high-gain observer is used to compute both position and velocity from available discrete position data. Each follower has a local observer which not only estimates the agent\u2019s own states but also the states of its neighbours. The structure of the proposed observer-based formation tracking algorithm also has some advantages when it comes to communication delays and data packet dropouts. As each controller is using the estimated states provided by the local observer, by time stamping the measured position data, the estimation can be provided as soon as the data is received, even with a delay, by compensating it through increasing the computation speed of the observer. Moreover, if the information packet is lost during communication, the observer could still provide an estimation. An Artificial Potential Function (APF) based repulsive controller is also incorporated with the formation controller to avoid any kind of collision among the agents and with obstacles. APF provides a repulsive force whenever the distance between two bodies becomes less than some threshold. It is considered that each drone is equipped with some kind of proximity sensor to detect anything in its path. Attitude controller: For the Matlab simulations, the desired attitude is computed through the auxiliary controller and then a finite-time attitude controller is used to compute the required torques to achieve this desired attitude. Simulations: The designed control scheme is simulated for a group of one leader and three follower quadrotors where followers are required to make a triangle around the leader in xy-plane and maintain the same altitude as of the leader. Figure 58 illustrates the formation with a stationary leader while Figure 59 shows formation tracking results with a moving leader. The threshold distance for the repulsive force is considered as 2 meters while the protection radius around each drone is 0.5 meter. Figure 60 shows that the inter-agent distance always remains greater than 1 meter which implies that the agents never collide during the whole process. Figure 58: Formation tracking with stationary leader, distances in meters Figure 59: Formation tracking with a moving leader, distances in meters Figure 60: Inter-drone distance, in meters The simulation results show the stability of the system with the designed algorithm for formation tracking. However, simulation results are not sufficient to prove the stability of the system. Therefore, a rigorous mathematical proof has been also carried out to ensure the formation tracking and stability of the closed loop system with the designed controller.","title":"Design and Implementation"},{"location":"enabling_technologies/WP3/HIB/","text":"WP3-04 - Computer Vision Components for drones ID WP3-04 Contributor HIB Levels Functional Require Payload data (drone images) Provide DL-based SW to detect and classify objects (road elements, traffic signs) from images captured by drones Input RGB images captured by drone (Payload data) Output Inventory of road elements with their position in the terrain C4D building block (Video) Data Analytics TRL 5 Detailed Description Machine learning and deep learning are an arising approach in dealing with large amount of data gained from drones [19]. For infrastructure planning and design, typical data acquired through drones are images. For construction monitoring, either real time videos or 3D models are needed. Focusing on object recognition and detection in aerial images captured by drones, a major challenge with the integration of artificial intelligence and machine learning with autonomous drones\u2019 operations is that these tasks are not executable in real-time or near-real-time due to the complexities of these tasks and their computational costs. In the last few years, deep convolutional neural networks have shown to be a reliable approach for image object detection and classification due to their relatively high accuracy and speed. Furthermore, a CNN algorithm enables drones to convert object information from the immediate environment into abstract information that can be interpreted by machines without human interference. Thereby, the main advantage of CNN algorithms is that they can detect and classify objects while being computationally less expensive and superior in performance when compared with other machine-learning methods. In the C4D project scope, the computer vision component for drones brings a deep learning-based software which uses a convolutional neural network (CNN) algorithm to detect, and classify objects from raw data in such a way that it brings to the project capabilities of interpreting surroundings and detecting scenarios from data captured with drones. Contribution and Improvements Concretely, the proposed application scenario for the computer vision component for drones is the inspection, assessment and maintenance of civil infrastructures and construction elements, in which many inventories for damage and defects detection are carried out. Such inspections and the corresponding inventories are performed through human visual observations, being a tedious and time-consuming work prone to human errors, and therefore, an expensive work, so accelerating such inspections is a current challenge in the construction industry. Then, the Computer vision component for drones is a post-processing computer vision system based on previously trained CNN algorithms which enables the auto-detection and geo-referencing of different objects from RGB images captured by the drones\u2019 on-board camera. Particularly, this computer vision system intends to improve the digitalization of the state of the constructive process of a Civil Infrastructure by auto-detecting and geo-referencing different road elements which can be found in any civil infrastructure or construction elements. Thereby, one of the main challenges in the construction industry like the road element inventory realization is solved. Figure 36: Computer Vision Component in the application scenario Considering the C4D Reference Architecture Context, the computer vision component for drones supports the (Video) Data Analytics building block in the Data Management block by performing an offline data analysis over the RGB images (that is, payload data) captured by the UAV in order to provide an auto-detection and geo-referencing of different objects. Although this computer vision component for drones will provide to the Business domain an inventory of the road elements detected and their corresponding position in the terrain to serve as input for the creation of the BIM model, this computer vision component for drones could be extended for other business functions following the generic Building Block defined for the Payload Data Analytics. Design and Implementation The computer vision component for drones is being developed following the typical steps for a deep-learning-based SW: Dataset creation Election of the most accurate object detection model. Training of the object detection model using previously created dataset. Testing of the object detection model already trained. So, it is possible to state that the computer vision component for drones is built upon two main axes: the dataset and the object detection model. For the dataset creation, several available datasets have been contemplated: German Traffic Sign Detection Benchmark (GTSDB) [20], which is a single-image detection dataset based on images taken in Germany. It contains around 900 images, 600 for training and 300 for test. The traffic sign classes in this dataset are shown below: Figure 37: Traffic sign classes in GTSDB Images from drone flight videos. This dataset is built with the images captured during the 1st data acquisition campaign performed in UC2 \u2013 Construction in Ja\u00e9n (Spain). An example is shown below: Figure 38: Example of images from drone flight videos Dataset built from the videos generated with AirSim (drone simulation environment) . Figure 39: Example of images from drone simulated flight videos Mapillary Traffic Sign Dataset (MTSD) [21], which it is a dataset with street-level images around the world. These images are annotated with bounding boxes and traffic sign classes. There are 100000 high-resolution images, 52000 of them fully annotated. Also, there are over 300 traffic sign classes, as it is shown below. Figure 40: Overview of all traffic sign classes in MTSD Related to the object detection model, Faster R-CNN [22] is the model used to date, and it is an object detection system proposed by Ren et al. in 2015. Its architecture is shown in Figure 41 and it is formed by two components: Region Proposal Network and Fast Region-Convolutional Neural Network. Figure 41: Faster R-CNN model general architecture Concretely, Region Proposal Network (RPN) is a deep convolutional network that proposes image regions where an object might be found. RPN component acts as an attention mechanism that accelerates region proposal phase, and in our use case, it creates new regions (bounding boxes) where a traffic signal exists. Complementarily, the Fast Region-Convolutional Neural Network (Fast R-CNN) [23] takes the regions proposed by RPN and classifies them efficiently. In our use case, this component classifies traffic signal regions in \u2018Mandatory\u2019, \u2018Danger\u2019, \u2018Forbidden\u2019. Below, it is shown an implementation conceptual diagram for the computer vision component for drones. Figure 42: Implementation conceptual diagram for computer vision component Based on Figure 42, the data flow of computer vision component for drones is detailed as follows: 1. Setup of all prerequisites: software libraries are installed and loaded, and dataset and pretrained model are downloaded. 2. The pretrained Faster R-CNN model is loaded into memory using TensorFlow Object Detection API [24] 3. The label map is loaded into memory. This object maps the three category indices to category names for the translation of the model\u2019s predictions. 4. The pretrained model performs detection over test set images. 1. Each image is prepared with Pillow library. 2. Images are transformed into Numpy arrays. 3. Images are preprocessed for the model to make predictions on them. 4. Detection takes place. 5. Predictions are visualized with TensorFlow Object Detection API and Matplotlib\u2019s Pyplot module. Below, it is possible to see traffic sign detections on images from real Drone Flight Videos by the Faster R-CNN model implemented in the computer vision component for drones. Figure 43: Detections performed by Faster R-CNN on a Drone Flight image. (top) The model fails to perform any detection over the whole image. (bottom) After chopping the image, the model correctly detects and identifies the four traffic signs.","title":"HIB - Computer Vision Components for drones"},{"location":"enabling_technologies/WP3/HIB/#wp3-04-computer-vision-components-for-drones","text":"ID WP3-04 Contributor HIB Levels Functional Require Payload data (drone images) Provide DL-based SW to detect and classify objects (road elements, traffic signs) from images captured by drones Input RGB images captured by drone (Payload data) Output Inventory of road elements with their position in the terrain C4D building block (Video) Data Analytics TRL 5","title":"WP3-04 - Computer Vision Components for drones"},{"location":"enabling_technologies/WP3/HIB/#detailed-description","text":"Machine learning and deep learning are an arising approach in dealing with large amount of data gained from drones [19]. For infrastructure planning and design, typical data acquired through drones are images. For construction monitoring, either real time videos or 3D models are needed. Focusing on object recognition and detection in aerial images captured by drones, a major challenge with the integration of artificial intelligence and machine learning with autonomous drones\u2019 operations is that these tasks are not executable in real-time or near-real-time due to the complexities of these tasks and their computational costs. In the last few years, deep convolutional neural networks have shown to be a reliable approach for image object detection and classification due to their relatively high accuracy and speed. Furthermore, a CNN algorithm enables drones to convert object information from the immediate environment into abstract information that can be interpreted by machines without human interference. Thereby, the main advantage of CNN algorithms is that they can detect and classify objects while being computationally less expensive and superior in performance when compared with other machine-learning methods. In the C4D project scope, the computer vision component for drones brings a deep learning-based software which uses a convolutional neural network (CNN) algorithm to detect, and classify objects from raw data in such a way that it brings to the project capabilities of interpreting surroundings and detecting scenarios from data captured with drones.","title":"Detailed Description"},{"location":"enabling_technologies/WP3/HIB/#contribution-and-improvements","text":"Concretely, the proposed application scenario for the computer vision component for drones is the inspection, assessment and maintenance of civil infrastructures and construction elements, in which many inventories for damage and defects detection are carried out. Such inspections and the corresponding inventories are performed through human visual observations, being a tedious and time-consuming work prone to human errors, and therefore, an expensive work, so accelerating such inspections is a current challenge in the construction industry. Then, the Computer vision component for drones is a post-processing computer vision system based on previously trained CNN algorithms which enables the auto-detection and geo-referencing of different objects from RGB images captured by the drones\u2019 on-board camera. Particularly, this computer vision system intends to improve the digitalization of the state of the constructive process of a Civil Infrastructure by auto-detecting and geo-referencing different road elements which can be found in any civil infrastructure or construction elements. Thereby, one of the main challenges in the construction industry like the road element inventory realization is solved. Figure 36: Computer Vision Component in the application scenario Considering the C4D Reference Architecture Context, the computer vision component for drones supports the (Video) Data Analytics building block in the Data Management block by performing an offline data analysis over the RGB images (that is, payload data) captured by the UAV in order to provide an auto-detection and geo-referencing of different objects. Although this computer vision component for drones will provide to the Business domain an inventory of the road elements detected and their corresponding position in the terrain to serve as input for the creation of the BIM model, this computer vision component for drones could be extended for other business functions following the generic Building Block defined for the Payload Data Analytics.","title":"Contribution and Improvements"},{"location":"enabling_technologies/WP3/HIB/#design-and-implementation","text":"The computer vision component for drones is being developed following the typical steps for a deep-learning-based SW: Dataset creation Election of the most accurate object detection model. Training of the object detection model using previously created dataset. Testing of the object detection model already trained. So, it is possible to state that the computer vision component for drones is built upon two main axes: the dataset and the object detection model. For the dataset creation, several available datasets have been contemplated: German Traffic Sign Detection Benchmark (GTSDB) [20], which is a single-image detection dataset based on images taken in Germany. It contains around 900 images, 600 for training and 300 for test. The traffic sign classes in this dataset are shown below: Figure 37: Traffic sign classes in GTSDB Images from drone flight videos. This dataset is built with the images captured during the 1st data acquisition campaign performed in UC2 \u2013 Construction in Ja\u00e9n (Spain). An example is shown below: Figure 38: Example of images from drone flight videos Dataset built from the videos generated with AirSim (drone simulation environment) . Figure 39: Example of images from drone simulated flight videos Mapillary Traffic Sign Dataset (MTSD) [21], which it is a dataset with street-level images around the world. These images are annotated with bounding boxes and traffic sign classes. There are 100000 high-resolution images, 52000 of them fully annotated. Also, there are over 300 traffic sign classes, as it is shown below. Figure 40: Overview of all traffic sign classes in MTSD Related to the object detection model, Faster R-CNN [22] is the model used to date, and it is an object detection system proposed by Ren et al. in 2015. Its architecture is shown in Figure 41 and it is formed by two components: Region Proposal Network and Fast Region-Convolutional Neural Network. Figure 41: Faster R-CNN model general architecture Concretely, Region Proposal Network (RPN) is a deep convolutional network that proposes image regions where an object might be found. RPN component acts as an attention mechanism that accelerates region proposal phase, and in our use case, it creates new regions (bounding boxes) where a traffic signal exists. Complementarily, the Fast Region-Convolutional Neural Network (Fast R-CNN) [23] takes the regions proposed by RPN and classifies them efficiently. In our use case, this component classifies traffic signal regions in \u2018Mandatory\u2019, \u2018Danger\u2019, \u2018Forbidden\u2019. Below, it is shown an implementation conceptual diagram for the computer vision component for drones. Figure 42: Implementation conceptual diagram for computer vision component Based on Figure 42, the data flow of computer vision component for drones is detailed as follows: 1. Setup of all prerequisites: software libraries are installed and loaded, and dataset and pretrained model are downloaded. 2. The pretrained Faster R-CNN model is loaded into memory using TensorFlow Object Detection API [24] 3. The label map is loaded into memory. This object maps the three category indices to category names for the translation of the model\u2019s predictions. 4. The pretrained model performs detection over test set images. 1. Each image is prepared with Pillow library. 2. Images are transformed into Numpy arrays. 3. Images are preprocessed for the model to make predictions on them. 4. Detection takes place. 5. Predictions are visualized with TensorFlow Object Detection API and Matplotlib\u2019s Pyplot module. Below, it is possible to see traffic sign detections on images from real Drone Flight Videos by the Faster R-CNN model implemented in the computer vision component for drones. Figure 43: Detections performed by Faster R-CNN on a Drone Flight image. (top) The model fails to perform any detection over the whole image. (bottom) After chopping the image, the model correctly detects and identifies the four traffic signs.","title":"Design and Implementation"},{"location":"enabling_technologies/WP3/IFAT/","text":"WP3-10 - Generic API and component for trusted communication ID WP3-10 Contributor IFAT Levels Function Require HSM-chip Provide C-based API-lib for accessing HSM hardware-functionality Input API-call + generic byte-array (e.g. data to be signed) Output Low-level I2C commands, return data to API C4D building block Communication TRL 5 Figure 44: Building block diagram for WP3-10 Scope and contribution One \u2013 small yet important \u2013 piece of a jigsaw of an overall drone architecture is protecting the data to be transmitted via various wireless communication channels, and as a consequence thereof the required remote authentication. Protecting the communication link of a drone to any other end point (e.g. other drone, base station, and infrastructure) is important. Otherwise, the communication partner cannot verify if the communication link is established to the partner intended to. Further, in is required to maintain integrity as well as confidentiality. Therefore, in many embedded systems, a protected communication link is established via TLS between the communication partners. However, in today\u2019s embedded systems, all security-critical TLS mechanisms are typically purely executed in software in the microcontroller alongside the embedded OS and various applications. This conventional system design also makes the pure software-based TLS prone to software and side-channel attacks. Therefore, to minimize the attack vector, IFAT is developing an enhanced security concept, in which the TLS handshake is supported with a Hardware Security Module (HSM, alternatively also denoted as Secure Element (SE)) in order to perform certain security critical operations and store confidential key material. While in the course of WP5 IFAT is working on the higher-level implementation of the required security-specific algorithms and libraries, in the course of WP3 IFAT is working different aspects of defining a standardized lower-level API for the easy and modular integration of such a hardware security component into any modular drone architecture. The main purpose of this (low-level) API is to make the required functionalities of an integrated HSM accessible to various drone software-frameworks, which typically are executed on a general purpose microcontroller in C programming language (such as ROS for example). Furthermore, the API is designed in a generic way, in the sense that the same API shall be usable for more than just one hardware security device (since today most new generation/releases of HSM devices have slightly different command sets). In this way, within WP3 IFAT provides a concept to integrate hardware security components into drones with an API to be usable for future drone system integrators for various security relevant tasks. This concept will be described in more details in the paragraphs below. Design and Implementation To access the functionality of an HSM, a command library needs to be designed and implemented. An important requirement in this regard is modularity. This is important to prevent code duplication in future developments and easy replacement of HSMs. Based on that design decision, various different use cases can be addressed due to the high reusability. Therefore, the API for the command library used to establish a trusted communication channel shall be design in a generic way. Additional requirements would be a small memory footprint and multi-threading support. First, many embedded systems have limited resources, and second, the driver should not use any blocking functions, because this would be disastrous in multi-threaded environments. The latter requires synchronization between the API function. Architecture Design As depicted in Figure 45 the multi-threaded architecture is split into two main parts, the transport driver and the command library. The transport driver is communicating with the HSM, whereas the command library exposes the functionality to the higher level user application. Splitting the architecture in transport driver and command library allows being independent of the hardware, since the provided features of the hardware are different, and the command set can easily be extended. Executing a specific API function results in a serialized command which is added into the command queue by the command library. The transport driver processes the command and relays it via the I2C driver to the HSM, where it is executed and passed back via the transport driver to the command library and subsequently to the user application. Transport Driver: The implemented driver (SW-Library) reflects similar layers, which are depicted in yellow in Figure 45. The physical layer mainly consists of a set of registers to communicate with the two surrounding modules, the I\u00b2C driver and the data link layer. The data link layer is providing reliability to the communication channel by adding checksums and sequence numbers. Network and transport layer are combined in one module for simplification, since they share the same header structure. The main task of the transport layer is the packet fragmentation. The command queue worker is the direct interface to the command library and handles the state machine to communicate with the HSM. The chip control is out of scope for this deliverable, but it handles the power management of the HSM. Implemented generic HSM command library: When the user application calls an API function, the command library serializes with the APDU-builder, into the standardized \u201cAPDU\u201d1 format and submits it to the command queue. The command format is different for various devices, but devices from one product family mostly share the same format. This shows again the importance of splitting the architecture into the lower level transport driver and the command library to offer a generic solution. As depicted in Figure 45, the command library offers a basic set of functions which is used by the user application to establish a trusted communication channel. This generic library-abstraction a) helps to ease the integration effort for drone system integrators to integrate a HSM component into their drones, and b) provides a more generic API (as compared to state-of-the-art) in the way that the API is not limited to just one specific version of HSM chip-variant. In the case of IFAT, this API is planned to support different Infineon hardware-security chip family derivates and also upcoming versions. Figure 45: Generic architecture design and implemented SW-API for integrating Infineon HSM modules into a drone","title":"IFAT - Component for trusted communication"},{"location":"enabling_technologies/WP3/IFAT/#wp3-10-generic-api-and-component-for-trusted-communication","text":"ID WP3-10 Contributor IFAT Levels Function Require HSM-chip Provide C-based API-lib for accessing HSM hardware-functionality Input API-call + generic byte-array (e.g. data to be signed) Output Low-level I2C commands, return data to API C4D building block Communication TRL 5 Figure 44: Building block diagram for WP3-10","title":"WP3-10 - Generic API and component for trusted communication"},{"location":"enabling_technologies/WP3/IFAT/#scope-and-contribution","text":"One \u2013 small yet important \u2013 piece of a jigsaw of an overall drone architecture is protecting the data to be transmitted via various wireless communication channels, and as a consequence thereof the required remote authentication. Protecting the communication link of a drone to any other end point (e.g. other drone, base station, and infrastructure) is important. Otherwise, the communication partner cannot verify if the communication link is established to the partner intended to. Further, in is required to maintain integrity as well as confidentiality. Therefore, in many embedded systems, a protected communication link is established via TLS between the communication partners. However, in today\u2019s embedded systems, all security-critical TLS mechanisms are typically purely executed in software in the microcontroller alongside the embedded OS and various applications. This conventional system design also makes the pure software-based TLS prone to software and side-channel attacks. Therefore, to minimize the attack vector, IFAT is developing an enhanced security concept, in which the TLS handshake is supported with a Hardware Security Module (HSM, alternatively also denoted as Secure Element (SE)) in order to perform certain security critical operations and store confidential key material. While in the course of WP5 IFAT is working on the higher-level implementation of the required security-specific algorithms and libraries, in the course of WP3 IFAT is working different aspects of defining a standardized lower-level API for the easy and modular integration of such a hardware security component into any modular drone architecture. The main purpose of this (low-level) API is to make the required functionalities of an integrated HSM accessible to various drone software-frameworks, which typically are executed on a general purpose microcontroller in C programming language (such as ROS for example). Furthermore, the API is designed in a generic way, in the sense that the same API shall be usable for more than just one hardware security device (since today most new generation/releases of HSM devices have slightly different command sets). In this way, within WP3 IFAT provides a concept to integrate hardware security components into drones with an API to be usable for future drone system integrators for various security relevant tasks. This concept will be described in more details in the paragraphs below.","title":"Scope and contribution"},{"location":"enabling_technologies/WP3/IFAT/#design-and-implementation","text":"To access the functionality of an HSM, a command library needs to be designed and implemented. An important requirement in this regard is modularity. This is important to prevent code duplication in future developments and easy replacement of HSMs. Based on that design decision, various different use cases can be addressed due to the high reusability. Therefore, the API for the command library used to establish a trusted communication channel shall be design in a generic way. Additional requirements would be a small memory footprint and multi-threading support. First, many embedded systems have limited resources, and second, the driver should not use any blocking functions, because this would be disastrous in multi-threaded environments. The latter requires synchronization between the API function.","title":"Design and Implementation"},{"location":"enabling_technologies/WP3/IFAT/#architecture-design","text":"As depicted in Figure 45 the multi-threaded architecture is split into two main parts, the transport driver and the command library. The transport driver is communicating with the HSM, whereas the command library exposes the functionality to the higher level user application. Splitting the architecture in transport driver and command library allows being independent of the hardware, since the provided features of the hardware are different, and the command set can easily be extended. Executing a specific API function results in a serialized command which is added into the command queue by the command library. The transport driver processes the command and relays it via the I2C driver to the HSM, where it is executed and passed back via the transport driver to the command library and subsequently to the user application.","title":"Architecture Design"},{"location":"enabling_technologies/WP3/IFAT/#transport-driver","text":"The implemented driver (SW-Library) reflects similar layers, which are depicted in yellow in Figure 45. The physical layer mainly consists of a set of registers to communicate with the two surrounding modules, the I\u00b2C driver and the data link layer. The data link layer is providing reliability to the communication channel by adding checksums and sequence numbers. Network and transport layer are combined in one module for simplification, since they share the same header structure. The main task of the transport layer is the packet fragmentation. The command queue worker is the direct interface to the command library and handles the state machine to communicate with the HSM. The chip control is out of scope for this deliverable, but it handles the power management of the HSM.","title":"Transport Driver:"},{"location":"enabling_technologies/WP3/IFAT/#implemented-generic-hsm-command-library","text":"When the user application calls an API function, the command library serializes with the APDU-builder, into the standardized \u201cAPDU\u201d1 format and submits it to the command queue. The command format is different for various devices, but devices from one product family mostly share the same format. This shows again the importance of splitting the architecture into the lower level transport driver and the command library to offer a generic solution. As depicted in Figure 45, the command library offers a basic set of functions which is used by the user application to establish a trusted communication channel. This generic library-abstraction a) helps to ease the integration effort for drone system integrators to integrate a HSM component into their drones, and b) provides a more generic API (as compared to state-of-the-art) in the way that the API is not limited to just one specific version of HSM chip-variant. In the case of IFAT, this API is planned to support different Infineon hardware-security chip family derivates and also upcoming versions. Figure 45: Generic architecture design and implemented SW-API for integrating Infineon HSM modules into a drone","title":"Implemented generic HSM command library:"},{"location":"enabling_technologies/WP3/IKERLAN/","text":"WP3-01 - Drone Pre-certified MPSoC based module ID WP3-01 Contributor IKERLAN Levels Functional Require Communication service to obtain data from the neighbours Provide Hardware blueprint for implementing different architectural blocks, such as, obstacle detection or obstacle avoidance. Input Sensor data or obstacle distance data Output Required obstacle distance or the new trajectory for avoiding the present obstacle. C4D building block Obstacle Avoidance, Obstacle Detection. TRL 4 Nowadays, commercial drones rely heavily in the use of microcontrollers to execute the autopilot that controls it. This is the case of the most used autopilots, such as, PX4, Paparazzi or Ardupilot, which has been analysed in the scope of this project. In recent years, these drones had added several features that makes them more autonomous, not needing an external input to offer more secure and reliable flights; and are capable of doing more and more tasks, such as in-flight object detection, sensor data gathering and processing, or SLAM algorithm execution. This has been enabled by additional computing capabilities delivered by companion computers packed in within drones. Year by year, this extra computing power is increasing in capability, power performance or factory form, allowing to implement more complex behaviours in drones, that were not possible not that long ago. Following this trend, the use of FPGA in edge devices hasn\u2019t been that common in contrast to GPUs or ARM-based processors. One of the reasons behind is because of its programming complexity, they require a deep knowledge of how they work and expertise to program them. But state-of-the-art AI techniques and data processing algorithms are nowadays commonly implemented in such systems, extending the usage of this devices. In the project scope, the Drone pre-certified MPSoC based-module brings a modular hardware blueprint to enable and ease the use of modern heterogeneous-computing architectures to the drone architecture. The device contains a Zynq-UltraScale+ SoC that provides of a flexible computing architecture, which contains a quad-core ARM processor capable of running Linux, dual-core Cortex-R5F optimized for real-time and safety-critical applications and a FPGA for parallel algorithm execution and data processing- to implement demanding computing and communication applications. This SoC provides the means of creating tailored computational architecture for the target applications, considering safety or real-time aspects. In addition, the device has been designed following modular approach, mimicking the reference architecture, to enable the reuse of the hardware in different use-cases and building blocks. Figure 29: Xilinx Zynq UltraScale+ architecture block diagram Design and Implementation To bring hardware modularity for such a complex piece of hardware, the system has been designed following the SMARC (Smart Mobility Architecture) standard. This standard was created by SGeT (Standardization Group for embedded Technologies), a non-profit organisation. This standard defines a set of requirements for Computer Modules, which are leveraged in size, mechanical and electrical characteristics, connection pin out and properties or capabilities. Thanks to this, self-contained and defined embedded Computer Modules has been created that are part of a new standardized ecosystem, allowing the use of several vendor Computer Modules in the same manner. Figure 30: Congatec NXP i.MX8 SMARC 2 module As the Computer Modules provides encapsules all the computing capabilities, but lack of interfaces to interact with other devices, the use of Carrier Boards is necessary. These Carrier Boards provides of the feeding voltages, communication PHYs or/and connector to interoperate with other devices. The Drone Pre-certified MPSoC based module is being designed following the SMARC standard, fitting a Zynq UltraScale+ SoC and adding additional electronic devices, such as RAM and eMMC memory chips, to be capable of running complex software and Oss, such as Linux. The Carrier Board is being designed having the drone hardware requirements in mind. The design provide means of connecting sensors to the hardware. For that purpose, Ethernet, USB, or CSI ports has been added. These communication means are commonly used to connect sensors like Lidars or cameras that enables the execution of object detection and classification, obstacle avoidance or other tasks, such as data gathering. In addition, it provides of I2C and SPI bus connection, which are widely used to attach low throughput sensors or microSD slot for data recording. It also provides 12v feeding port, to feed attached sensors. In addition, a safety assessment has been done to ease the compliance of regulatory requirements. As outcome to this assessment, additional hardware elements have been added to mitigate the identified risk sources. Figure 31: A preview of the Ikerlan\u2019s Mammut Carrier Board All this hardware provides the basis for implementing desired building blocks, thanks to the powerful computing capabilities and flexible hardware setup. The next section provides insight of how this developed design eases the integration of architectural building blocks. Contributions To understand better the impact of the developing hardware, Figure 32 abstracts how a C4D building block would be implemented in the Drone Pre-certified MPSoC based module, in this case, an Obstacle Detection block. As we can see, the developed hardware can contain all the hardware and software elements that would supports the building block. Thanks to its physical interfaces, an obstacle sensing device can be plugged, for example, a LiDAR. The Mammut Computer Module would be able to fetch the data thanks to the drivers and execute the specific algorithms. These algorithms could benefit of the programmable logic unit, which offers parallel computing and high frequency throughput. Once that is ready, it would send that data, in the specified data bus by the C4D architecture to the rest of C4D building blocks. This setup can be replaced, adapting the required functionality, and deploying new C4D building block. Figure 32: Building block implementation conceptual diagram","title":"IKERLAN - Mammut Compute Module"},{"location":"enabling_technologies/WP3/IKERLAN/#wp3-01-drone-pre-certified-mpsoc-based-module","text":"ID WP3-01 Contributor IKERLAN Levels Functional Require Communication service to obtain data from the neighbours Provide Hardware blueprint for implementing different architectural blocks, such as, obstacle detection or obstacle avoidance. Input Sensor data or obstacle distance data Output Required obstacle distance or the new trajectory for avoiding the present obstacle. C4D building block Obstacle Avoidance, Obstacle Detection. TRL 4 Nowadays, commercial drones rely heavily in the use of microcontrollers to execute the autopilot that controls it. This is the case of the most used autopilots, such as, PX4, Paparazzi or Ardupilot, which has been analysed in the scope of this project. In recent years, these drones had added several features that makes them more autonomous, not needing an external input to offer more secure and reliable flights; and are capable of doing more and more tasks, such as in-flight object detection, sensor data gathering and processing, or SLAM algorithm execution. This has been enabled by additional computing capabilities delivered by companion computers packed in within drones. Year by year, this extra computing power is increasing in capability, power performance or factory form, allowing to implement more complex behaviours in drones, that were not possible not that long ago. Following this trend, the use of FPGA in edge devices hasn\u2019t been that common in contrast to GPUs or ARM-based processors. One of the reasons behind is because of its programming complexity, they require a deep knowledge of how they work and expertise to program them. But state-of-the-art AI techniques and data processing algorithms are nowadays commonly implemented in such systems, extending the usage of this devices. In the project scope, the Drone pre-certified MPSoC based-module brings a modular hardware blueprint to enable and ease the use of modern heterogeneous-computing architectures to the drone architecture. The device contains a Zynq-UltraScale+ SoC that provides of a flexible computing architecture, which contains a quad-core ARM processor capable of running Linux, dual-core Cortex-R5F optimized for real-time and safety-critical applications and a FPGA for parallel algorithm execution and data processing- to implement demanding computing and communication applications. This SoC provides the means of creating tailored computational architecture for the target applications, considering safety or real-time aspects. In addition, the device has been designed following modular approach, mimicking the reference architecture, to enable the reuse of the hardware in different use-cases and building blocks. Figure 29: Xilinx Zynq UltraScale+ architecture block diagram","title":"WP3-01 - Drone Pre-certified MPSoC based module"},{"location":"enabling_technologies/WP3/IKERLAN/#design-and-implementation","text":"To bring hardware modularity for such a complex piece of hardware, the system has been designed following the SMARC (Smart Mobility Architecture) standard. This standard was created by SGeT (Standardization Group for embedded Technologies), a non-profit organisation. This standard defines a set of requirements for Computer Modules, which are leveraged in size, mechanical and electrical characteristics, connection pin out and properties or capabilities. Thanks to this, self-contained and defined embedded Computer Modules has been created that are part of a new standardized ecosystem, allowing the use of several vendor Computer Modules in the same manner. Figure 30: Congatec NXP i.MX8 SMARC 2 module As the Computer Modules provides encapsules all the computing capabilities, but lack of interfaces to interact with other devices, the use of Carrier Boards is necessary. These Carrier Boards provides of the feeding voltages, communication PHYs or/and connector to interoperate with other devices. The Drone Pre-certified MPSoC based module is being designed following the SMARC standard, fitting a Zynq UltraScale+ SoC and adding additional electronic devices, such as RAM and eMMC memory chips, to be capable of running complex software and Oss, such as Linux. The Carrier Board is being designed having the drone hardware requirements in mind. The design provide means of connecting sensors to the hardware. For that purpose, Ethernet, USB, or CSI ports has been added. These communication means are commonly used to connect sensors like Lidars or cameras that enables the execution of object detection and classification, obstacle avoidance or other tasks, such as data gathering. In addition, it provides of I2C and SPI bus connection, which are widely used to attach low throughput sensors or microSD slot for data recording. It also provides 12v feeding port, to feed attached sensors. In addition, a safety assessment has been done to ease the compliance of regulatory requirements. As outcome to this assessment, additional hardware elements have been added to mitigate the identified risk sources. Figure 31: A preview of the Ikerlan\u2019s Mammut Carrier Board All this hardware provides the basis for implementing desired building blocks, thanks to the powerful computing capabilities and flexible hardware setup. The next section provides insight of how this developed design eases the integration of architectural building blocks.","title":"Design and Implementation"},{"location":"enabling_technologies/WP3/IKERLAN/#contributions","text":"To understand better the impact of the developing hardware, Figure 32 abstracts how a C4D building block would be implemented in the Drone Pre-certified MPSoC based module, in this case, an Obstacle Detection block. As we can see, the developed hardware can contain all the hardware and software elements that would supports the building block. Thanks to its physical interfaces, an obstacle sensing device can be plugged, for example, a LiDAR. The Mammut Computer Module would be able to fetch the data thanks to the drivers and execute the specific algorithms. These algorithms could benefit of the programmable logic unit, which offers parallel computing and high frequency throughput. Once that is ready, it would send that data, in the specified data bus by the C4D architecture to the rest of C4D building blocks. This setup can be replaced, adapting the required functionality, and deploying new C4D building block. Figure 32: Building block implementation conceptual diagram","title":"Contributions"},{"location":"enabling_technologies/WP3/IMEC_1/","text":"WP3-19_1 - Hyperspectral payload ID WP3-19_1 Contributor IMEC Levels Functional Require Power, GPS coordinates, data & control channel to ground controller Provide Data analytics & spectral data Input Start/end capture signal, exposure time, GPS coordinates, system time Output Current captured hyperspectral frame, classified images, data analytics, spectral hyperspectral data C4D building block Hyperspectral payload TRL 6 Figure 71: Building Block diagram Detailed Description Essentially, the Hyperspectral payload captures hyperspectral images, tags the images together with GPS coordinates, stores the images locally, processes the data to provide certain analytics, and sends the raw and classified/processed images to ground controller. The hyperspectral payload can be coupled together with a certain drone system. An example of the integration is shown in the figures below. The integration is on three aspects: physical connection, control interface and the data connection between drone and payload. The physical connection is usually done via a gimbal. The control connection is done via a serial port over which commands can be sent to the payload to set the relevant parameters for the sensors in the payload. The data connection is done via Ethernet/HDMI connection, where the drone controller can request specific frames/info from the sensors that can eventually be sent to a ground controller. This information can also be used to synchronize between payload data and other sensors (e.g., GPS) or other payloads connected to the drone. The images from this sensor are unique in that they can provide hyper/multi-spectral images of up to 40 spectral bands in the range 450-900 nm. Contribution and Improvements Currently, there are no real lightweight hyperspectral UAV cameras which have more than 4/5 bands. Such a camera would be a real breakthrough in the domain of UAV precision agriculture. Parrot\u2019s sequoia multispectral camera with about 4-5 spectral bands is the leading state of the art in this domain. However, with 4-5 spectral bands only simple agriculture indices like NDVI can be extracted. Tetra cam\u2019s 3-filter camera or multi-camera systems supporting up to 12 bands are other alternatives. However, multi camera systems lead to much more bulkier systems with additional complexity of software to register images from different cameras to obtain the same spatial field of view, which could potentially lead to loss in image quality. For our target applications more spectral information would be required (>10 bands in VISNIR) to provide accurate diagnostic and actionable information. Our proposed camera can enable such applications. Headwall\u2019s micro-hyperspec is another camera intended for UAV platforms, which uses conventional grating-- based solutions for the spectral unit. This leads to a bulkier camera than our proposed solution, making this unsuitable for lightweight drones. Micro-hyperspecs cameras can weigh up to 1kg or more, making this perhaps more suitable for larger drones/UAVs. Compared to other multispectral payload systems, this system has a significant improvement in the number of spectral bands, typically from 5-10spectral bands to up to 40 spectral bands. This enables higher precision and accuracies in current inspections and also enables new inspection methods (e.g., inspection of soil quality, which conventionally would have required sending the samples to a lab). Design and Implementation The core design of the payload and its integration with the drone system is shown in Figure 72. Figure 72: System architecture of UAV payload with compute enabled system A first prototype payload has been built by Airobot to be able successfully perform first data collection flights. The pictures below show the integration on the Airobot Mapper drone and of the first test flight. Figure 73: Prototype payload on Airobot Mapper As a second prototype, IMEC-BG has implemented a second iteration of the payload. The payload consists of two multispectral sensors in the spectral range 470-900nm, a jetson Tx2 to enable onboard computation and about 1TB of storage to collect spectral data during flight. In addition to the payload integration with a drone as described by Airobot, IMEC has done initial integration (both hardware and software) of this payload with a DJI M600 drone. This integration was done to show the modular and flexible aspect of our payload and data acquisition software blocks. Current software development for this version of the prototype has two parts (1) firmware/acquisition software running on the payload/jetson system and (2) ground controller software running on a tablet which is connected to a drone controller. The key functionality of firmware block is to capture the data from the two multispectral sensors and perform initial pre-processing steps and store them on onboard disk. The key functionality of the ground controller software is to enable user to control camera parameters and to provide a live preview of the images from spectral sensors. Figure 74: Prototype IMEC payload Finally, Airobot has been working together with IMEC on working out the detailed design to integrate their payload. The mechanical, electrical and software interface has been defined. To speed up the development a setup has been created so that the software development can be done without needing physical access to the drone. Figure 75: Architecture of interface with IMEC payload","title":"IMEC - Hyperspectral payload"},{"location":"enabling_technologies/WP3/IMEC_1/#wp3-19_1-hyperspectral-payload","text":"ID WP3-19_1 Contributor IMEC Levels Functional Require Power, GPS coordinates, data & control channel to ground controller Provide Data analytics & spectral data Input Start/end capture signal, exposure time, GPS coordinates, system time Output Current captured hyperspectral frame, classified images, data analytics, spectral hyperspectral data C4D building block Hyperspectral payload TRL 6 Figure 71: Building Block diagram","title":"WP3-19_1 - Hyperspectral payload"},{"location":"enabling_technologies/WP3/IMEC_1/#detailed-description","text":"Essentially, the Hyperspectral payload captures hyperspectral images, tags the images together with GPS coordinates, stores the images locally, processes the data to provide certain analytics, and sends the raw and classified/processed images to ground controller. The hyperspectral payload can be coupled together with a certain drone system. An example of the integration is shown in the figures below. The integration is on three aspects: physical connection, control interface and the data connection between drone and payload. The physical connection is usually done via a gimbal. The control connection is done via a serial port over which commands can be sent to the payload to set the relevant parameters for the sensors in the payload. The data connection is done via Ethernet/HDMI connection, where the drone controller can request specific frames/info from the sensors that can eventually be sent to a ground controller. This information can also be used to synchronize between payload data and other sensors (e.g., GPS) or other payloads connected to the drone. The images from this sensor are unique in that they can provide hyper/multi-spectral images of up to 40 spectral bands in the range 450-900 nm.","title":"Detailed Description"},{"location":"enabling_technologies/WP3/IMEC_1/#contribution-and-improvements","text":"Currently, there are no real lightweight hyperspectral UAV cameras which have more than 4/5 bands. Such a camera would be a real breakthrough in the domain of UAV precision agriculture. Parrot\u2019s sequoia multispectral camera with about 4-5 spectral bands is the leading state of the art in this domain. However, with 4-5 spectral bands only simple agriculture indices like NDVI can be extracted. Tetra cam\u2019s 3-filter camera or multi-camera systems supporting up to 12 bands are other alternatives. However, multi camera systems lead to much more bulkier systems with additional complexity of software to register images from different cameras to obtain the same spatial field of view, which could potentially lead to loss in image quality. For our target applications more spectral information would be required (>10 bands in VISNIR) to provide accurate diagnostic and actionable information. Our proposed camera can enable such applications. Headwall\u2019s micro-hyperspec is another camera intended for UAV platforms, which uses conventional grating-- based solutions for the spectral unit. This leads to a bulkier camera than our proposed solution, making this unsuitable for lightweight drones. Micro-hyperspecs cameras can weigh up to 1kg or more, making this perhaps more suitable for larger drones/UAVs. Compared to other multispectral payload systems, this system has a significant improvement in the number of spectral bands, typically from 5-10spectral bands to up to 40 spectral bands. This enables higher precision and accuracies in current inspections and also enables new inspection methods (e.g., inspection of soil quality, which conventionally would have required sending the samples to a lab).","title":"Contribution and Improvements"},{"location":"enabling_technologies/WP3/IMEC_1/#design-and-implementation","text":"The core design of the payload and its integration with the drone system is shown in Figure 72. Figure 72: System architecture of UAV payload with compute enabled system A first prototype payload has been built by Airobot to be able successfully perform first data collection flights. The pictures below show the integration on the Airobot Mapper drone and of the first test flight. Figure 73: Prototype payload on Airobot Mapper As a second prototype, IMEC-BG has implemented a second iteration of the payload. The payload consists of two multispectral sensors in the spectral range 470-900nm, a jetson Tx2 to enable onboard computation and about 1TB of storage to collect spectral data during flight. In addition to the payload integration with a drone as described by Airobot, IMEC has done initial integration (both hardware and software) of this payload with a DJI M600 drone. This integration was done to show the modular and flexible aspect of our payload and data acquisition software blocks. Current software development for this version of the prototype has two parts (1) firmware/acquisition software running on the payload/jetson system and (2) ground controller software running on a tablet which is connected to a drone controller. The key functionality of firmware block is to capture the data from the two multispectral sensors and perform initial pre-processing steps and store them on onboard disk. The key functionality of the ground controller software is to enable user to control camera parameters and to provide a live preview of the images from spectral sensors. Figure 74: Prototype IMEC payload Finally, Airobot has been working together with IMEC on working out the detailed design to integrate their payload. The mechanical, electrical and software interface has been defined. To speed up the development a setup has been created so that the software development can be done without needing physical access to the drone. Figure 75: Architecture of interface with IMEC payload","title":"Design and Implementation"},{"location":"enabling_technologies/WP3/IMEC_2/","text":"Hyperspectral image processing ID WP3-19_2 Contributor IMEC Detailed Description The hyperspectral imaging (HSI) pipeline is a system that processes and analyses the hyperspectral data originating from the hyperspectral payload developed by IMEC. It supports a drone system in many aspects. First, it provides an API to easily access radiometric- and reflectance-corrected hyperspectral images. Second, it implements tools to compress the data in a lossless manner to limit storage needs. Third, it provides a fully automated way of stitching the images together into a georeferenced orthomosaic. Finally, the HSI pipeline also accommodates an API to easily interpret the data by semantically labelling the data (i.e., assigning a class label to each pixel of the image) also known as classification. The latter is done in a semi-supervised way and supports many different use cases, including applications on surveillance and inspection. In the context of the reference architecture, the hyperspectral imaging (HSI) pipeline supports payload data analytics block. The second innovation involves data-analytics on images to detect imperfections, like corrosion, deterioration of paint, real-time or offline. The hyperspectral imaging (HIS) processing pipeline that we are developing is depicted below. It is made up of three main modules: pre-processing, on-board analysis, and post-processing. Figure 76: The hyperspectral imaging processing pipeline The hyperspectral images are finally stitched together, and a 3D model is constructed. To that end, our photogrammetry (or structure from motion) software was further enhanced in a way that it is better suited to cope with repetitive patterns in the environment. Photogrammetry in such challenging conditions generally fails using existing, commercial, software packages. Experiments with our improvements show that our pipeline is a lot better suited to cope with repetitive patterns in the scene. The on-board analysis is dealing with the detection of degradations, i.e., corrosion, of the infrastructure. To this end we are developing AI-algorithms (CNN-based) that exploit both spectral and spatial futures to identify the regions where corrosion appears. Contribution and Improvements Currently, there exists many hyperspectral image processing algorithms (e.g., de-mosaicking for mosaicked sensor layouts or deep learning based detection, segmentation or classification). However, they are developed and designed for (off-board) PC platforms and are totally not optimized for the IMEC\u2019s hyperspectral dual camera payload, integrated nor run on embedded hardware platforms such as the Jetson TX2 board. Classic deep learning frameworks rely on massive amount of annotated data, over which we will not dispose (and are not able to collect ourselves). Therefore, we rely on recently developed few-shot learning techniques, which are trained with only a limited number of annotated samples. However, the robustness under various noise conditions and few-shot learning performance needs further research. In the case of hyperspectral imaging, this will also impact the acquisition: e.g., the varying incident sun light will create different appearances of the same physical material. Proper normalization procedures are needed to be developed. The entire pipeline is made up of three main modules: pre-processing, on-board analysis, and post-processing, as can be seen in the figure above. The on-board analysis is dealing with the detection of degradations, i.e., corrosion, of the infrastructure. To this end, AI-algorithms (CNN-based) are developed. A result of the current corrosion detection algorithm is depicted in Figure 40. The purpose is to implement these algorithms on the Jetson TX2 board for them to be executed in real-time. This way the corrosion parts can be identified and located online, and the drone can be instructed to fly towards the most degraded areas in order to limit fly-time and avoid those relevant areas remain uncaptured. The final goal is to achieve automatic HS-image-based detection and quantification of corrosion using AI technology with an accuracy of 80% compared to human inspections. Design and Implementation A schematic overview of the dataflow for the HSI processing pipeline is depicted on the figure below. A raw HS image acquired by the payload is sent to the preprocessing module where several corrections are carried out, de-mosaicking is taking place and a HS cube is created. The corrected HS-cubes are then fed to the offline analysis module, where traditional classification algorithms can generate a labelled HS image. Alternatively, the labelled HS image can be fed to the dataset generation and model definition submodule in order to set up a CNN training environment. After the training, the CNN model is used in the online analysis module where inference can be applied on another HS cube. In parallel, position and orientation information (provided by the GNSS and IMU) together with the corrected HS cube is fed to the photogrammetry module to generate both, the HS point cloud and the camera poses. Eventually, both are fed to the inference submodule to create labelled HS images and a fully georeferenced, labelled HS point cloud. Figure 77: The hyperspectral imaging processing pipeline The HSI processing pipeline can be used to develop a state-of-the-art detector for corrosion on infrastructure or pollution in soil. A few collected HS cubes can be classified using traditional techniques to serve as a training dataset (after minor manual verification). A CNN model can then be trained and used to classify additional images using the inference module. Together with the output from the photogrammetry a georeferenced and labelled HS point cloud can be obtained. The deep learning framework was evaluated using data captured near a gate that was subject to corrosion. Experiments demonstrated that we achieved an accuracy of 95% to 98% (depending on the CNN model) for classifying corroded areas (i.e., pixels) within the HS images. Moreover, we successfully tested the deployment of the network on a Nvidia Jetson Xavier NX. The inference to generate a labelled HS Image of 254x510 pixels can be conducted in a handful seconds on the Xavier NX. This allows for online processing and hence real-time applications, e.g., to instruct the drone to fly towards the areas of interest (indicated by the labelled image). Using some of the prototypes from IMEC & AIROBOT, initial work on corrosion detection shows promising results as indicated below. The focus for the next period will be to reorient the instantiation of these algorithms towards soil quality instead of corrosion. Figure 78: Result of the corrosion detection algorithm at 1 meter distance: purple denotes back ground, yellow denotes corrosion Figure 79: Result of the corrosion detection algorithm at 10 meter distance: purple denotes back ground, yellow denotes corrosion","title":"IMEC - Hyperspectral image processing"},{"location":"enabling_technologies/WP3/IMEC_2/#hyperspectral-image-processing","text":"ID WP3-19_2 Contributor IMEC","title":"Hyperspectral image processing"},{"location":"enabling_technologies/WP3/IMEC_2/#detailed-description","text":"The hyperspectral imaging (HSI) pipeline is a system that processes and analyses the hyperspectral data originating from the hyperspectral payload developed by IMEC. It supports a drone system in many aspects. First, it provides an API to easily access radiometric- and reflectance-corrected hyperspectral images. Second, it implements tools to compress the data in a lossless manner to limit storage needs. Third, it provides a fully automated way of stitching the images together into a georeferenced orthomosaic. Finally, the HSI pipeline also accommodates an API to easily interpret the data by semantically labelling the data (i.e., assigning a class label to each pixel of the image) also known as classification. The latter is done in a semi-supervised way and supports many different use cases, including applications on surveillance and inspection. In the context of the reference architecture, the hyperspectral imaging (HSI) pipeline supports payload data analytics block. The second innovation involves data-analytics on images to detect imperfections, like corrosion, deterioration of paint, real-time or offline. The hyperspectral imaging (HIS) processing pipeline that we are developing is depicted below. It is made up of three main modules: pre-processing, on-board analysis, and post-processing. Figure 76: The hyperspectral imaging processing pipeline The hyperspectral images are finally stitched together, and a 3D model is constructed. To that end, our photogrammetry (or structure from motion) software was further enhanced in a way that it is better suited to cope with repetitive patterns in the environment. Photogrammetry in such challenging conditions generally fails using existing, commercial, software packages. Experiments with our improvements show that our pipeline is a lot better suited to cope with repetitive patterns in the scene. The on-board analysis is dealing with the detection of degradations, i.e., corrosion, of the infrastructure. To this end we are developing AI-algorithms (CNN-based) that exploit both spectral and spatial futures to identify the regions where corrosion appears.","title":"Detailed Description"},{"location":"enabling_technologies/WP3/IMEC_2/#contribution-and-improvements","text":"Currently, there exists many hyperspectral image processing algorithms (e.g., de-mosaicking for mosaicked sensor layouts or deep learning based detection, segmentation or classification). However, they are developed and designed for (off-board) PC platforms and are totally not optimized for the IMEC\u2019s hyperspectral dual camera payload, integrated nor run on embedded hardware platforms such as the Jetson TX2 board. Classic deep learning frameworks rely on massive amount of annotated data, over which we will not dispose (and are not able to collect ourselves). Therefore, we rely on recently developed few-shot learning techniques, which are trained with only a limited number of annotated samples. However, the robustness under various noise conditions and few-shot learning performance needs further research. In the case of hyperspectral imaging, this will also impact the acquisition: e.g., the varying incident sun light will create different appearances of the same physical material. Proper normalization procedures are needed to be developed. The entire pipeline is made up of three main modules: pre-processing, on-board analysis, and post-processing, as can be seen in the figure above. The on-board analysis is dealing with the detection of degradations, i.e., corrosion, of the infrastructure. To this end, AI-algorithms (CNN-based) are developed. A result of the current corrosion detection algorithm is depicted in Figure 40. The purpose is to implement these algorithms on the Jetson TX2 board for them to be executed in real-time. This way the corrosion parts can be identified and located online, and the drone can be instructed to fly towards the most degraded areas in order to limit fly-time and avoid those relevant areas remain uncaptured. The final goal is to achieve automatic HS-image-based detection and quantification of corrosion using AI technology with an accuracy of 80% compared to human inspections.","title":"Contribution and Improvements"},{"location":"enabling_technologies/WP3/IMEC_2/#design-and-implementation","text":"A schematic overview of the dataflow for the HSI processing pipeline is depicted on the figure below. A raw HS image acquired by the payload is sent to the preprocessing module where several corrections are carried out, de-mosaicking is taking place and a HS cube is created. The corrected HS-cubes are then fed to the offline analysis module, where traditional classification algorithms can generate a labelled HS image. Alternatively, the labelled HS image can be fed to the dataset generation and model definition submodule in order to set up a CNN training environment. After the training, the CNN model is used in the online analysis module where inference can be applied on another HS cube. In parallel, position and orientation information (provided by the GNSS and IMU) together with the corrected HS cube is fed to the photogrammetry module to generate both, the HS point cloud and the camera poses. Eventually, both are fed to the inference submodule to create labelled HS images and a fully georeferenced, labelled HS point cloud. Figure 77: The hyperspectral imaging processing pipeline The HSI processing pipeline can be used to develop a state-of-the-art detector for corrosion on infrastructure or pollution in soil. A few collected HS cubes can be classified using traditional techniques to serve as a training dataset (after minor manual verification). A CNN model can then be trained and used to classify additional images using the inference module. Together with the output from the photogrammetry a georeferenced and labelled HS point cloud can be obtained. The deep learning framework was evaluated using data captured near a gate that was subject to corrosion. Experiments demonstrated that we achieved an accuracy of 95% to 98% (depending on the CNN model) for classifying corroded areas (i.e., pixels) within the HS images. Moreover, we successfully tested the deployment of the network on a Nvidia Jetson Xavier NX. The inference to generate a labelled HS Image of 254x510 pixels can be conducted in a handful seconds on the Xavier NX. This allows for online processing and hence real-time applications, e.g., to instruct the drone to fly towards the areas of interest (indicated by the labelled image). Using some of the prototypes from IMEC & AIROBOT, initial work on corrosion detection shows promising results as indicated below. The focus for the next period will be to reorient the instantiation of these algorithms towards soil quality instead of corrosion. Figure 78: Result of the corrosion detection algorithm at 1 meter distance: purple denotes back ground, yellow denotes corrosion Figure 79: Result of the corrosion detection algorithm at 10 meter distance: purple denotes back ground, yellow denotes corrosion","title":"Design and Implementation"},{"location":"enabling_technologies/WP3/MODIS/","text":"WP3-20 - Multi-sensor positioning ID WP3-20 Contributor MODIS Levels Functional Require Provide GPS spoofing message detection using ML technique , position computation in absence of GPS based on sensors data Input Sensors data (gyro, accelerometer, magnetometer, GPS) Output Binary data to indicate absence or not of spoofing; X,Y,Z coordinates estimation if spoofing is detected C4D building block Continuous flight in without GPS information, spoofing detection TRL 4 Detailed Description Px4 autopilot is a system involving open source hardware and software freely available to everyone under BSD license. A Pixhawk board is developed according to the Pixhawk standard and conforms to the Pixhawk standard requirements. The Holybro board based on CPU STM32F765 is one of these supported HW and has chosen as the main board for drone controller. Holybro board is therefore responsible for piloting the drone, being equipped with an IMU sensor to control its angle, acceleration and orientation. The path that the drone must follow is established in advance and is followed by the drone thanks to the GPS receiver. Detection of GPS spoofed messages is done with ML techniques, so a dedicated board is used to perform in parallel this task. There is also the need to collect debugging data and monitor the state of the execution so a Raspberripy 4B board is used to accomplish this. Figure 80: A Pixhawk board Figure 81: A Raspberry Pi board Raspberry Pi has the ability to interact with the outside world and has been used in a wide array of digital maker projects, from music machines and parent detectors to weather stations and tweeting birdhouses with infra-red cameras. The computational power is given by a quad core CPU (ARM) and its main memory (8 GB), which allows to manage the amount of necessary data to execute ML algorithms giving an answer in an acceptable time. Raspberry Pi OS (Raspbian) is the recommended operating system for normal use on Raspberry Pi, it is Linux based and permits all useful canonical operation such as remote connection (ssh) and log management. Contribution and Improvements Currently commercial drones do not have the ability to avoid certain attacks on the GPS for several reasons: there are different strategies to avoid a spoofing attack, many of these include expensive hardware devices both from the point of view of energy and space resources therefore not suitable for mounting on a UAV. The proposed solution combines the already strongly supported family of opensource drones with the potential of mini computers such as the raspberry Pi, and an attack detention strategy based on a supervised machine learning technique. This has several advantages: decoupling between flight management and GPS flow analysis, different power lines for the boards, continuous improvement of the classifier performance without affecting anything else of the system\u2026 Design and Implementation In order to cooperate between the two devices, Holybro and Raspberry are connected via a UART serial interface. Figure 82: Holybro and Raspberry communication Both devices are tuned to the same transmission frequency and a data exchange algorithm is established to synchronize the devices on read/write. The Px4 basically has the task of recovering data, while the Raspberry is equipped with: a SVM (Support Vector machine) classifier previously trained on a private dataset of about 6000 data; a custom implementation of the extended Kalman filter for estimating the next geographic point without using GPS coordinates. Basically, the Px4\u2019s task is to collect a quantity of data coming from the sensors over a period of time and sending them to the Raspberry Pi. On the other hand, the Raspberry reads the received data, classifies it using an SVM classifier, in case of spoofing it calculates the correct coordinates using a Kalman filter. Finally, it sends back the answer to Px4 which decides whenever to overwrite or not its GPS position based on the response. Figure 83: Global behaviour of the anti-spoofing function","title":"MODIS - multi-sensor positioning"},{"location":"enabling_technologies/WP3/MODIS/#wp3-20-multi-sensor-positioning","text":"ID WP3-20 Contributor MODIS Levels Functional Require Provide GPS spoofing message detection using ML technique , position computation in absence of GPS based on sensors data Input Sensors data (gyro, accelerometer, magnetometer, GPS) Output Binary data to indicate absence or not of spoofing; X,Y,Z coordinates estimation if spoofing is detected C4D building block Continuous flight in without GPS information, spoofing detection TRL 4","title":"WP3-20 - Multi-sensor positioning"},{"location":"enabling_technologies/WP3/MODIS/#detailed-description","text":"Px4 autopilot is a system involving open source hardware and software freely available to everyone under BSD license. A Pixhawk board is developed according to the Pixhawk standard and conforms to the Pixhawk standard requirements. The Holybro board based on CPU STM32F765 is one of these supported HW and has chosen as the main board for drone controller. Holybro board is therefore responsible for piloting the drone, being equipped with an IMU sensor to control its angle, acceleration and orientation. The path that the drone must follow is established in advance and is followed by the drone thanks to the GPS receiver. Detection of GPS spoofed messages is done with ML techniques, so a dedicated board is used to perform in parallel this task. There is also the need to collect debugging data and monitor the state of the execution so a Raspberripy 4B board is used to accomplish this. Figure 80: A Pixhawk board Figure 81: A Raspberry Pi board Raspberry Pi has the ability to interact with the outside world and has been used in a wide array of digital maker projects, from music machines and parent detectors to weather stations and tweeting birdhouses with infra-red cameras. The computational power is given by a quad core CPU (ARM) and its main memory (8 GB), which allows to manage the amount of necessary data to execute ML algorithms giving an answer in an acceptable time. Raspberry Pi OS (Raspbian) is the recommended operating system for normal use on Raspberry Pi, it is Linux based and permits all useful canonical operation such as remote connection (ssh) and log management.","title":"Detailed Description"},{"location":"enabling_technologies/WP3/MODIS/#contribution-and-improvements","text":"Currently commercial drones do not have the ability to avoid certain attacks on the GPS for several reasons: there are different strategies to avoid a spoofing attack, many of these include expensive hardware devices both from the point of view of energy and space resources therefore not suitable for mounting on a UAV. The proposed solution combines the already strongly supported family of opensource drones with the potential of mini computers such as the raspberry Pi, and an attack detention strategy based on a supervised machine learning technique. This has several advantages: decoupling between flight management and GPS flow analysis, different power lines for the boards, continuous improvement of the classifier performance without affecting anything else of the system\u2026","title":"Contribution and Improvements"},{"location":"enabling_technologies/WP3/MODIS/#design-and-implementation","text":"In order to cooperate between the two devices, Holybro and Raspberry are connected via a UART serial interface. Figure 82: Holybro and Raspberry communication Both devices are tuned to the same transmission frequency and a data exchange algorithm is established to synchronize the devices on read/write. The Px4 basically has the task of recovering data, while the Raspberry is equipped with: a SVM (Support Vector machine) classifier previously trained on a private dataset of about 6000 data; a custom implementation of the extended Kalman filter for estimating the next geographic point without using GPS coordinates. Basically, the Px4\u2019s task is to collect a quantity of data coming from the sensors over a period of time and sending them to the Raspberry Pi. On the other hand, the Raspberry reads the received data, classifies it using an SVM classifier, in case of spoofing it calculates the correct coordinates using a Kalman filter. Finally, it sends back the answer to Px4 which decides whenever to overwrite or not its GPS position based on the response. Figure 83: Global behaviour of the anti-spoofing function","title":"Design and Implementation"},{"location":"enabling_technologies/WP3/SCALIAN-ez_chains/","text":"WP3-16 - EZ_Chains Fleet Architecture ID WP3-16 Contributor SCALIAN Levels Functional Require Interface with autopilot, communication link with other drones in fleet Provide Mission planning and monitoring for fleet missions, Flight planning and guidance taking into account the other UAVs Input Mission status, periodic updates of other UAVs, UAV position and attitude from autopilot Output Mission updates, periodic updates on current UAV, commands to the autopilot C4D building block Mission Management, Flight Guidance, Flight Planning TRL 7 Detailed description Figure 70: EZ_Chains general architecture EZ_Chains is a generic architecture that allows agents to perform collaborative mission as a fleet. The architecture is built for distributed decision, and each agent relies on its (synchronised) copy of the knowledge base to choose the next steps. This knowledge base contains the current mission status, updated with transaction sent by the agents of the system (the transaction is validated by the ground segment, centralising the consistency process) and the status of each agent, updated periodically. EZ_Chains can be seen as three main layers, mission management to handle the highest level of information, flight management to handle the flight while ensuring coordination with the rest of the fleet, and finally the low-level interfaces to the autopilot and payload. The mission management is composed of two main components, the first is the task planner that computes the next best actions to achieve based on the current mission status and actions planned by the other agents. The second component is the task monitor that ensures the execution of the chosen actions, it triggers the other components. It is also responsible of the sanity of the agent, it uses watchdogs to verify the status of the different components and also detect important changes in the mission parameters that forces to reconsider the current planned actions. The management of the flight is also composed of two main components, similarly a planner and a monitor. The path planner ensures the computation of trajectories that avoid the geofences but also reduce the intersection with the other agent trajectories. The navigation monitor triggers the path planner when requested by the task monitor (to perform a given action), but also handles the air traffic management internal to the fleet. It books airspace as segments of the trajectory as the agent progress over its trajectory. It allows to prevent collision by stopping one of the agents that would try to use or crossed an airspace already booked. The navigation monitor also verifies the validity of the trajectory during its execution. Indeed if the mission status changes in the knowledge base, due to the addition of a new geofence for instance, the navigation monitor may request a new trajectory. Finally the navigation monitor talks to the autopilot for the agent to perform the planned trajectory. Contribution and Improvements EZ_Chains architecture described above has been designed to allow UAVs to perform collaborative missions. The work carried out during C4D is twofold: (i) generalise the architecture to work with new components and demonstrate its genericity, (ii) extend it to support mort type of agents than just UAVs. The description in the previous section uses the notion of agent (instead of only UAV) to demonstrate that the concepts we use in the architecture actually extend to other types of agents, which demonstrates that the architecture is mostly ready for new types of agents at a formal level. In order to ease the work aimed at in C4D, we used the first and second year to integrate new missions and tasks for UAVs. Making the system capable of using a fleet of heterogeneous UAV. It allowed us to focus on realising the demonstration for the UC3 D1 (Logistics, dropping sensors). We aim at using the third year to work on integrating new types of agents and components. Integrating new types of missions, already required to define new types of agents in the knowledge base. We now have a hierarchy of agents, for instance we have UAV>Dropper, UAV>Surveillance and GCS. To support different agents, at different level of hierarchy, required to improve the internal structure, the relation between the mission status and the periodic updates sent by the agents. Once the knowledge base can support different types of agents, the task monitor must take it into account when following the mission progress before triggering the task planner. For instance, in the UC3 D1 demonstration, due to regulatory limitations, we had to ask the fleet to trigger a landing when the surveillance UAV declared its trajectory back to base due to low battery. New behaviour must be created when new hierarchies of agents are added. In the future we would like to add, weather stations as part of the system, they would also require the triggering of conservative actions when the weather becomes too bad. The task planner is improved to allow new mission decomposition, indeed each new agent has its own goal and types of actions it can achieve. For instance, again with the surveillance UAV, it required the definition of new goals called Point of Interest (POI), where the UAV had to go survey the area border and cycle though all those POI. The definition of those POI, a goal only corresponding to the surveillance UAV, had to be incorporated in the knowledge base. Additionally the task planner, for the surveillance UAV, could not reason with the strategy used by the droppers, it required to redefine how such a mission is completed.","title":"SCALIAN \u2013 EZ_Chains Fleet Architecture"},{"location":"enabling_technologies/WP3/SCALIAN-ez_chains/#wp3-16-ez_chains-fleet-architecture","text":"ID WP3-16 Contributor SCALIAN Levels Functional Require Interface with autopilot, communication link with other drones in fleet Provide Mission planning and monitoring for fleet missions, Flight planning and guidance taking into account the other UAVs Input Mission status, periodic updates of other UAVs, UAV position and attitude from autopilot Output Mission updates, periodic updates on current UAV, commands to the autopilot C4D building block Mission Management, Flight Guidance, Flight Planning TRL 7","title":"WP3-16 - EZ_Chains Fleet Architecture"},{"location":"enabling_technologies/WP3/SCALIAN-ez_chains/#detailed-description","text":"Figure 70: EZ_Chains general architecture EZ_Chains is a generic architecture that allows agents to perform collaborative mission as a fleet. The architecture is built for distributed decision, and each agent relies on its (synchronised) copy of the knowledge base to choose the next steps. This knowledge base contains the current mission status, updated with transaction sent by the agents of the system (the transaction is validated by the ground segment, centralising the consistency process) and the status of each agent, updated periodically. EZ_Chains can be seen as three main layers, mission management to handle the highest level of information, flight management to handle the flight while ensuring coordination with the rest of the fleet, and finally the low-level interfaces to the autopilot and payload. The mission management is composed of two main components, the first is the task planner that computes the next best actions to achieve based on the current mission status and actions planned by the other agents. The second component is the task monitor that ensures the execution of the chosen actions, it triggers the other components. It is also responsible of the sanity of the agent, it uses watchdogs to verify the status of the different components and also detect important changes in the mission parameters that forces to reconsider the current planned actions. The management of the flight is also composed of two main components, similarly a planner and a monitor. The path planner ensures the computation of trajectories that avoid the geofences but also reduce the intersection with the other agent trajectories. The navigation monitor triggers the path planner when requested by the task monitor (to perform a given action), but also handles the air traffic management internal to the fleet. It books airspace as segments of the trajectory as the agent progress over its trajectory. It allows to prevent collision by stopping one of the agents that would try to use or crossed an airspace already booked. The navigation monitor also verifies the validity of the trajectory during its execution. Indeed if the mission status changes in the knowledge base, due to the addition of a new geofence for instance, the navigation monitor may request a new trajectory. Finally the navigation monitor talks to the autopilot for the agent to perform the planned trajectory.","title":"Detailed description"},{"location":"enabling_technologies/WP3/SCALIAN-ez_chains/#contribution-and-improvements","text":"EZ_Chains architecture described above has been designed to allow UAVs to perform collaborative missions. The work carried out during C4D is twofold: (i) generalise the architecture to work with new components and demonstrate its genericity, (ii) extend it to support mort type of agents than just UAVs. The description in the previous section uses the notion of agent (instead of only UAV) to demonstrate that the concepts we use in the architecture actually extend to other types of agents, which demonstrates that the architecture is mostly ready for new types of agents at a formal level. In order to ease the work aimed at in C4D, we used the first and second year to integrate new missions and tasks for UAVs. Making the system capable of using a fleet of heterogeneous UAV. It allowed us to focus on realising the demonstration for the UC3 D1 (Logistics, dropping sensors). We aim at using the third year to work on integrating new types of agents and components. Integrating new types of missions, already required to define new types of agents in the knowledge base. We now have a hierarchy of agents, for instance we have UAV>Dropper, UAV>Surveillance and GCS. To support different agents, at different level of hierarchy, required to improve the internal structure, the relation between the mission status and the periodic updates sent by the agents. Once the knowledge base can support different types of agents, the task monitor must take it into account when following the mission progress before triggering the task planner. For instance, in the UC3 D1 demonstration, due to regulatory limitations, we had to ask the fleet to trigger a landing when the surveillance UAV declared its trajectory back to base due to low battery. New behaviour must be created when new hierarchies of agents are added. In the future we would like to add, weather stations as part of the system, they would also require the triggering of conservative actions when the weather becomes too bad. The task planner is improved to allow new mission decomposition, indeed each new agent has its own goal and types of actions it can achieve. For instance, again with the surveillance UAV, it required the definition of new goals called Point of Interest (POI), where the UAV had to go survey the area border and cycle though all those POI. The definition of those POI, a goal only corresponding to the surveillance UAV, had to be incorporated in the knowledge base. Additionally the task planner, for the surveillance UAV, could not reason with the strategy used by the droppers, it required to redefine how such a mission is completed.","title":"Contribution and Improvements"},{"location":"enabling_technologies/WP3/UDANET_1/","text":"WP3-36_1 - Smart and predictive energy management system ID WP3-36_1 Contributor UDANET Levels Functional Require Sensors that provide measurements and mission details Provide Energy trajectories to perform path tracking missions Input X, Y, Z coordinates of the drone and its velocities Roll, pitch, and yaw angles of the drone Initial and final position of the mission Output Rotor\u2019s speed (or forces) C4D building block Flight planning TRL 3-4 Detailed Description An energy management system is vital to optimize the energy life and the purpose of the system. It will continuously monitor important system parameters, while dealing with the varying power demands of the many aspects, the objectives of the mission and optimizing the usage of the energy. Given the initial and final positions, the objective of the component is to compute the control inputs that rule the motion and vehicle trajectory to optimize energy consumption and the computational burden of the algorithm. The aim of this activity is to use a model-based approach to control UAV flight to minimize energy consumption. It is therefore necessary to consider the flight dynamics, battery, and energy flow and actuator models. The strategy that led to good results in some research is the optimal control and therefore this type is considered in the activity. A first objective is to improve control by making it robust with respect to model approximation errors or disturbance (for example under wind conditions). Contribution and Improvements The main contribution of the component is to provide excellent trajectories from an energy point of view for the position, speeds that are the result of an elaboration of the associated optimal control problem and analysis of the excellent results. The improvement is due to the fact that a real-time resolution of the control is not required, but rule-based strategies are required which therefore have a low computational cost. Furthermore, in collaboration with other partners, they want to experiment in non-ideal conditions (presence of wind) Design and Implementation The designed energy management system will be verified and tested via Software in The Loop using software Matlab-Simulink. The reference generator will be implemented to execute a mission and the associated controller and will be tested both with Matlab-Simulink and with complex simulators in less than ideal conditions.","title":"UDANET - Smart and predictive energy management system"},{"location":"enabling_technologies/WP3/UDANET_1/#wp3-36_1-smart-and-predictive-energy-management-system","text":"ID WP3-36_1 Contributor UDANET Levels Functional Require Sensors that provide measurements and mission details Provide Energy trajectories to perform path tracking missions Input X, Y, Z coordinates of the drone and its velocities Roll, pitch, and yaw angles of the drone Initial and final position of the mission Output Rotor\u2019s speed (or forces) C4D building block Flight planning TRL 3-4","title":"WP3-36_1 - Smart and predictive energy management system"},{"location":"enabling_technologies/WP3/UDANET_1/#detailed-description","text":"An energy management system is vital to optimize the energy life and the purpose of the system. It will continuously monitor important system parameters, while dealing with the varying power demands of the many aspects, the objectives of the mission and optimizing the usage of the energy. Given the initial and final positions, the objective of the component is to compute the control inputs that rule the motion and vehicle trajectory to optimize energy consumption and the computational burden of the algorithm. The aim of this activity is to use a model-based approach to control UAV flight to minimize energy consumption. It is therefore necessary to consider the flight dynamics, battery, and energy flow and actuator models. The strategy that led to good results in some research is the optimal control and therefore this type is considered in the activity. A first objective is to improve control by making it robust with respect to model approximation errors or disturbance (for example under wind conditions).","title":"Detailed Description"},{"location":"enabling_technologies/WP3/UDANET_1/#contribution-and-improvements","text":"The main contribution of the component is to provide excellent trajectories from an energy point of view for the position, speeds that are the result of an elaboration of the associated optimal control problem and analysis of the excellent results. The improvement is due to the fact that a real-time resolution of the control is not required, but rule-based strategies are required which therefore have a low computational cost. Furthermore, in collaboration with other partners, they want to experiment in non-ideal conditions (presence of wind)","title":"Contribution and Improvements"},{"location":"enabling_technologies/WP3/UDANET_1/#design-and-implementation","text":"The designed energy management system will be verified and tested via Software in The Loop using software Matlab-Simulink. The reference generator will be implemented to execute a mission and the associated controller and will be tested both with Matlab-Simulink and with complex simulators in less than ideal conditions.","title":"Design and Implementation"},{"location":"enabling_technologies/WP3/UDANET_2/","text":"WP3-36_2 - AI drone system modules ID WP3-36_2 Contributor UDANET Levels Functional Require Dataset Provide Designed and implemented algorithm Input Plant images Output Disease classification C4D building block Data analytics TRL 4-5 Detailed Description An artificial intelligent method to classify leaf diseases will be designed and implemented. The methods inputs are images from cameras, and the output is the plant health status classification. Artificial intelligence algorithms with different characteristics will be designed and implemented with the aim of taking into account the computational cost of each, as well as the over-fitting problem and the dataset that is not always adequate, and the diagnostic performance is drastically decreased when used on test datasets from new environments (i.e. generalization problem). A large amount of data increases the performance of machine learning algorithms and avoids over-fitting problems. Creating a large amount of data in the agricultural sector for the design of models for the diagnosis and detection of plant diseases is an open and challenging task that takes a lot of time and resources. One way to increase the dataset will be to use data augmentation techniques. It increases the diversity of training data for machine learning algorithms without collecting new data. Contribution and Improvements The improving objective of this component is to develop and test the algorithms, increase the reference dataset using basic image manipulation and deep learning based image augmentation techniques such as image flipping, cropping, rotation, colour transformation, PCA colour augmentation, noise injection, Generative Adversarial Networks (GANs) and Neural Style Transfer (NST) techniques. Performance of the data augmentation techniques was studied using state of the art transfer learning techniques both in terms of accuracy and computational cost. Design and Implementation It is developed through the Tensorflow and Python framework.","title":"UDANET - AI drone system modules"},{"location":"enabling_technologies/WP3/UDANET_2/#wp3-36_2-ai-drone-system-modules","text":"ID WP3-36_2 Contributor UDANET Levels Functional Require Dataset Provide Designed and implemented algorithm Input Plant images Output Disease classification C4D building block Data analytics TRL 4-5","title":"WP3-36_2 - AI drone system modules"},{"location":"enabling_technologies/WP3/UDANET_2/#detailed-description","text":"An artificial intelligent method to classify leaf diseases will be designed and implemented. The methods inputs are images from cameras, and the output is the plant health status classification. Artificial intelligence algorithms with different characteristics will be designed and implemented with the aim of taking into account the computational cost of each, as well as the over-fitting problem and the dataset that is not always adequate, and the diagnostic performance is drastically decreased when used on test datasets from new environments (i.e. generalization problem). A large amount of data increases the performance of machine learning algorithms and avoids over-fitting problems. Creating a large amount of data in the agricultural sector for the design of models for the diagnosis and detection of plant diseases is an open and challenging task that takes a lot of time and resources. One way to increase the dataset will be to use data augmentation techniques. It increases the diversity of training data for machine learning algorithms without collecting new data.","title":"Detailed Description"},{"location":"enabling_technologies/WP3/UDANET_2/#contribution-and-improvements","text":"The improving objective of this component is to develop and test the algorithms, increase the reference dataset using basic image manipulation and deep learning based image augmentation techniques such as image flipping, cropping, rotation, colour transformation, PCA colour augmentation, noise injection, Generative Adversarial Networks (GANs) and Neural Style Transfer (NST) techniques. Performance of the data augmentation techniques was studied using state of the art transfer learning techniques both in terms of accuracy and computational cost.","title":"Contribution and Improvements"},{"location":"enabling_technologies/WP3/UDANET_2/#design-and-implementation","text":"It is developed through the Tensorflow and Python framework.","title":"Design and Implementation"},{"location":"enabling_technologies/WP3/UNIMORE/","text":"WP3-22 \u2013 Onboard Overlay Compute Platform (OOCP) ID WP3-22 Contributor UNIMORE Levels System Require Application definition and FPGA-based System-on-Chip Provide Accelerator-rich Overlay for FPGA-based System-on-Chip Input Application Specific HW Accelerators description in HDL, or HLS, or using WP3-28 Methodology Output Synthesizable Overlay for FPGA with integrated Application Specific HW Accelerators C4D building block The methodology is generic and applicable to host HW accelerators for different tasks and scenarios. With respect to C4D, it could be used to implement HW accelerators related to perception, actuation, flight-control, payload management or data management. TRL 4 Figure 84: Building Block diagram for WP3-22 Detailed Description The Onboard Overlay Compute Platform Design Methodology is composed of two main contributions: Onboard Overlay Compute Platform (OOCP) . The OOCP is an evolution of the HERO architecture targeting specifically FPGA acceleration on FPGA-based heterogeneous systems-on-chip (HeSoCs). Onboard Overlay Development Kit (OODK) . The OODK contains tools and library for the automatic integration, programming, and offloading to Application Specific Hardware Accelerators. The Onboard Overlay Compute Platform Design Methodology is generic and applicable to host accelerators for different tasks and scenarios. With respect to C4D Drone Reference Architecture, the OOCP could potentially be used to host and integrate on a FPGA-based SoC different HW accelerators (e.g., perception, actuation, flight-control, payload management or data management). Specifications and contribution Increased demand of autonomy on UAV requires adequate on-board smart sensing and computing capability to support safe decision making, based on large amounts of data that is sensed, analysed and understood in real-time. The capability of flexibly defining parallel, non-Von-Neumann processing logic and custom memory hierarchies, all within contained power envelopes, makes the FPGA-based heterogeneous systems-on-chip (HeSoCs) an ideal candidate for implementing onboard compute platforms for UAV. Withing the C4D project, we are developing an Onboard Overlay Compute Platform Design Methodology for FPGA-based HeSoCs and leverages soft-cores for flexible control of user-defined, application-specific accelerators. Different accelerators can flexibly operate and re-configure their operation without the costly need for host intervention, thus avoiding significant performance degradation. Normal accelerator operation and accelerator reconfiguration can both be achieved via standard computation offloading from the host CPU to the soft-cores (e.g., OpenMP v4.x+). The user can rely on any methodology of his/her/their choice to design the accelerators (e.g., by WP3-28 C4D components or Vivado HLS). Moreover, the Onboard Overlay Compute Platform includes dedicated logic (the wrapper) to provide plug-and-play HW/SW integration of such accelerators developed within C4D WP6 activities. Design and Implementation Figure 84 shows an overview of the proposed Onboard Overlay Compute Platform (OOCP). The Onboard Overlay is based on The Parallel Ultra Low Power Platform (PULP)1, and particularly on HERO2 is an open-source research platform based on FPGA emulation of PULP-based heterogeneous many-core systems. HERO can be instantiated on FPGA SoCs like the Xilinx Zynq family. HERO constitutes a convenient starting point to implement the Onboard Overlay Compute Platform: being conceived as a many-core architecture, HERO naturally complies with some of the basic requirements to build an accelerator-rich design, most notably the cluster-based design and the multi-bank shared memory design. However, HERO clusters are designed for general-purpose (or, at best, signal-processing oriented) parallel execution and thus have substantial limitations in the context of FPGA hardware acceleration that we target. HERO uses the FPGA merely as a medium for emulation of projects meant for IC realization. The proposed Onboard Overlay Compute Platform uses the FPGA as a target for acceleration. For an overlay to be an efficient and convenient solution, it should offer: (i) System-level design capabilities; (ii) transparent accelerator integration flow; (iii) streamlined resource usage. The Onboard Overlay Compute Platform is designed to be light (in terms of resources utilization) and configurable. The OOCP features a customized number of clusters, and each cluster is composed of one (or more) RISC-V IBEX 3core (RV32IMC), an instruction cache, a DMA, and a multi-ported multi-banked L1 Data Memory (scratchpad). The cluster can host one (or more) Application Specific Accelerators that can interfaced to the shared L1 Data Memory thought a wrapper.","title":"UNIMORE - Onboard Overlay Compute Platform (OOCP)"},{"location":"enabling_technologies/WP3/UNIMORE/#wp3-22-onboard-overlay-compute-platform-oocp","text":"ID WP3-22 Contributor UNIMORE Levels System Require Application definition and FPGA-based System-on-Chip Provide Accelerator-rich Overlay for FPGA-based System-on-Chip Input Application Specific HW Accelerators description in HDL, or HLS, or using WP3-28 Methodology Output Synthesizable Overlay for FPGA with integrated Application Specific HW Accelerators C4D building block The methodology is generic and applicable to host HW accelerators for different tasks and scenarios. With respect to C4D, it could be used to implement HW accelerators related to perception, actuation, flight-control, payload management or data management. TRL 4 Figure 84: Building Block diagram for WP3-22","title":"WP3-22 \u2013 Onboard Overlay Compute Platform (OOCP)"},{"location":"enabling_technologies/WP3/UNIMORE/#detailed-description","text":"The Onboard Overlay Compute Platform Design Methodology is composed of two main contributions: Onboard Overlay Compute Platform (OOCP) . The OOCP is an evolution of the HERO architecture targeting specifically FPGA acceleration on FPGA-based heterogeneous systems-on-chip (HeSoCs). Onboard Overlay Development Kit (OODK) . The OODK contains tools and library for the automatic integration, programming, and offloading to Application Specific Hardware Accelerators. The Onboard Overlay Compute Platform Design Methodology is generic and applicable to host accelerators for different tasks and scenarios. With respect to C4D Drone Reference Architecture, the OOCP could potentially be used to host and integrate on a FPGA-based SoC different HW accelerators (e.g., perception, actuation, flight-control, payload management or data management).","title":"Detailed Description"},{"location":"enabling_technologies/WP3/UNIMORE/#specifications-and-contribution","text":"Increased demand of autonomy on UAV requires adequate on-board smart sensing and computing capability to support safe decision making, based on large amounts of data that is sensed, analysed and understood in real-time. The capability of flexibly defining parallel, non-Von-Neumann processing logic and custom memory hierarchies, all within contained power envelopes, makes the FPGA-based heterogeneous systems-on-chip (HeSoCs) an ideal candidate for implementing onboard compute platforms for UAV. Withing the C4D project, we are developing an Onboard Overlay Compute Platform Design Methodology for FPGA-based HeSoCs and leverages soft-cores for flexible control of user-defined, application-specific accelerators. Different accelerators can flexibly operate and re-configure their operation without the costly need for host intervention, thus avoiding significant performance degradation. Normal accelerator operation and accelerator reconfiguration can both be achieved via standard computation offloading from the host CPU to the soft-cores (e.g., OpenMP v4.x+). The user can rely on any methodology of his/her/their choice to design the accelerators (e.g., by WP3-28 C4D components or Vivado HLS). Moreover, the Onboard Overlay Compute Platform includes dedicated logic (the wrapper) to provide plug-and-play HW/SW integration of such accelerators developed within C4D WP6 activities.","title":"Specifications and contribution"},{"location":"enabling_technologies/WP3/UNIMORE/#design-and-implementation","text":"Figure 84 shows an overview of the proposed Onboard Overlay Compute Platform (OOCP). The Onboard Overlay is based on The Parallel Ultra Low Power Platform (PULP)1, and particularly on HERO2 is an open-source research platform based on FPGA emulation of PULP-based heterogeneous many-core systems. HERO can be instantiated on FPGA SoCs like the Xilinx Zynq family. HERO constitutes a convenient starting point to implement the Onboard Overlay Compute Platform: being conceived as a many-core architecture, HERO naturally complies with some of the basic requirements to build an accelerator-rich design, most notably the cluster-based design and the multi-bank shared memory design. However, HERO clusters are designed for general-purpose (or, at best, signal-processing oriented) parallel execution and thus have substantial limitations in the context of FPGA hardware acceleration that we target. HERO uses the FPGA merely as a medium for emulation of projects meant for IC realization. The proposed Onboard Overlay Compute Platform uses the FPGA as a target for acceleration. For an overlay to be an efficient and convenient solution, it should offer: (i) System-level design capabilities; (ii) transparent accelerator integration flow; (iii) streamlined resource usage. The Onboard Overlay Compute Platform is designed to be light (in terms of resources utilization) and configurable. The OOCP features a customized number of clusters, and each cluster is composed of one (or more) RISC-V IBEX 3core (RV32IMC), an instruction cache, a DMA, and a multi-ported multi-banked L1 Data Memory (scratchpad). The cluster can host one (or more) Application Specific Accelerators that can interfaced to the shared L1 Data Memory thought a wrapper.","title":"Design and Implementation"},{"location":"enabling_technologies/WP3/UNISS/","text":"WP3-28 \u2013 Accelerator Design Methodology for OOCP ID WP3-28 Contributor UNISS Levels System Require Application definition and FPGA-based System-on-Chip Provide Ready-to-use reconfigurable HW accelerator Input Dataflow application specification(s) HDL actor definition(s) Communication protocol Target architecture Output Multi-dataflow network Coarse-grained reconfigurable accelerator RTL Co-processor RTL Programming tables C4D building block The methodology is generic and applicable to generate accelerators for different tasks and scenarios. With respect to C4D, it could POTENTIALLY be used to implement HW accelerators related to perception, actuation, flight-control, payload management or data management. TRL From 3 to 4 Figure 87: Building Block diagram for WP3-28 Detailed Description Although FPGA technology has the potential to satisfy the many performances, energy and predictability requirements of drone systems and applications, FPGA development is notoriously a complex task. To deal with this problematic, the baseline feature of this component revolves around the composition of coarse-grained reconfigurable HW accelerators (CGRA) starting from a set of dataflow applications. The baseline feature involves two main components: Multi-Dataflow Generator (MDG) : it merges together different dataflows into one unique reconfigurable multi-dataflow by the insertion of switching modules. Currently, two merging algorithms are supported: empiric and Moreano. The former is more suitable for non-recursive dataflows but less optimized than the latter. Platform composer (PC) : it derives the RTL description of the CGRA from the multi-dataflow. It requires the user to define the communication protocol between actors in hardware (XML) and the RTL description of the actors involved in the dataflows (HDL Components Library, HCL). This component also provides an automatic coprocessor generation, which automatically embeds the generated CGRA into a ready-to-use Xilinx IP. The user can choose among different options: Processor : soft-core (Microblaze) or hardcore (ARM) Processor-Coprocessor coupling : Memory-mapped or FIFO-based Direct Access Memory Module : enable or not the usage of DMA Specifications and contribution The purpose of this component is to provide a methodology to generate application-specific HW accelerators that can be directly plugged in the final system. Additionally, this component automatically enables coarse-grained reconfiguration capabilities, which enables the capability of having a HW accelerator that can work at different working points (i.e. trade-offs among Quality of Service and Energy consumption) or functionalities (i.e. different implementations related to the same C4D building block). Regarding the contribution associated to C4D, this component will be extended to be able to automatically generate plug-and-play coarse-grained reconfigurable HW accelerators that can be used by WP3-22 component. To do so, a unified methodology will be provided so as to combine the FPGA overlay provided in WP3-22 with the CGRA generation supported by this component. Design and Implementation Considering that the HW accelerators that are generated using this component are application specific, the required steps to generate the multi-dataflow networks and its associated CGRA are the following: Define the three inputs that are required: Implement the task(s) to be accelerated using a dataflow approach. Define the HDL version of the actors composing the tasks (manually or with HLS tools). Define the communication protocol to be used inside and outside the accelerator. Using the MDG functionality, if more than one task has been specified, merge the tasks to be accelerated into a reconfigurable multi-dataflow. Depending on the target architecture, the user must select the files to be generated, that could include the accelerator RTL description and a wrapping logic surrounding the accelerator itself that could allow the user to 1) use the coarse-grained reconfigurable accelerator as a co-processor or 2) to plug the accelerator with an FPGA overlay, as in the case of this project. Run the automatically generated scripts to port the code to Vivado. Synthesize and implement it on the target FPGA device.","title":"UNISS - Accelerator Design Methodology for OOCP"},{"location":"enabling_technologies/WP3/UNISS/#wp3-28-accelerator-design-methodology-for-oocp","text":"ID WP3-28 Contributor UNISS Levels System Require Application definition and FPGA-based System-on-Chip Provide Ready-to-use reconfigurable HW accelerator Input Dataflow application specification(s) HDL actor definition(s) Communication protocol Target architecture Output Multi-dataflow network Coarse-grained reconfigurable accelerator RTL Co-processor RTL Programming tables C4D building block The methodology is generic and applicable to generate accelerators for different tasks and scenarios. With respect to C4D, it could POTENTIALLY be used to implement HW accelerators related to perception, actuation, flight-control, payload management or data management. TRL From 3 to 4 Figure 87: Building Block diagram for WP3-28","title":"WP3-28 \u2013 Accelerator Design Methodology for OOCP"},{"location":"enabling_technologies/WP3/UNISS/#detailed-description","text":"Although FPGA technology has the potential to satisfy the many performances, energy and predictability requirements of drone systems and applications, FPGA development is notoriously a complex task. To deal with this problematic, the baseline feature of this component revolves around the composition of coarse-grained reconfigurable HW accelerators (CGRA) starting from a set of dataflow applications. The baseline feature involves two main components: Multi-Dataflow Generator (MDG) : it merges together different dataflows into one unique reconfigurable multi-dataflow by the insertion of switching modules. Currently, two merging algorithms are supported: empiric and Moreano. The former is more suitable for non-recursive dataflows but less optimized than the latter. Platform composer (PC) : it derives the RTL description of the CGRA from the multi-dataflow. It requires the user to define the communication protocol between actors in hardware (XML) and the RTL description of the actors involved in the dataflows (HDL Components Library, HCL). This component also provides an automatic coprocessor generation, which automatically embeds the generated CGRA into a ready-to-use Xilinx IP. The user can choose among different options: Processor : soft-core (Microblaze) or hardcore (ARM) Processor-Coprocessor coupling : Memory-mapped or FIFO-based Direct Access Memory Module : enable or not the usage of DMA","title":"Detailed Description"},{"location":"enabling_technologies/WP3/UNISS/#specifications-and-contribution","text":"The purpose of this component is to provide a methodology to generate application-specific HW accelerators that can be directly plugged in the final system. Additionally, this component automatically enables coarse-grained reconfiguration capabilities, which enables the capability of having a HW accelerator that can work at different working points (i.e. trade-offs among Quality of Service and Energy consumption) or functionalities (i.e. different implementations related to the same C4D building block). Regarding the contribution associated to C4D, this component will be extended to be able to automatically generate plug-and-play coarse-grained reconfigurable HW accelerators that can be used by WP3-22 component. To do so, a unified methodology will be provided so as to combine the FPGA overlay provided in WP3-22 with the CGRA generation supported by this component.","title":"Specifications and contribution"},{"location":"enabling_technologies/WP3/UNISS/#design-and-implementation","text":"Considering that the HW accelerators that are generated using this component are application specific, the required steps to generate the multi-dataflow networks and its associated CGRA are the following: Define the three inputs that are required: Implement the task(s) to be accelerated using a dataflow approach. Define the HDL version of the actors composing the tasks (manually or with HLS tools). Define the communication protocol to be used inside and outside the accelerator. Using the MDG functionality, if more than one task has been specified, merge the tasks to be accelerated into a reconfigurable multi-dataflow. Depending on the target architecture, the user must select the files to be generated, that could include the accelerator RTL description and a wrapping logic surrounding the accelerator itself that could allow the user to 1) use the coarse-grained reconfigurable accelerator as a co-processor or 2) to plug the accelerator with an FPGA overlay, as in the case of this project. Run the automatically generated scripts to port the code to Vivado. Synthesize and implement it on the target FPGA device.","title":"Design and Implementation"},{"location":"enabling_technologies/WP3/UNIVAQ/","text":"Efficient digital implementation of controllers ID: WP3-24 Contributor: UNIVAQ Owner: UNIVAQ Licence: OPEN SOURCE expected TRL: 4-5 KET: Command and control; Detect and Avoid; Intelligent Mission Management; Fail-safe Mission; Obstacle Detection and Avoidance; Outdoor positioning and Attitude; Data fusion and processing. Contact: Stefano Di Gennaro (stefano.digennaro@univaq.it); Mario Di Ferdinando (mario.diferdinando@univaq.it). Efficient digital implementation of controllers on FPGAs. In the context of C4D project, the component WP3-24 aims to provide an efficient methodology for the digital implementation of controllers on FPGAs. It is well-known that the use of UAVs in many complex tasks has increased the complexity of the embedded control algorithms that are necessary in order to face more challenging performances, such as: detection and avoiding of obstacles; cooperation among drones; efficient trajectory execution; etc. However, many software solutions also present some limitations, due to its fixed internal architecture. This leads to a full serialization of the data treatment. The more complex is the control and decision algorithm, the longer is its execution time. This, in turn, constitutes a lower bound for the sampling time that can be used in the specific application. Clearly, longer sampling times determine worse controller performances. To obtain higher control performances, one can work in two possible directions. The first is methodological, and consists of designing the control algorithms on the basis of better discrete-time dynamic representations of the vehicle. The second is technological, and regards the use of more performing devices used to implement a given controller. As far as the technological solution is concerned, field-programmable gate arrays (FPGAs) can ensure better performances than software solutions, thanks to the possibility of parallelism and to the increasing integration density, which allows implementing complex control algorithms. In fact, FPGAs are full system-on-chip (SoC) solutions. They allow more flexibility for the implementation of embedded controllers, due to the fact that they include in the same chip various components (processors, memories, hardware multiplier blocks, analog\u2013digital converters, matrix of programmable logic elements-fabric-and buses). The fact that FPGAs integrate both software and hardware resources allow faster implementations of controllers making use of the parallelism. Therefore, FPGAs constitute a valid hardware solution, since it is possible to design an architecture that is customized for the control algorithm to be implemented. This ensures shorter execution times of the algorithm. To further reduce the execution time in FPGAs, some techniques can be used that allow transforming the circuit structure, in order to reduce this time, and possibly the power consumption, maintaining the desired functionality, i.e., implementing the required control input. These (methodological, not technological) techniques include retiming/pipelining, folding/unfolding, interleaving, etc. The proposed methodology for the efficient implementation of controllers on FPGA is focused on retiming and pipelining. The former is a transformation technique used to change the locations of the delay elements in a circuit without affecting the input/output characteristics of the circuit. Pipelining is a special case of retiming used to reduce the critical path, introducing pipelining latches along the data path. Shortening the critical paths, one can increase the clock speed or the sample speed, or one can reduce the power consumption at the same speed. The pipelined implementation technique, here proposed in the context of UAVs control (WP3-24), allows lower execution times, and hence smaller sampling times, than its naive implementation. Moreover, the power consumption of the pipelined control algorithm is lower. These two aspects constitute the main benefits of using a pipelining technique for the implementation of UAV control algorithm on an FPGA. In other words, the main aim behind the component (WP3-24) is to provide an implementation guideline for UAVs control algorithms showing that, when technological solutions such as FPGAs are used, the pipelining methodology can be successfully applied to obtain lower sampling periods, thereby allowing the implementation of more sophisticated controllers for UAVs. The efficiency of the proposed implementation methodology is shown by developing on FPGA a robust sampled\u2014data controller for the autonomous navigation of drone which has been designed in the context of WP4 for C4D project. Finally, through experimental results, it can be shown that the pipelining methodology also allows taking into account the energetic aspects of the controller implementations, and not only the controller performance. This is another very important aspect for onboard systems as in UAVs.","title":"UNIVAQ - Efficient digital implementation of controllers"},{"location":"enabling_technologies/WP3/UNIVAQ/#efficient-digital-implementation-of-controllers","text":"ID: WP3-24 Contributor: UNIVAQ Owner: UNIVAQ Licence: OPEN SOURCE expected TRL: 4-5 KET: Command and control; Detect and Avoid; Intelligent Mission Management; Fail-safe Mission; Obstacle Detection and Avoidance; Outdoor positioning and Attitude; Data fusion and processing. Contact: Stefano Di Gennaro (stefano.digennaro@univaq.it); Mario Di Ferdinando (mario.diferdinando@univaq.it). Efficient digital implementation of controllers on FPGAs. In the context of C4D project, the component WP3-24 aims to provide an efficient methodology for the digital implementation of controllers on FPGAs. It is well-known that the use of UAVs in many complex tasks has increased the complexity of the embedded control algorithms that are necessary in order to face more challenging performances, such as: detection and avoiding of obstacles; cooperation among drones; efficient trajectory execution; etc. However, many software solutions also present some limitations, due to its fixed internal architecture. This leads to a full serialization of the data treatment. The more complex is the control and decision algorithm, the longer is its execution time. This, in turn, constitutes a lower bound for the sampling time that can be used in the specific application. Clearly, longer sampling times determine worse controller performances. To obtain higher control performances, one can work in two possible directions. The first is methodological, and consists of designing the control algorithms on the basis of better discrete-time dynamic representations of the vehicle. The second is technological, and regards the use of more performing devices used to implement a given controller. As far as the technological solution is concerned, field-programmable gate arrays (FPGAs) can ensure better performances than software solutions, thanks to the possibility of parallelism and to the increasing integration density, which allows implementing complex control algorithms. In fact, FPGAs are full system-on-chip (SoC) solutions. They allow more flexibility for the implementation of embedded controllers, due to the fact that they include in the same chip various components (processors, memories, hardware multiplier blocks, analog\u2013digital converters, matrix of programmable logic elements-fabric-and buses). The fact that FPGAs integrate both software and hardware resources allow faster implementations of controllers making use of the parallelism. Therefore, FPGAs constitute a valid hardware solution, since it is possible to design an architecture that is customized for the control algorithm to be implemented. This ensures shorter execution times of the algorithm. To further reduce the execution time in FPGAs, some techniques can be used that allow transforming the circuit structure, in order to reduce this time, and possibly the power consumption, maintaining the desired functionality, i.e., implementing the required control input. These (methodological, not technological) techniques include retiming/pipelining, folding/unfolding, interleaving, etc. The proposed methodology for the efficient implementation of controllers on FPGA is focused on retiming and pipelining. The former is a transformation technique used to change the locations of the delay elements in a circuit without affecting the input/output characteristics of the circuit. Pipelining is a special case of retiming used to reduce the critical path, introducing pipelining latches along the data path. Shortening the critical paths, one can increase the clock speed or the sample speed, or one can reduce the power consumption at the same speed. The pipelined implementation technique, here proposed in the context of UAVs control (WP3-24), allows lower execution times, and hence smaller sampling times, than its naive implementation. Moreover, the power consumption of the pipelined control algorithm is lower. These two aspects constitute the main benefits of using a pipelining technique for the implementation of UAV control algorithm on an FPGA. In other words, the main aim behind the component (WP3-24) is to provide an implementation guideline for UAVs control algorithms showing that, when technological solutions such as FPGAs are used, the pipelining methodology can be successfully applied to obtain lower sampling periods, thereby allowing the implementation of more sophisticated controllers for UAVs. The efficiency of the proposed implementation methodology is shown by developing on FPGA a robust sampled\u2014data controller for the autonomous navigation of drone which has been designed in the context of WP4 for C4D project. Finally, through experimental results, it can be shown that the pipelining methodology also allows taking into account the energetic aspects of the controller implementations, and not only the controller performance. This is another very important aspect for onboard systems as in UAVs.","title":"Efficient digital implementation of controllers"},{"location":"enabling_technologies/WP3/UWB/","text":"WP3-26 Droneport: an autonomous drone battery management system ID WP3-26 Contributor UWB Levels Functional Require Communication service to obtain data from the assigned drones and master controller Provide Autonomous battery management and robotic battery exchange Input UAV mission, UAV battery status Output Available battery resources C4D building block Health management, Mission management TRL 4 Detailed Description Droneport (DP) is a system for autonomous drone battery management. Droneport complies with a reference architecture, dealing with drone power management, more precisely battery management. Regarding the reference architecture, the DP consists of a battery management unit for charging and storage, a data link to the drone, and telemetry data. Power and storage management building blocks are implemented in Droneport, with a datalink connection to the outside world. Users can use telemetry data via MAVLink protocol to monitor the state of the DP. The DP acts autonomously, it monitors the state of the batteries, controls the charging and battery exchange process provides necessary navigation information for drone landing and broadcasts the status of the remaining batteries. Contribution and Improvement The Droneport aims to extend the UAV mission by autonomously changing the battery of the drone. It behaves as a standalone autonomous unit that is wirelessly interconnected with one or many UAVs. It is compatible with other C4D components. it uses an open MAVLink protocol with defined messages for communication. At the beginning of the project, there was only a notion of how to extend the UAVs mission. We are targeting TRL 4 (the prototype) at the end. The device would autonomously manage the batteries and cooperates with selected drones. Design and Implementation Figure 85: Droneport manipulator arm with a custom gripper The Droneport development is done together with SmartMotion company. Our team collaborates together on hardware and software design. The DP will be a unique device that will be able to communicate with various UAVs. For better support of battery exchange, the DP contains a newly designed gripper and a special battery holder. Droneport hardware consists of: Drone landing place with ArUco based landing markers Uniquely designed robotic manipulator Uniquely designed gripper Battery management unit for charging and storage Wireless communication to the drone/swarms The whole device will communicate wirelessly over MAVLink protocol [29] with other devices in the network. The device is controlled using Linux based PC with REXYGEN [34] as real-time control software. Droneport SW architecture consists of: Drone to DP communication protocol (defined MAVLink messages) Drone landing assistant - vision navigation to ArUco markers Battery exchange system control - manipulator controller Charge control - communication interface between charges and battery slots Battery management software DP software provides an open API for interoperability with various drones flight controllers. Figure 86: Top view of the Droneport with landed drone","title":"UWB - Droneport, an autonomous drone battery management system"},{"location":"enabling_technologies/WP3/UWB/#wp3-26-droneport-an-autonomous-drone-battery-management-system","text":"ID WP3-26 Contributor UWB Levels Functional Require Communication service to obtain data from the assigned drones and master controller Provide Autonomous battery management and robotic battery exchange Input UAV mission, UAV battery status Output Available battery resources C4D building block Health management, Mission management TRL 4","title":"WP3-26 Droneport: an autonomous drone battery management system"},{"location":"enabling_technologies/WP3/UWB/#detailed-description","text":"Droneport (DP) is a system for autonomous drone battery management. Droneport complies with a reference architecture, dealing with drone power management, more precisely battery management. Regarding the reference architecture, the DP consists of a battery management unit for charging and storage, a data link to the drone, and telemetry data. Power and storage management building blocks are implemented in Droneport, with a datalink connection to the outside world. Users can use telemetry data via MAVLink protocol to monitor the state of the DP. The DP acts autonomously, it monitors the state of the batteries, controls the charging and battery exchange process provides necessary navigation information for drone landing and broadcasts the status of the remaining batteries.","title":"Detailed Description"},{"location":"enabling_technologies/WP3/UWB/#contribution-and-improvement","text":"The Droneport aims to extend the UAV mission by autonomously changing the battery of the drone. It behaves as a standalone autonomous unit that is wirelessly interconnected with one or many UAVs. It is compatible with other C4D components. it uses an open MAVLink protocol with defined messages for communication. At the beginning of the project, there was only a notion of how to extend the UAVs mission. We are targeting TRL 4 (the prototype) at the end. The device would autonomously manage the batteries and cooperates with selected drones.","title":"Contribution and Improvement"},{"location":"enabling_technologies/WP3/UWB/#design-and-implementation","text":"Figure 85: Droneport manipulator arm with a custom gripper The Droneport development is done together with SmartMotion company. Our team collaborates together on hardware and software design. The DP will be a unique device that will be able to communicate with various UAVs. For better support of battery exchange, the DP contains a newly designed gripper and a special battery holder. Droneport hardware consists of: Drone landing place with ArUco based landing markers Uniquely designed robotic manipulator Uniquely designed gripper Battery management unit for charging and storage Wireless communication to the drone/swarms The whole device will communicate wirelessly over MAVLink protocol [29] with other devices in the network. The device is controlled using Linux based PC with REXYGEN [34] as real-time control software. Droneport SW architecture consists of: Drone to DP communication protocol (defined MAVLink messages) Drone landing assistant - vision navigation to ArUco markers Battery exchange system control - manipulator controller Charge control - communication interface between charges and battery slots Battery management software DP software provides an open API for interoperability with various drones flight controllers. Figure 86: Top view of the Droneport with landed drone","title":"Design and Implementation"},{"location":"enabling_technologies/other_WP/SCALIAN-ai_stabilization/","text":"EZ_Chains AI Stabilization ID: WP4-42 Contributor: SCALIAN Owner: SCALIAN Licence: Proprietary expected TRL: 4 KET: 1.1.6 Command and control, 2.4.1 Data fusion and processing Contact: david.cherel@scalian.com Scalian has experimented with using Deep Reinforcement Learning (DRL) to stabilise UAS flight. The goal is to experiment with the robustness this solution could offer. We train and test our algorithms on a complex drone model. In addition, we take into account the measurement noise and the introduction of disturbances, to test the robustness of the DRL algorithms. To test DRL algorithms, we use the \"OpenAI Baselines\" API available in open-source on GitHub. This API comes with a set of ready-to-use DRL algorithms. It allows researchers to test and improve DRL algorithms on their projects without needing to redevelop everything from scratch. DRL algorithms are generally implemented on simulators developed in a test environment provided by OpenAI. This simulation environment is called \u201cGym\u201d. The developers of OpenAI as well as contributors in this community have provided researchers with \"Gym\"-based simulators for different types of dynamic systems such as the inverted pendulum, the autonomous car, and the drone. These simulators also allow visualization of the behavior of these dynamic systems on a graphical interface while testing DRL algorithms to allow researchers to assess their results. In our work, we used a drone simulator, quadgym, developed in the \"Gym\" environment and based on a more complex mathematical model than the models mentioned in the state of the art. The simulations have shown promising results, so the next step would be to deploy these methods on real drones to assess the quality of these approaches and achieve complete autonomy with as few errors as possible.","title":"SCALIAN \u2013 AI Stabilization"},{"location":"enabling_technologies/other_WP/SCALIAN-ai_stabilization/#ez_chains-ai-stabilization","text":"ID: WP4-42 Contributor: SCALIAN Owner: SCALIAN Licence: Proprietary expected TRL: 4 KET: 1.1.6 Command and control, 2.4.1 Data fusion and processing Contact: david.cherel@scalian.com Scalian has experimented with using Deep Reinforcement Learning (DRL) to stabilise UAS flight. The goal is to experiment with the robustness this solution could offer. We train and test our algorithms on a complex drone model. In addition, we take into account the measurement noise and the introduction of disturbances, to test the robustness of the DRL algorithms. To test DRL algorithms, we use the \"OpenAI Baselines\" API available in open-source on GitHub. This API comes with a set of ready-to-use DRL algorithms. It allows researchers to test and improve DRL algorithms on their projects without needing to redevelop everything from scratch. DRL algorithms are generally implemented on simulators developed in a test environment provided by OpenAI. This simulation environment is called \u201cGym\u201d. The developers of OpenAI as well as contributors in this community have provided researchers with \"Gym\"-based simulators for different types of dynamic systems such as the inverted pendulum, the autonomous car, and the drone. These simulators also allow visualization of the behavior of these dynamic systems on a graphical interface while testing DRL algorithms to allow researchers to assess their results. In our work, we used a drone simulator, quadgym, developed in the \"Gym\" environment and based on a more complex mathematical model than the models mentioned in the state of the art. The simulations have shown promising results, so the next step would be to deploy these methods on real drones to assess the quality of these approaches and achieve complete autonomy with as few errors as possible.","title":"EZ_Chains AI Stabilization"},{"location":"enabling_technologies/other_WP/SCALIAN-clearance/","text":"EZ_Chains Clearance ID: WP4-5 Contributor: SCALIAN Owner: SCALIAN Licence: Proprietary expected TRL: 6 KET: 2.2.7 Obstacle Detection and Avoindance, 2.4.1 Data fusion and processing, 3.1.2 Passive Optical Contact: david.cherel@scalian.com or mathieu.damour@scalian.com This module aims at providing an automatic method for person detection, vehicles and animals able to do real time analysis of video feeds while embedded on a drone. It is mainly used in the UC3 D1, where a fleet of UAS are dropping sensors. The component returns the respective position in the image of the detected elements and if an element is detected the dropping operation is aborted. The component takes input both from RGB and thermal cameras, the camera can be set in Nadir or oblique view. Since the view point is different, the detection will use different trained models. The component is developped using Tensorflow and Keras for the deep learning. In order to allow real-time detect, a VPU is added to the UAS onboard companion computer. We are using an Intel Neural Compute Stick 2 (NCS2) with the OpenVino framework. Not all instructions available in Tensorflow and Keras can be found in this framework, so some of the operations are kept on the companion computer. The figure below presents the different pipelines available, allowing different detection and outputs: In order to be more generic the component has been develop in the form of a training framework: it is easier to train the system for a new use-case. In order to demonstrate this feature, the component has been trained to detect vehicules on road: N.B.: This model has been not been properly trained, it jsut servesas a demonstrator of the new framework capabilities.","title":"SCALIAN \u2013 Clearance"},{"location":"enabling_technologies/other_WP/SCALIAN-clearance/#ez_chains-clearance","text":"ID: WP4-5 Contributor: SCALIAN Owner: SCALIAN Licence: Proprietary expected TRL: 6 KET: 2.2.7 Obstacle Detection and Avoindance, 2.4.1 Data fusion and processing, 3.1.2 Passive Optical Contact: david.cherel@scalian.com or mathieu.damour@scalian.com This module aims at providing an automatic method for person detection, vehicles and animals able to do real time analysis of video feeds while embedded on a drone. It is mainly used in the UC3 D1, where a fleet of UAS are dropping sensors. The component returns the respective position in the image of the detected elements and if an element is detected the dropping operation is aborted. The component takes input both from RGB and thermal cameras, the camera can be set in Nadir or oblique view. Since the view point is different, the detection will use different trained models. The component is developped using Tensorflow and Keras for the deep learning. In order to allow real-time detect, a VPU is added to the UAS onboard companion computer. We are using an Intel Neural Compute Stick 2 (NCS2) with the OpenVino framework. Not all instructions available in Tensorflow and Keras can be found in this framework, so some of the operations are kept on the companion computer. The figure below presents the different pipelines available, allowing different detection and outputs: In order to be more generic the component has been develop in the form of a training framework: it is easier to train the system for a new use-case. In order to demonstrate this feature, the component has been trained to detect vehicules on road: N.B.: This model has been not been properly trained, it jsut servesas a demonstrator of the new framework capabilities.","title":"EZ_Chains Clearance"},{"location":"enabling_technologies/other_WP/SCALIAN-ez_land/","text":"EZ_Land Precision Landing ID: WP4-2 Contributor: SCALIAN Owner: SCALIAN Licence: Proprietary expected TRL: 5 KET: 2.3 Positioning, 2.2.1 Take-off, 2.2.2 Landing, 2.2.7 Obstacle Detecton and Avoidance, 2.4.1 Data fusion and processing, 3.1.2 Passive Optical Contact: david.cherel@scalian.com The precision landing is a frequent subject for autonomous multicopter because it is an essential component for safety and autonomy in a drone system. Indeed, when relying solely on the GPS signal and IMU data, a drone can have a landing offset up to 5 meters if the signal is bad. Such an offset can be very dangerous: the drone can land on a damaged zone, risking the physical integrity of the drone, or land on a zone with human operators. To demonstrate the efficiency of such a component, the precision landing will be deployed during the METIS use case (UC3-Demo1) to improve the safety of the system and to facilitate the operations of refill and reload of the drone when landed. It will also be deployed during the use case of ATECHSYS (UC3-Demo2) where the multicopter needs to land precisely on a droid TwinswHeel to pick up and drop off a package. Paired up with the clearance component (human detection), the landing will be cancelled if a human is detected near the landing zone, making this component safer. The goal of the component is to ensure a safe , autonomous and precise landing . The component of SCALIAN aims at exploiting the strength of several sensors to ensure a robust, safe and precise landing. To answer the needs of UC3-Demo1 and UC3-Demo2, the sensor IR lock and a computer vision algorithm are integrated in the component. The visual processing algorithm is specific to a design of helipad (on the right). Aruco type markers are not used to have a more robust algorithm. Shadows and creases have a smaller impact on a refined helipad. The component is based on a modular architecture that allows the users to configure which sensors are needed. With this conception, it is also easy to integrate a new type of sensor. It is then possible to fine tune the component to the use-case and its constraints. EZ_Follow - Ground target following EZ_Follow is a mode of EZ_Land, it uses the algorithms to track and follow a ground target (the dedicated dronepad). Thanks to this mode, it is possible to build missions where a UAV take-off from a pad, follows it for a given time and lands back precisely. We have used this mode, on a tethered UAV carrying a camera, to construct a surveillance system very easy and convenient to deploy anywhere: a vehicle carries the pad on its back with the tethered UAV ready to take-off. When arrived where the inspection is required the UAV takes-off, then the vehicle can continue to drive slowly, the UAV will follow it. During the phase, the camera operator (or security personnel) can focus only on its mission, the inspection. Indeed the UAV maintains its altitude and position relative to the vehicle. When the mission is done, the UAV lands precisely on the back of the vehicle (the precision ensures that it will not fall off), finally the vehicle can move to the next inspection area. This system is called Long-Eye .","title":"SCALIAN \u2013 EZ_land Precision Landing"},{"location":"enabling_technologies/other_WP/SCALIAN-ez_land/#ez_land-precision-landing","text":"ID: WP4-2 Contributor: SCALIAN Owner: SCALIAN Licence: Proprietary expected TRL: 5 KET: 2.3 Positioning, 2.2.1 Take-off, 2.2.2 Landing, 2.2.7 Obstacle Detecton and Avoidance, 2.4.1 Data fusion and processing, 3.1.2 Passive Optical Contact: david.cherel@scalian.com The precision landing is a frequent subject for autonomous multicopter because it is an essential component for safety and autonomy in a drone system. Indeed, when relying solely on the GPS signal and IMU data, a drone can have a landing offset up to 5 meters if the signal is bad. Such an offset can be very dangerous: the drone can land on a damaged zone, risking the physical integrity of the drone, or land on a zone with human operators. To demonstrate the efficiency of such a component, the precision landing will be deployed during the METIS use case (UC3-Demo1) to improve the safety of the system and to facilitate the operations of refill and reload of the drone when landed. It will also be deployed during the use case of ATECHSYS (UC3-Demo2) where the multicopter needs to land precisely on a droid TwinswHeel to pick up and drop off a package. Paired up with the clearance component (human detection), the landing will be cancelled if a human is detected near the landing zone, making this component safer. The goal of the component is to ensure a safe , autonomous and precise landing . The component of SCALIAN aims at exploiting the strength of several sensors to ensure a robust, safe and precise landing. To answer the needs of UC3-Demo1 and UC3-Demo2, the sensor IR lock and a computer vision algorithm are integrated in the component. The visual processing algorithm is specific to a design of helipad (on the right). Aruco type markers are not used to have a more robust algorithm. Shadows and creases have a smaller impact on a refined helipad. The component is based on a modular architecture that allows the users to configure which sensors are needed. With this conception, it is also easy to integrate a new type of sensor. It is then possible to fine tune the component to the use-case and its constraints.","title":"EZ_Land Precision Landing"},{"location":"enabling_technologies/other_WP/SCALIAN-ez_land/#ez_follow-ground-target-following","text":"EZ_Follow is a mode of EZ_Land, it uses the algorithms to track and follow a ground target (the dedicated dronepad). Thanks to this mode, it is possible to build missions where a UAV take-off from a pad, follows it for a given time and lands back precisely. We have used this mode, on a tethered UAV carrying a camera, to construct a surveillance system very easy and convenient to deploy anywhere: a vehicle carries the pad on its back with the tethered UAV ready to take-off. When arrived where the inspection is required the UAV takes-off, then the vehicle can continue to drive slowly, the UAV will follow it. During the phase, the camera operator (or security personnel) can focus only on its mission, the inspection. Indeed the UAV maintains its altitude and position relative to the vehicle. When the mission is done, the UAV lands precisely on the back of the vehicle (the precision ensures that it will not fall off), finally the vehicle can move to the next inspection area. This system is called Long-Eye .","title":"EZ_Follow - Ground target following"},{"location":"enabling_technologies/other_WP/SCALIAN-safe_fleet_comm/","text":"EZ_Com Safe fleet communication ID: WP5-03-SCALIAN Contributor: SCALIAN Owner: SCALIAN Licence: Proprietary expected TRL: 5 KET: 1.3.1 Vehicle to Vehicle communication, 1.3.2 Vehicle to Infrastructure communication, 2.5.2 Swarm formation and cooperation Contact: david.cherel@scalian.com : Scalian has worked on developing a generic architecture to allow fleet of UAVs or miscellaneous agents to perform a variety of missions. This architecture is composed of several components and a Knowledge Base whose role is to store information on the mission status and UAVs status. In a system like this, the communication system is very important to provide all the information of each agent to each other. This module is the abstraction of the communication to ensure that the agents KB are correctly sychronised. On the hardware part of the communication system, Scalian has decided to use a 4G private network. However this solution is expensive and require operations (deployement of antenna, and so on) prior to any UAV operations. So Scalian aims at creating a component for safe fleet communcation along two axis: Use 4G public network, and setup a Virtual Private Netork (VPN) to ensure a proper level of safety and security. Additionnaly, dedicated services will be configured: e.g. video stream. Replace the 4G network with a new communication mean. A solution could be a mesh network. Both axis will be sutdied in parallel and could require to be merged into one, depending on the results obtained during the C4D project.","title":"SCALIAN \u2013 AI Stabilization"},{"location":"enabling_technologies/other_WP/SCALIAN-safe_fleet_comm/#ez_com-safe-fleet-communication","text":"ID: WP5-03-SCALIAN Contributor: SCALIAN Owner: SCALIAN Licence: Proprietary expected TRL: 5 KET: 1.3.1 Vehicle to Vehicle communication, 1.3.2 Vehicle to Infrastructure communication, 2.5.2 Swarm formation and cooperation Contact: david.cherel@scalian.com : Scalian has worked on developing a generic architecture to allow fleet of UAVs or miscellaneous agents to perform a variety of missions. This architecture is composed of several components and a Knowledge Base whose role is to store information on the mission status and UAVs status. In a system like this, the communication system is very important to provide all the information of each agent to each other. This module is the abstraction of the communication to ensure that the agents KB are correctly sychronised. On the hardware part of the communication system, Scalian has decided to use a 4G private network. However this solution is expensive and require operations (deployement of antenna, and so on) prior to any UAV operations. So Scalian aims at creating a component for safe fleet communcation along two axis: Use 4G public network, and setup a Virtual Private Netork (VPN) to ensure a proper level of safety and security. Additionnaly, dedicated services will be configured: e.g. video stream. Replace the 4G network with a new communication mean. A solution could be a mesh network. Both axis will be sutdied in parallel and could require to be merged into one, depending on the results obtained during the C4D project.","title":"EZ_Com Safe fleet communication"},{"location":"enabling_technologies/other_WP/TEKNE_WP4_18_A/","text":"Drone-Rover Transponder ID: WP4-18 Contributor: TEKNE Owner: TEKNE Licence: Proprietary expected TRL: 5 KET: 2.5.1 Drone and Rover Contact: https://en.tekne.it/ The UAV-UGV Transponder component estimates the still UGV (Unmanned Ground Vehicle) position with respect to the UAV (Unmanned Aerial Vehicle) on the basis of the distance between the two vehicles and of the UAV autopilot navigation data. The distance is computed from the measurements of Time Of Flight (TOF \u2014 time of electromagnetic propagation) taken by the Ultra-Wideband (UWB) transceivers which both vehicles are equipped with. To land on the UGV, the component drives the UAV towards a position that is given with respect to the UGV (on the vertical, at a certain height). The UAV moves by navigating on the basis of GPS data. At the end of this approaching maneuver, due to the limited GPS accuracy, it may happen that the UGV (the landing pad) is not in the field of view of the UAV dedicated video camera that controls the landing. The component, which doesn\u2019t require optical visibility, executes the positioning maneuver to bring the UAV to a position such that the autonomous landing maneuver can start.","title":"TEKNE \u2013 UAV-UGV Transponder"},{"location":"enabling_technologies/other_WP/TEKNE_WP4_18_A/#drone-rover-transponder","text":"ID: WP4-18 Contributor: TEKNE Owner: TEKNE Licence: Proprietary expected TRL: 5 KET: 2.5.1 Drone and Rover Contact: https://en.tekne.it/ The UAV-UGV Transponder component estimates the still UGV (Unmanned Ground Vehicle) position with respect to the UAV (Unmanned Aerial Vehicle) on the basis of the distance between the two vehicles and of the UAV autopilot navigation data. The distance is computed from the measurements of Time Of Flight (TOF \u2014 time of electromagnetic propagation) taken by the Ultra-Wideband (UWB) transceivers which both vehicles are equipped with. To land on the UGV, the component drives the UAV towards a position that is given with respect to the UGV (on the vertical, at a certain height). The UAV moves by navigating on the basis of GPS data. At the end of this approaching maneuver, due to the limited GPS accuracy, it may happen that the UGV (the landing pad) is not in the field of view of the UAV dedicated video camera that controls the landing. The component, which doesn\u2019t require optical visibility, executes the positioning maneuver to bring the UAV to a position such that the autonomous landing maneuver can start.","title":"Drone-Rover Transponder"},{"location":"enabling_technologies/other_WP/TEKNE_WP5_05_A/","text":"LP-WAN for UAV identification and monitoring ID: WP5-05 Contributor: TEKNE Owner: TEKNE Licence: Proprietary expected TRL: 5 KET: 1.1.1 E-Identification, 2.4.2 Intelligent Vehicle System Monitoring Contact: https://en.tekne.it/ The LP-WAN for UAV (Unmanned Aerial Vehicle) identification and monitoring component supports medium/long-range wireless communications for UAV identification, monitoring, and control. The component is based on the LP-WAN (Low Power Wide Area Network) LoRaWAN (LoRa Network) technology. The end-nodes, UAVs and also UGVs (Unmanned Ground Vehicle), connect to the Gateways through LoRaWAN. The Gateways are connected to the Network Server through Internet. The Network Server has two functions: (1) management of the network, and (2)support of the bidirectional messaging to and from end nodes. The second function uses the MQTT (Message Queue Telemetry Transport) protocol: end-nodes becomes MQTT publisher and their properties and methods can be accessed by any authorized MQTT publisher. The component includes an Application Server that connects to the Network Server, and that allows authorized interaction with the vehicles using any Web browser.","title":"TEKNE \u2013 LP-WAN for UAV identification and monitoring"},{"location":"enabling_technologies/other_WP/TEKNE_WP5_05_A/#lp-wan-for-uav-identification-and-monitoring","text":"ID: WP5-05 Contributor: TEKNE Owner: TEKNE Licence: Proprietary expected TRL: 5 KET: 1.1.1 E-Identification, 2.4.2 Intelligent Vehicle System Monitoring Contact: https://en.tekne.it/ The LP-WAN for UAV (Unmanned Aerial Vehicle) identification and monitoring component supports medium/long-range wireless communications for UAV identification, monitoring, and control. The component is based on the LP-WAN (Low Power Wide Area Network) LoRaWAN (LoRa Network) technology. The end-nodes, UAVs and also UGVs (Unmanned Ground Vehicle), connect to the Gateways through LoRaWAN. The Gateways are connected to the Network Server through Internet. The Network Server has two functions: (1) management of the network, and (2)support of the bidirectional messaging to and from end nodes. The second function uses the MQTT (Message Queue Telemetry Transport) protocol: end-nodes becomes MQTT publisher and their properties and methods can be accessed by any authorized MQTT publisher. The component includes an Application Server that connects to the Network Server, and that allows authorized interaction with the vehicles using any Web browser.","title":"LP-WAN for UAV identification and monitoring"},{"location":"enabling_technologies/other_WP/UNIVAQ_02/","text":"Autonomy, cooperation, and awareness ID: WP4-33 Contributor: UNIVAQ Owner: UNIVAQ Licence: OPEN SOURCE expected TRL: 3-4 KET: Command and control; Intelligent Mission Management; Obstacle Detection and Avoidance. Contact: Stefano Di Gennaro (stefano.digennaro@univaq.it); Mario Di Ferdinando (mario.diferdinando@univaq.it). Autonomy, cooperation, and awareness. The provided component concerns a robust digital algorithm for the autonomous navigation and cooperation of UAVs. In particular, the proposed digital control system is designed by taking simultaneously into account: the presence of sampling and quantization in the used devices; the presence of external disturbances such as measurement uncertainties and environment perturbations. The proposed control algorithm is able to arbitrarily attenuate the effects of such external disturbances and allows the main swarm cooperative strategies, namely time-varying flight formation, swarm tracking, and social foraging. The proposed control strategy includes also functionalities for the obstacle avoidance and the collision avoidance between adjacent members of the swarm. Limitations in the communication network are also considered. Indeed, the case in which each UAV can communicate with only a subset of the swarm is included. The external disturbances are unknown for the proposed control algorithm and no estimators are designed for the involved disturbances. The only knowledge required by the controller for attenuating external disturbances is their bound. An efficient strategy for the implementation on FPGAs of the proposed control algorithm is also presented. In particular, the proposed methodology is mainly focused on retiming and pipelining which allows: (i) lower execution times, and hence smaller sampling times, than its naive implementation; (ii) to take into account the energetic aspects of the controller implementations, and not only the controller performance. The proposed control strategy relies on methodology recently introduced in [1]-[7]. [1] M. Di Ferdinando, P. Pepe. Robustification of Sample-and-Hold Stabilizers for Control-Affine Time-Delay Systems, Automatica, vol 83, pp 141-154, 2017. [2] M. Di Ferdinando, P. Pepe, A. Borri, On Practical Stability Preservation under Fast Sampling and Accurate Quantization of Feedbacks for Nonlinear Time\u2013Delay Systems, IEEE Transactions on Automatic Control, vol 66, pp 314\u2013321, 2021. [3] C. A. L\u00faa, S. D. Gennaro et. al., Digital Implementation via FPGA of Controllers for Active Control of Ground Vehicles, IEEE Trans. on Ind. Inf., vol 15, pp 2253-2264, 2019. [4] F. Cesarone and P. Pepe, Sample-and-hold solution of a consensus problem with nonlinear dynamics and input/output disturbances, European Journal of Control, 2020. [5] F. Cesarone and P. Pepe, Robustification of sample-and-hold controllers for the consensus problem, 2019 Annual European Control Conference, pp. 471--476, 2019, doi:10.23919/ECC.2019.8796076. [6] R. Carli, G. Cavone, N. Epicoco, M. Di Ferdinando, P. Scarabaggio and M. Dotoli, Consensus-Based Algorithms for Controlling Swarms of Unmanned Aerial Vehicles, Ad-Hoc, Mobile, and Wireless Networks. ADHOC-NOW 2020. Lecture Notes in Computer Science, vol. 12338, 2020, https://doi.org/10.1007. [7] M. Di Ferdinando, P. Pepe, S. Di Gennaro, Robust Sampled-Data Consensus-Based Cooperative Control of Multi UAVs, 29th Mediterranean Conference on Control and Automation, Bari, Italy, 22-25 June 2021, to appear.","title":"UNIVAQ - Autonomy, cooperation, and awareness"},{"location":"enabling_technologies/other_WP/UNIVAQ_02/#autonomy-cooperation-and-awareness","text":"ID: WP4-33 Contributor: UNIVAQ Owner: UNIVAQ Licence: OPEN SOURCE expected TRL: 3-4 KET: Command and control; Intelligent Mission Management; Obstacle Detection and Avoidance. Contact: Stefano Di Gennaro (stefano.digennaro@univaq.it); Mario Di Ferdinando (mario.diferdinando@univaq.it). Autonomy, cooperation, and awareness. The provided component concerns a robust digital algorithm for the autonomous navigation and cooperation of UAVs. In particular, the proposed digital control system is designed by taking simultaneously into account: the presence of sampling and quantization in the used devices; the presence of external disturbances such as measurement uncertainties and environment perturbations. The proposed control algorithm is able to arbitrarily attenuate the effects of such external disturbances and allows the main swarm cooperative strategies, namely time-varying flight formation, swarm tracking, and social foraging. The proposed control strategy includes also functionalities for the obstacle avoidance and the collision avoidance between adjacent members of the swarm. Limitations in the communication network are also considered. Indeed, the case in which each UAV can communicate with only a subset of the swarm is included. The external disturbances are unknown for the proposed control algorithm and no estimators are designed for the involved disturbances. The only knowledge required by the controller for attenuating external disturbances is their bound. An efficient strategy for the implementation on FPGAs of the proposed control algorithm is also presented. In particular, the proposed methodology is mainly focused on retiming and pipelining which allows: (i) lower execution times, and hence smaller sampling times, than its naive implementation; (ii) to take into account the energetic aspects of the controller implementations, and not only the controller performance. The proposed control strategy relies on methodology recently introduced in [1]-[7]. [1] M. Di Ferdinando, P. Pepe. Robustification of Sample-and-Hold Stabilizers for Control-Affine Time-Delay Systems, Automatica, vol 83, pp 141-154, 2017. [2] M. Di Ferdinando, P. Pepe, A. Borri, On Practical Stability Preservation under Fast Sampling and Accurate Quantization of Feedbacks for Nonlinear Time\u2013Delay Systems, IEEE Transactions on Automatic Control, vol 66, pp 314\u2013321, 2021. [3] C. A. L\u00faa, S. D. Gennaro et. al., Digital Implementation via FPGA of Controllers for Active Control of Ground Vehicles, IEEE Trans. on Ind. Inf., vol 15, pp 2253-2264, 2019. [4] F. Cesarone and P. Pepe, Sample-and-hold solution of a consensus problem with nonlinear dynamics and input/output disturbances, European Journal of Control, 2020. [5] F. Cesarone and P. Pepe, Robustification of sample-and-hold controllers for the consensus problem, 2019 Annual European Control Conference, pp. 471--476, 2019, doi:10.23919/ECC.2019.8796076. [6] R. Carli, G. Cavone, N. Epicoco, M. Di Ferdinando, P. Scarabaggio and M. Dotoli, Consensus-Based Algorithms for Controlling Swarms of Unmanned Aerial Vehicles, Ad-Hoc, Mobile, and Wireless Networks. ADHOC-NOW 2020. Lecture Notes in Computer Science, vol. 12338, 2020, https://doi.org/10.1007. [7] M. Di Ferdinando, P. Pepe, S. Di Gennaro, Robust Sampled-Data Consensus-Based Cooperative Control of Multi UAVs, 29th Mediterranean Conference on Control and Automation, Bari, Italy, 22-25 June 2021, to appear.","title":"Autonomy, cooperation, and awareness"}]}